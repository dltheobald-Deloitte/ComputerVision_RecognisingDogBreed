{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scientist Nanodegree\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "\n",
    "## Project: Write an Algorithm for a Dog Identification App \n",
    "\n",
    "\n",
    "This notebook walks you through one of the most popular Udacity projects across machine learning and artificial intellegence nanodegree programs.  The goal is to classify images of dogs according to their breed.  \n",
    "\n",
    "If you are looking for a more guided capstone project related to deep learning and convolutional neural networks, this might be just it.  Notice that even if you follow the notebook to creating your classifier, you must still create a blog post or deploy an application to fulfill the requirements of the capstone project.\n",
    "\n",
    "Also notice, you may be able to use only parts of this notebook (for example certain coding portions or the data) without completing all parts and still meet all requirements of the capstone project.\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this project. You will not need to modify the included code beyond what is requested. Sections that begin with **'(IMPLEMENTATION)'** in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section, and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully! \n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question X'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut.  Markdown cells can be edited by double-clicking the cell to enter edit mode.\n",
    "\n",
    "The rubric contains _optional_ \"Stand Out Suggestions\" for enhancing the project beyond the minimum requirements. If you decide to pursue the \"Stand Out Suggestions\", you should include the code in this IPython notebook.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Why We're Here \n",
    "\n",
    "In this notebook, you will make the first steps towards developing an algorithm that could be used as part of a mobile or web app.  At the end of this project, your code will accept any user-supplied image as input.  If a dog is detected in the image, it will provide an estimate of the dog's breed.  If a human is detected, it will provide an estimate of the dog breed that is most resembling.  The image below displays potential sample output of your finished project (... but we expect that each student's algorithm will behave differently!). \n",
    "\n",
    "![Sample Dog Output](images/sample_dog_output.png)\n",
    "\n",
    "In this real-world setting, you will need to piece together a series of models to perform different tasks; for instance, the algorithm that detects humans in an image will be different from the CNN that infers dog breed.  There are many points of possible failure, and no perfect algorithm exists.  Your imperfect solution will nonetheless create a fun user experience!\n",
    "\n",
    "### The Road Ahead\n",
    "\n",
    "We break the notebook into separate steps.  Feel free to use the links below to navigate the notebook.\n",
    "\n",
    "* [Step 0](#step0): Import Datasets\n",
    "* [Step 1](#step1): Detect Humans\n",
    "* [Step 2](#step2): Detect Dogs\n",
    "* [Step 3](#step3): Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "* [Step 4](#step4): Use a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 5](#step5): Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 6](#step6): Write your Algorithm\n",
    "* [Step 7](#step7): Test Your Algorithm\n",
    "\n",
    "---\n",
    "<a id='step0'></a>\n",
    "## Step 0: Import Datasets\n",
    "\n",
    "### Import Dog Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of dog images.  We populate a few variables through the use of the `load_files` function from the scikit-learn library:\n",
    "- `train_files`, `valid_files`, `test_files` - numpy arrays containing file paths to images\n",
    "- `train_targets`, `valid_targets`, `test_targets` - numpy arrays containing onehot-encoded classification labels \n",
    "- `dog_names` - list of string-valued dog breed names for translating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 133 total dog categories.\n",
      "There are 8351 total dog images.\n",
      "\n",
      "There are 6680 training dog images.\n",
      "There are 835 validation dog images.\n",
      "There are 836 test dog images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    \"\"\"Loads all files from a path and returns it as file and labels\n",
    "    \n",
    "    Parameters:\n",
    "    path (String): THis is a directory containing data required\n",
    "    \n",
    "    Returns:\n",
    "    dog_files (np.array): arrays of file names loaded from the path\n",
    "    dog_targets (np.array): Array of folder names (i.e. dog breed) relating to the files\n",
    "    \"\"\"\n",
    "    #Loads in all files from path\n",
    "    data = load_files(path)\n",
    "    \n",
    "    #Isolates all file and folder names within\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    \n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('../dogImages/train')\n",
    "valid_files, valid_targets = load_dataset('../dogImages/valid')\n",
    "test_files, test_targets = load_dataset('../dogImages/test')\n",
    "\n",
    "# load list of dog names\n",
    "dog_names = [item[20:-1] for item in sorted(glob(\"../dogImages/train/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Human Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of human images, where the file paths are stored in the numpy array `human_files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13233 total human images.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(8675309)\n",
    "\n",
    "# load filenames in shuffled human dataset\n",
    "human_files = np.array(glob(\"../lfw/*/*\"))\n",
    "random.shuffle(human_files)\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total human images.' % len(human_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step1'></a>\n",
    "## Step 1: Detect Humans\n",
    "\n",
    "We use OpenCV's implementation of [Haar feature-based cascade classifiers](http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html) to detect human faces in images.  OpenCV provides many pre-trained face detectors, stored as XML files on [github](https://github.com/opencv/opencv/tree/master/data/haarcascades).  We have downloaded one of these detectors and stored it in the `haarcascades` directory.\n",
    "\n",
    "In the next code cell, we demonstrate how to use this detector to find human faces in a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of faces detected: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2                \n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline                               \n",
    "\n",
    "# extract pre-trained face detector\n",
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "# load color (BGR) image\n",
    "img = cv2.imread(human_files[1])\n",
    "# convert BGR image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# find faces in image\n",
    "faces = face_cascade.detectMultiScale(gray)\n",
    "\n",
    "# print number of faces detected in the image\n",
    "print('Number of faces detected:', len(faces))\n",
    "\n",
    "# get bounding box for each detected face\n",
    "for (x,y,w,h) in faces:\n",
    "    # add bounding box to color image\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    \n",
    "# convert BGR image to RGB for plotting\n",
    "cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# display the image, along with bounding box\n",
    "plt.imshow(cv_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using any of the face detectors, it is standard procedure to convert the images to grayscale.  The `detectMultiScale` function executes the classifier stored in `face_cascade` and takes the grayscale image as a parameter.  \n",
    "\n",
    "In the above code, `faces` is a numpy array of detected faces, where each row corresponds to a detected face.  Each detected face is a 1D array with four entries that specifies the bounding box of the detected face.  The first two entries in the array (extracted in the above code as `x` and `y`) specify the horizontal and vertical positions of the top left corner of the bounding box.  The last two entries in the array (extracted here as `w` and `h`) specify the width and height of the box.\n",
    "\n",
    "### Write a Human Face Detector\n",
    "\n",
    "We can use this procedure to write a function that returns `True` if a human face is detected in an image and `False` otherwise.  This function, aptly named `face_detector`, takes a string-valued file path to an image as input and appears in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns \"True\" if face is detected in image stored at img_path\n",
    "def face_detector(img_path):\n",
    "    \"\"\"Takes a file path of an image and detects whether a human face is present.\n",
    "\n",
    "    Parameters:\n",
    "    img_path (String): Path to a file containing the image.\n",
    "\n",
    "    Returns:\n",
    "    (Boolean): Presence of a human face\n",
    "    \"\"\"\n",
    "    #Loads face classifier\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "    \n",
    "    #Reads in images and converts format\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #generates list of faces\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    \n",
    "    return len(faces) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Assess the Human Face Detector\n",
    "\n",
    "__Question 1:__ Use the code cell below to test the performance of the `face_detector` function.  \n",
    "- What percentage of the first 100 images in `human_files` have a detected human face?  \n",
    "- What percentage of the first 100 images in `dog_files` have a detected human face? \n",
    "\n",
    "Ideally, we would like 100% of human images with a detected face and 0% of dog images with a detected face.  You will see that our algorithm falls short of this goal, but still gives acceptable performance.  We extract the file paths for the first 100 images from each of the datasets and store them in the numpy arrays `human_files_short` and `dog_files_short`.\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The face detector correctly guessed 99% presence of human faces.\n",
      "The face detector incorrectly guessed 12% of dogs as having human faces.\n"
     ]
    }
   ],
   "source": [
    "#Generates a small subset of data\n",
    "human_files_short = human_files[:100]\n",
    "dog_files_short = train_files[:100]\n",
    "# Do NOT modify the code above this line.\n",
    "\n",
    "## TODO: Test the performance of the face_detector algorithm \n",
    "## on the images in human_files_short and dog_files_short.\n",
    "\n",
    "#Finds number of human faces detected\n",
    "human_face_detected = 0\n",
    "for human in human_files_short:\n",
    "    if face_detector(human):\n",
    "        human_face_detected += 1\n",
    "\n",
    "#Finds number of dogs mistakenly detected\n",
    "dog_face_detected = 0\n",
    "for dog in dog_files_short:\n",
    "    if face_detector(dog):\n",
    "        dog_face_detected += 1\n",
    "\n",
    "#Creating accuracy metric for the face detector\n",
    "human_accuracy = float(human_face_detected) / float(len(human_files_short))\n",
    "dog_accuracy = float(dog_face_detected) / float(len(dog_files_short))\n",
    "human_accuracy = \"{0:.0%}\".format(human_accuracy)\n",
    "dog_accuracy = \"{0:.0%}\".format(dog_accuracy)\n",
    "\n",
    "#Printing performance of face detector\n",
    "print(\"The face detector correctly guessed %s presence of human faces.\"%(human_accuracy))    \n",
    "print(\"The face detector incorrectly guessed %s of dogs as having human faces.\"%(dog_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2:__ This algorithmic choice necessitates that we communicate to the user that we accept human images only when they provide a clear view of a face (otherwise, we risk having unneccessarily frustrated users!). In your opinion, is this a reasonable expectation to pose on the user? If not, can you think of a way to detect humans in images that does not necessitate an image with a clearly presented face?\n",
    "\n",
    "__Answer:__\n",
    "\n",
    "We suggest the face detector from OpenCV as a potential way to detect human images in your algorithm, but you are free to explore other approaches, especially approaches that make use of deep learning :).  Please use the code cell below to design and test your own face detection algorithm.  If you decide to pursue this _optional_ task, report performance on each of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (Optional) TODO: Report the performance of another \n",
    "## face detection algorithm on the LFW dataset\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step2'></a>\n",
    "## Step 2: Detect Dogs\n",
    "\n",
    "In this section, we use a pre-trained [ResNet-50](http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006) model to detect dogs in images.  Our first line of code downloads the ResNet-50 model, along with weights that have been trained on [ImageNet](http://www.image-net.org/), a very large, very popular dataset used for image classification and other vision tasks.  ImageNet contains over 10 million URLs, each linking to an image containing an object from one of [1000 categories](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).  Given an image, this pre-trained ResNet-50 model returns a prediction (derived from the available categories in ImageNet) for the object that is contained in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dltheobald\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\dltheobald\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\dltheobald\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\dltheobald\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\dltheobald\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\dltheobald\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\dltheobald\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "# define ResNet50 model\n",
    "ResNet50_model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Data\n",
    "\n",
    "When using TensorFlow as backend, Keras CNNs require a 4D array (which we'll also refer to as a 4D tensor) as input, with shape\n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, \\text{rows}, \\text{columns}, \\text{channels}),\n",
    "$$\n",
    "\n",
    "where `nb_samples` corresponds to the total number of images (or samples), and `rows`, `columns`, and `channels` correspond to the number of rows, columns, and channels for each image, respectively.  \n",
    "\n",
    "The `path_to_tensor` function below takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN.  The function first loads the image and resizes it to a square image that is $224 \\times 224$ pixels.  Next, the image is converted to an array, which is then resized to a 4D tensor.  In this case, since we are working with color images, each image has three channels.  Likewise, since we are processing a single image (or sample), the returned tensor will always have shape\n",
    "\n",
    "$$\n",
    "(1, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "The `paths_to_tensor` function takes a numpy array of string-valued image paths as input and returns a 4D tensor with shape \n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "Here, `nb_samples` is the number of samples, or number of images, in the supplied array of image paths.  It is best to think of `nb_samples` as the number of 3D tensors (where each 3D tensor corresponds to a different image) in your dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    \"\"\"Takes a file path of an image, loads it and then extracts information into a fixed shape numpy \n",
    "    array, ready to be used for VGG19 feature extracrion.\n",
    "\n",
    "    Parameters:\n",
    "    img_path (String): Path to a file containing the image.\n",
    "\n",
    "    Returns:\n",
    "    (np.array): A numpy array with fixed dimensions and information from an image\n",
    "    \"\"\"\n",
    "    \n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    \n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    \"\"\"Takes multiple file path of an image, loads it and then extracts information into a fixed shape numpy \n",
    "    array, ready to be used for VGG19 feature extraction.\n",
    "\n",
    "    Parameters:\n",
    "    img_path (String): Path to a file containing the image.\n",
    "\n",
    "    Returns:\n",
    "    (np.array): A numpy array with fixed dimensions and information from a selection image\n",
    "    \"\"\"\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    \n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions with ResNet-50\n",
    "\n",
    "Getting the 4D tensor ready for ResNet-50, and for any other pre-trained model in Keras, requires some additional processing.  First, the RGB image is converted to BGR by reordering the channels.  All pre-trained models have the additional normalization step that the mean pixel (expressed in RGB as $[103.939, 116.779, 123.68]$ and calculated from all pixels in all images in ImageNet) must be subtracted from every pixel in each image.  This is implemented in the imported function `preprocess_input`.  If you're curious, you can check the code for `preprocess_input` [here](https://github.com/fchollet/keras/blob/master/keras/applications/imagenet_utils.py).\n",
    "\n",
    "Now that we have a way to format our image for supplying to ResNet-50, we are now ready to use the model to extract the predictions.  This is accomplished with the `predict` method, which returns an array whose $i$-th entry is the model's predicted probability that the image belongs to the $i$-th ImageNet category.  This is implemented in the `ResNet50_predict_labels` function below.\n",
    "\n",
    "By taking the argmax of the predicted probability vector, we obtain an integer corresponding to the model's predicted object class, which we can identify with an object category through the use of this [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "def ResNet50_predict_labels(img_path):\n",
    "    \"\"\"Takes an image path, extracts information from it and uses the resnet model to classify the image\n",
    "\n",
    "    Parameters:\n",
    "    img_path (String): Path to a file containing the image.\n",
    "\n",
    "    Returns:\n",
    "    (Integer): A integer corresponding to the classification of the image\n",
    "    \"\"\"\n",
    "    # returns prediction vector for image located at img_path\n",
    "    img = preprocess_input(path_to_tensor(img_path))\n",
    "    \n",
    "    return np.argmax(ResNet50_model.predict(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Dog Detector\n",
    "\n",
    "While looking at the [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a), you will notice that the categories corresponding to dogs appear in an uninterrupted sequence and correspond to dictionary keys 151-268, inclusive, to include all categories from `'Chihuahua'` to `'Mexican hairless'`.  Thus, in order to check to see if an image is predicted to contain a dog by the pre-trained ResNet-50 model, we need only check if the `ResNet50_predict_labels` function above returns a value between 151 and 268 (inclusive).\n",
    "\n",
    "We use these ideas to complete the `dog_detector` function below, which returns `True` if a dog is detected in an image (and `False` if not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### returns \"True\" if a dog is detected in the image stored at img_path\n",
    "def dog_detector(img_path):\n",
    "    \"\"\"Takes a file path of an image and returns whether it is a dog or not\n",
    "\n",
    "    Parameters:\n",
    "    img_path (String): Path to a file containing the image.\n",
    "\n",
    "    Returns:\n",
    "    (Boolean): Presence of a dog\n",
    "    \"\"\"\n",
    "    #USesresnet to predict category of image\n",
    "    prediction = ResNet50_predict_labels(img_path)\n",
    "    \n",
    "    #Dogs are between the numbers returns below\n",
    "    return ((prediction <= 268) & (prediction >= 151)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Assess the Dog Detector\n",
    "\n",
    "__Question 3:__ Use the code cell below to test the performance of your `dog_detector` function.  \n",
    "- What percentage of the images in `human_files_short` have a detected dog?  \n",
    "- What percentage of the images in `dog_files_short` have a detected dog?\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The face detector incorrectly guessed 1% of humans as a dog.\n",
      "The face detector correctly guessed 100% of dog faces.\n"
     ]
    }
   ],
   "source": [
    "### TODO: Test the performance of the dog_detector function\n",
    "### on the images in human_files_short and dog_files_short.\n",
    "\n",
    "#Detects the mistakes of the dog detector on humans\n",
    "human_count = 0\n",
    "dog_count = 0\n",
    "for human in human_files_short:\n",
    "    if dog_detector(human):\n",
    "        human_count += 1\n",
    "\n",
    "#Detects the accuracy of the dog detector        \n",
    "dog_count = 0\n",
    "for dog in dog_files_short:\n",
    "    if dog_detector(dog):\n",
    "        dog_count += 1\n",
    "        \n",
    "#calculates the accuracy of dog_detector\n",
    "human_dog_accuracy = float(human_count) / float(len(human_files_short))\n",
    "dog_dog_accuracy = float(dog_count) / float(len(dog_files_short))\n",
    "human_dog_accuracy = \"{0:.0%}\".format(human_dog_accuracy)\n",
    "dog_dog_accuracy = \"{0:.0%}\".format(dog_dog_accuracy)\n",
    "\n",
    "#Prints accuracy metric\n",
    "print(\"The face detector incorrectly guessed %s of humans as a dog.\"%(human_dog_accuracy))    \n",
    "print(\"The face detector correctly guessed %s of dog faces.\"%(dog_dog_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step3'></a>\n",
    "## Step 3: Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "\n",
    "Now that we have functions for detecting humans and dogs in images, we need a way to predict breed from images.  In this step, you will create a CNN that classifies dog breeds.  You must create your CNN _from scratch_ (so, you can't use transfer learning _yet_!), and you must attain a test accuracy of at least 1%.  In Step 5 of this notebook, you will have the opportunity to use transfer learning to create a CNN that attains greatly improved accuracy.\n",
    "\n",
    "Be careful with adding too many trainable layers!  More parameters means longer training, which means you are more likely to need a GPU to accelerate the training process.  Thankfully, Keras provides a handy estimate of the time that each epoch is likely to take; you can extrapolate this estimate to figure out how long it will take for your algorithm to train. \n",
    "\n",
    "We mention that the task of assigning breed to dogs from images is considered exceptionally challenging.  To see why, consider that *even a human* would have great difficulty in distinguishing between a Brittany and a Welsh Springer Spaniel.  \n",
    "\n",
    "Brittany | Welsh Springer Spaniel\n",
    "- | - \n",
    "<img src=\"images/Brittany_02625.jpg\" width=\"100\"> | <img src=\"images/Welsh_springer_spaniel_08203.jpg\" width=\"200\">\n",
    "\n",
    "It is not difficult to find other dog breed pairs with minimal inter-class variation (for instance, Curly-Coated Retrievers and American Water Spaniels).  \n",
    "\n",
    "Curly-Coated Retriever | American Water Spaniel\n",
    "- | -\n",
    "<img src=\"images/Curly-coated_retriever_03896.jpg\" width=\"200\"> | <img src=\"images/American_water_spaniel_00648.jpg\" width=\"200\">\n",
    "\n",
    "\n",
    "Likewise, recall that labradors come in yellow, chocolate, and black.  Your vision-based algorithm will have to conquer this high intra-class variation to determine how to classify all of these different shades as the same breed.  \n",
    "\n",
    "Yellow Labrador | Chocolate Labrador | Black Labrador\n",
    "- | -\n",
    "<img src=\"images/Labrador_retriever_06457.jpg\" width=\"150\"> | <img src=\"images/Labrador_retriever_06455.jpg\" width=\"240\"> | <img src=\"images/Labrador_retriever_06449.jpg\" width=\"220\">\n",
    "\n",
    "We also mention that random chance presents an exceptionally low bar: setting aside the fact that the classes are slightly imabalanced, a random guess will provide a correct answer roughly 1 in 133 times, which corresponds to an accuracy of less than 1%.  \n",
    "\n",
    "Remember that the practice is far ahead of the theory in deep learning.  Experiment with many different architectures, and trust your intuition.  And, of course, have fun! \n",
    "\n",
    "### Pre-process the Data\n",
    "\n",
    "We rescale the images by dividing every pixel in every image by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 6680/6680 [01:13<00:00, 90.41it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 835/835 [00:08<00:00, 100.31it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 836/836 [00:08<00:00, 102.97it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        model.summary()\n",
    "\n",
    "We have imported some Python modules to get you started, but feel free to import as many modules as you need.  If you end up getting stuck, here's a hint that specifies a model that trains relatively fast on CPU and attains >1% test accuracy in 5 epochs:\n",
    "\n",
    "![Sample CNN](images/sample_cnn.png)\n",
    "           \n",
    "__Question 4:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  If you chose to use the hinted architecture above, describe why you think that CNN architecture should work well for the image classification task.\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 8)       224       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 112, 112, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 112, 112, 16)      1168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 56, 56, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 56, 56, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 133)               3336837   \n",
      "=================================================================\n",
      "Total params: 3,342,869\n",
      "Trainable params: 3,342,869\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "#Instatiate model with Sequential Keras object\n",
    "model = Sequential()\n",
    "\n",
    "#Defines model architecture (activations etc.)\n",
    "model.add(Conv2D(8, kernel_size = 3, strides = 1,\n",
    "                 activation = 'relu', padding = 'same',\n",
    "                 input_shape = (224,224,3)))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Conv2D(16, kernel_size = 3, strides = 1,\n",
    "                activation = 'relu', padding = 'same'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Conv2D(32, kernel_size = 3, strides = 1,\n",
    "                activation = 'relu', padding = 'same'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(133, activation = 'softmax'))\n",
    "\n",
    "#Print models architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dltheobald\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Defines the metric to measure against, the optimiser and the loss function used for training the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.\n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dltheobald\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4220/6680 [=================>............] - ETA: 7:49 - loss: 4.9729 - acc: 0.0000e+0 - ETA: 4:20 - loss: 7.8642 - acc: 0.0000e+0 - ETA: 3:09 - loss: 7.3319 - acc: 0.0000e+0 - ETA: 2:35 - loss: 6.8535 - acc: 0.0000e+0 - ETA: 2:14 - loss: 6.5384 - acc: 0.0000e+0 - ETA: 2:00 - loss: 6.3121 - acc: 0.0000e+0 - ETA: 1:50 - loss: 6.1125 - acc: 0.0000e+0 - ETA: 1:43 - loss: 5.9819 - acc: 0.0063    - ETA: 1:36 - loss: 5.8585 - acc: 0.005 - ETA: 1:33 - loss: 5.7637 - acc: 0.005 - ETA: 1:31 - loss: 5.6849 - acc: 0.004 - ETA: 1:27 - loss: 5.6126 - acc: 0.004 - ETA: 1:25 - loss: 5.5752 - acc: 0.003 - ETA: 1:22 - loss: 5.5258 - acc: 0.003 - ETA: 1:20 - loss: 5.4874 - acc: 0.003 - ETA: 1:18 - loss: 5.4493 - acc: 0.003 - ETA: 1:16 - loss: 5.4168 - acc: 0.002 - ETA: 1:14 - loss: 5.3881 - acc: 0.002 - ETA: 1:13 - loss: 5.3607 - acc: 0.002 - ETA: 1:11 - loss: 5.3397 - acc: 0.002 - ETA: 1:11 - loss: 5.3187 - acc: 0.002 - ETA: 1:10 - loss: 5.3002 - acc: 0.002 - ETA: 1:09 - loss: 5.2831 - acc: 0.002 - ETA: 1:09 - loss: 5.2662 - acc: 0.002 - ETA: 1:10 - loss: 5.2505 - acc: 0.002 - ETA: 1:10 - loss: 5.2371 - acc: 0.001 - ETA: 1:10 - loss: 5.2238 - acc: 0.001 - ETA: 1:10 - loss: 5.2120 - acc: 0.001 - ETA: 1:11 - loss: 5.2004 - acc: 0.001 - ETA: 1:10 - loss: 5.1899 - acc: 0.001 - ETA: 1:10 - loss: 5.1804 - acc: 0.003 - ETA: 1:10 - loss: 5.1711 - acc: 0.003 - ETA: 1:10 - loss: 5.1625 - acc: 0.004 - ETA: 1:10 - loss: 5.1544 - acc: 0.004 - ETA: 1:10 - loss: 5.1468 - acc: 0.004 - ETA: 1:09 - loss: 5.1388 - acc: 0.005 - ETA: 1:09 - loss: 5.1314 - acc: 0.005 - ETA: 1:09 - loss: 5.1252 - acc: 0.005 - ETA: 1:09 - loss: 5.1186 - acc: 0.005 - ETA: 1:09 - loss: 5.1135 - acc: 0.005 - ETA: 1:08 - loss: 5.1079 - acc: 0.004 - ETA: 1:08 - loss: 5.1029 - acc: 0.004 - ETA: 1:07 - loss: 5.0979 - acc: 0.005 - ETA: 1:07 - loss: 5.0929 - acc: 0.005 - ETA: 1:06 - loss: 5.0878 - acc: 0.005 - ETA: 1:06 - loss: 5.0833 - acc: 0.006 - ETA: 1:05 - loss: 5.0784 - acc: 0.006 - ETA: 1:04 - loss: 5.0711 - acc: 0.006 - ETA: 1:04 - loss: 5.0705 - acc: 0.006 - ETA: 1:03 - loss: 5.0672 - acc: 0.006 - ETA: 1:03 - loss: 5.0639 - acc: 0.005 - ETA: 1:02 - loss: 5.0606 - acc: 0.005 - ETA: 1:02 - loss: 5.0569 - acc: 0.006 - ETA: 1:01 - loss: 5.0539 - acc: 0.006 - ETA: 1:01 - loss: 5.0510 - acc: 0.006 - ETA: 1:00 - loss: 5.0479 - acc: 0.007 - ETA: 1:00 - loss: 5.0440 - acc: 0.007 - ETA: 59s - loss: 5.0404 - acc: 0.006 - ETA: 59s - loss: 5.0385 - acc: 0.00 - ETA: 58s - loss: 5.0358 - acc: 0.00 - ETA: 58s - loss: 5.0329 - acc: 0.00 - ETA: 58s - loss: 5.0298 - acc: 0.00 - ETA: 57s - loss: 5.0257 - acc: 0.00 - ETA: 57s - loss: 5.0230 - acc: 0.00 - ETA: 57s - loss: 5.0220 - acc: 0.00 - ETA: 56s - loss: 5.0197 - acc: 0.00 - ETA: 56s - loss: 5.0175 - acc: 0.00 - ETA: 56s - loss: 5.0154 - acc: 0.00 - ETA: 55s - loss: 5.0145 - acc: 0.00 - ETA: 55s - loss: 5.0124 - acc: 0.00 - ETA: 55s - loss: 5.0098 - acc: 0.00 - ETA: 54s - loss: 5.0068 - acc: 0.00 - ETA: 54s - loss: 5.0052 - acc: 0.00 - ETA: 54s - loss: 5.0028 - acc: 0.00 - ETA: 53s - loss: 5.0004 - acc: 0.00 - ETA: 53s - loss: 4.9982 - acc: 0.00 - ETA: 53s - loss: 4.9966 - acc: 0.00 - ETA: 52s - loss: 4.9954 - acc: 0.00 - ETA: 52s - loss: 4.9936 - acc: 0.00 - ETA: 52s - loss: 4.9914 - acc: 0.00 - ETA: 53s - loss: 4.9899 - acc: 0.00 - ETA: 52s - loss: 4.9891 - acc: 0.00 - ETA: 52s - loss: 4.9871 - acc: 0.00 - ETA: 52s - loss: 4.9853 - acc: 0.00 - ETA: 52s - loss: 4.9839 - acc: 0.00 - ETA: 51s - loss: 4.9845 - acc: 0.00 - ETA: 51s - loss: 4.9827 - acc: 0.00 - ETA: 51s - loss: 4.9807 - acc: 0.00 - ETA: 50s - loss: 4.9789 - acc: 0.00 - ETA: 50s - loss: 4.9779 - acc: 0.00 - ETA: 50s - loss: 4.9768 - acc: 0.00 - ETA: 49s - loss: 4.9755 - acc: 0.00 - ETA: 49s - loss: 4.9741 - acc: 0.00 - ETA: 49s - loss: 4.9730 - acc: 0.00 - ETA: 48s - loss: 4.9724 - acc: 0.00 - ETA: 48s - loss: 4.9716 - acc: 0.00 - ETA: 48s - loss: 4.9701 - acc: 0.00 - ETA: 48s - loss: 4.9686 - acc: 0.00 - ETA: 47s - loss: 4.9660 - acc: 0.00 - ETA: 47s - loss: 4.9702 - acc: 0.00 - ETA: 47s - loss: 4.9689 - acc: 0.00 - ETA: 46s - loss: 4.9675 - acc: 0.00 - ETA: 46s - loss: 4.9668 - acc: 0.00 - ETA: 46s - loss: 4.9661 - acc: 0.00 - ETA: 46s - loss: 4.9651 - acc: 0.00 - ETA: 45s - loss: 4.9637 - acc: 0.00 - ETA: 45s - loss: 4.9626 - acc: 0.00 - ETA: 45s - loss: 4.9610 - acc: 0.00 - ETA: 45s - loss: 4.9612 - acc: 0.00 - ETA: 44s - loss: 4.9600 - acc: 0.00 - ETA: 44s - loss: 4.9586 - acc: 0.00 - ETA: 44s - loss: 4.9561 - acc: 0.00 - ETA: 44s - loss: 4.9550 - acc: 0.00 - ETA: 43s - loss: 4.9540 - acc: 0.00 - ETA: 43s - loss: 4.9533 - acc: 0.00 - ETA: 43s - loss: 4.9513 - acc: 0.00 - ETA: 43s - loss: 4.9507 - acc: 0.00 - ETA: 42s - loss: 4.9499 - acc: 0.00 - ETA: 42s - loss: 4.9497 - acc: 0.01 - ETA: 42s - loss: 4.9489 - acc: 0.01 - ETA: 42s - loss: 4.9475 - acc: 0.00 - ETA: 41s - loss: 4.9474 - acc: 0.01 - ETA: 41s - loss: 4.9471 - acc: 0.01 - ETA: 41s - loss: 4.9459 - acc: 0.01 - ETA: 41s - loss: 4.9454 - acc: 0.01 - ETA: 41s - loss: 4.9449 - acc: 0.01 - ETA: 41s - loss: 4.9447 - acc: 0.01 - ETA: 41s - loss: 4.9439 - acc: 0.01 - ETA: 41s - loss: 4.9429 - acc: 0.01 - ETA: 40s - loss: 4.9418 - acc: 0.01 - ETA: 40s - loss: 4.9407 - acc: 0.01 - ETA: 40s - loss: 4.9391 - acc: 0.01 - ETA: 40s - loss: 4.9391 - acc: 0.01 - ETA: 40s - loss: 4.9381 - acc: 0.01 - ETA: 40s - loss: 4.9380 - acc: 0.01 - ETA: 40s - loss: 4.9373 - acc: 0.01 - ETA: 39s - loss: 4.9365 - acc: 0.01 - ETA: 39s - loss: 4.9353 - acc: 0.01 - ETA: 39s - loss: 4.9344 - acc: 0.01 - ETA: 39s - loss: 4.9344 - acc: 0.01 - ETA: 38s - loss: 4.9333 - acc: 0.01 - ETA: 38s - loss: 4.9329 - acc: 0.01 - ETA: 38s - loss: 4.9314 - acc: 0.01 - ETA: 38s - loss: 4.9315 - acc: 0.01 - ETA: 38s - loss: 4.9311 - acc: 0.01 - ETA: 37s - loss: 4.9302 - acc: 0.01 - ETA: 37s - loss: 4.9299 - acc: 0.01 - ETA: 37s - loss: 4.9294 - acc: 0.01 - ETA: 37s - loss: 4.9286 - acc: 0.01 - ETA: 36s - loss: 4.9281 - acc: 0.01 - ETA: 36s - loss: 4.9278 - acc: 0.01 - ETA: 36s - loss: 4.9272 - acc: 0.01 - ETA: 36s - loss: 4.9263 - acc: 0.01 - ETA: 36s - loss: 4.9236 - acc: 0.01 - ETA: 35s - loss: 4.9292 - acc: 0.01 - ETA: 35s - loss: 4.9281 - acc: 0.01 - ETA: 35s - loss: 4.9276 - acc: 0.01 - ETA: 35s - loss: 4.9273 - acc: 0.01 - ETA: 35s - loss: 4.9264 - acc: 0.01 - ETA: 34s - loss: 4.9247 - acc: 0.01 - ETA: 34s - loss: 4.9240 - acc: 0.01 - ETA: 34s - loss: 4.9226 - acc: 0.01 - ETA: 34s - loss: 4.9207 - acc: 0.01 - ETA: 34s - loss: 4.9192 - acc: 0.01 - ETA: 33s - loss: 4.9201 - acc: 0.01 - ETA: 33s - loss: 4.9204 - acc: 0.01 - ETA: 33s - loss: 4.9200 - acc: 0.01 - ETA: 33s - loss: 4.9194 - acc: 0.01 - ETA: 33s - loss: 4.9187 - acc: 0.01 - ETA: 32s - loss: 4.9181 - acc: 0.01 - ETA: 32s - loss: 4.9171 - acc: 0.01 - ETA: 32s - loss: 4.9162 - acc: 0.01 - ETA: 32s - loss: 4.9153 - acc: 0.01 - ETA: 31s - loss: 4.9145 - acc: 0.01 - ETA: 31s - loss: 4.9130 - acc: 0.01 - ETA: 31s - loss: 4.9120 - acc: 0.01 - ETA: 31s - loss: 4.9124 - acc: 0.01 - ETA: 30s - loss: 4.9125 - acc: 0.01 - ETA: 30s - loss: 4.9123 - acc: 0.01 - ETA: 30s - loss: 4.9115 - acc: 0.01 - ETA: 30s - loss: 4.9107 - acc: 0.01 - ETA: 30s - loss: 4.9095 - acc: 0.01 - ETA: 29s - loss: 4.9095 - acc: 0.01 - ETA: 29s - loss: 4.9079 - acc: 0.01 - ETA: 29s - loss: 4.9079 - acc: 0.01 - ETA: 29s - loss: 4.9079 - acc: 0.01 - ETA: 28s - loss: 4.9078 - acc: 0.01 - ETA: 28s - loss: 4.9072 - acc: 0.01 - ETA: 28s - loss: 4.9070 - acc: 0.01 - ETA: 28s - loss: 4.9067 - acc: 0.01 - ETA: 27s - loss: 4.9059 - acc: 0.01 - ETA: 27s - loss: 4.9045 - acc: 0.01 - ETA: 27s - loss: 4.9042 - acc: 0.01 - ETA: 27s - loss: 4.9036 - acc: 0.01 - ETA: 27s - loss: 4.9023 - acc: 0.01 - ETA: 26s - loss: 4.9028 - acc: 0.01 - ETA: 26s - loss: 4.9029 - acc: 0.01 - ETA: 26s - loss: 4.9023 - acc: 0.01 - ETA: 26s - loss: 4.9008 - acc: 0.01 - ETA: 26s - loss: 4.8989 - acc: 0.01 - ETA: 25s - loss: 4.8986 - acc: 0.01 - ETA: 25s - loss: 4.8979 - acc: 0.01 - ETA: 25s - loss: 4.8978 - acc: 0.01 - ETA: 25s - loss: 4.8965 - acc: 0.01 - ETA: 24s - loss: 4.8959 - acc: 0.01 - ETA: 24s - loss: 4.8952 - acc: 0.01 - ETA: 24s - loss: 4.8942 - acc: 0.01 - ETA: 24s - loss: 4.8945 - acc: 0.01 - ETA: 24s - loss: 4.8936 - acc: 0.01 - ETA: 23s - loss: 4.8932 - acc: 0.01 - ETA: 23s - loss: 4.8926 - acc: 0.0166"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 23s - loss: 4.8921 - acc: 0.01 - ETA: 23s - loss: 4.8914 - acc: 0.01 - ETA: 23s - loss: 4.8909 - acc: 0.01 - ETA: 22s - loss: 4.8901 - acc: 0.01 - ETA: 22s - loss: 4.8898 - acc: 0.01 - ETA: 22s - loss: 4.8894 - acc: 0.01 - ETA: 22s - loss: 4.8874 - acc: 0.01 - ETA: 22s - loss: 4.8865 - acc: 0.01 - ETA: 21s - loss: 4.8858 - acc: 0.01 - ETA: 21s - loss: 4.8851 - acc: 0.01 - ETA: 21s - loss: 4.8858 - acc: 0.01 - ETA: 21s - loss: 4.8852 - acc: 0.01 - ETA: 21s - loss: 4.8839 - acc: 0.01 - ETA: 20s - loss: 4.8823 - acc: 0.01 - ETA: 20s - loss: 4.8828 - acc: 0.01 - ETA: 20s - loss: 4.8833 - acc: 0.01 - ETA: 20s - loss: 4.8825 - acc: 0.01 - ETA: 20s - loss: 4.8824 - acc: 0.01 - ETA: 19s - loss: 4.8823 - acc: 0.01 - ETA: 19s - loss: 4.8817 - acc: 0.01 - ETA: 19s - loss: 4.8813 - acc: 0.01 - ETA: 19s - loss: 4.8811 - acc: 0.01 - ETA: 19s - loss: 4.8812 - acc: 0.01 - ETA: 18s - loss: 4.8813 - acc: 0.01 - ETA: 18s - loss: 4.8805 - acc: 0.01 - ETA: 18s - loss: 4.8792 - acc: 0.01 - ETA: 18s - loss: 4.8781 - acc: 0.01 - ETA: 18s - loss: 4.8786 - acc: 0.01 - ETA: 17s - loss: 4.8778 - acc: 0.01 - ETA: 17s - loss: 4.8769 - acc: 0.01 - ETA: 17s - loss: 4.8766 - acc: 0.01 - ETA: 17s - loss: 4.8761 - acc: 0.01 - ETA: 17s - loss: 4.8760 - acc: 0.01 - ETA: 16s - loss: 4.8759 - acc: 0.01 - ETA: 16s - loss: 4.8761 - acc: 0.01 - ETA: 16s - loss: 4.8762 - acc: 0.01 - ETA: 16s - loss: 4.8755 - acc: 0.01 - ETA: 16s - loss: 4.8745 - acc: 0.01 - ETA: 15s - loss: 4.8746 - acc: 0.01 - ETA: 15s - loss: 4.8741 - acc: 0.01 - ETA: 15s - loss: 4.8742 - acc: 0.01 - ETA: 15s - loss: 4.8737 - acc: 0.01 - ETA: 15s - loss: 4.8733 - acc: 0.01 - ETA: 14s - loss: 4.8725 - acc: 0.01 - ETA: 14s - loss: 4.8721 - acc: 0.01 - ETA: 14s - loss: 4.8707 - acc: 0.01 - ETA: 14s - loss: 4.8714 - acc: 0.01 - ETA: 14s - loss: 4.8710 - acc: 0.01 - ETA: 13s - loss: 4.8708 - acc: 0.01 - ETA: 13s - loss: 4.8705 - acc: 0.01 - ETA: 13s - loss: 4.8711 - acc: 0.01 - ETA: 13s - loss: 4.8699 - acc: 0.01 - ETA: 13s - loss: 4.8688 - acc: 0.01 - ETA: 13s - loss: 4.8687 - acc: 0.01 - ETA: 12s - loss: 4.8677 - acc: 0.01 - ETA: 12s - loss: 4.8668 - acc: 0.01 - ETA: 12s - loss: 4.8667 - acc: 0.01 - ETA: 12s - loss: 4.8655 - acc: 0.01 - ETA: 12s - loss: 4.8663 - acc: 0.01 - ETA: 11s - loss: 4.8663 - acc: 0.01 - ETA: 11s - loss: 4.8658 - acc: 0.01 - ETA: 11s - loss: 4.8657 - acc: 0.01 - ETA: 11s - loss: 4.8649 - acc: 0.01 - ETA: 11s - loss: 4.8647 - acc: 0.01 - ETA: 10s - loss: 4.8640 - acc: 0.01 - ETA: 10s - loss: 4.8637 - acc: 0.01 - ETA: 10s - loss: 4.8635 - acc: 0.01 - ETA: 10s - loss: 4.8623 - acc: 0.01 - ETA: 10s - loss: 4.8614 - acc: 0.01 - ETA: 9s - loss: 4.8604 - acc: 0.0194 - ETA: 9s - loss: 4.8596 - acc: 0.019 - ETA: 9s - loss: 4.8592 - acc: 0.019 - ETA: 9s - loss: 4.8584 - acc: 0.019 - ETA: 9s - loss: 4.8577 - acc: 0.019 - ETA: 9s - loss: 4.8583 - acc: 0.019 - ETA: 8s - loss: 4.8575 - acc: 0.019 - ETA: 8s - loss: 4.8573 - acc: 0.019 - ETA: 8s - loss: 4.8573 - acc: 0.019 - ETA: 8s - loss: 4.8569 - acc: 0.019 - ETA: 8s - loss: 4.8564 - acc: 0.019 - ETA: 7s - loss: 4.8556 - acc: 0.019 - ETA: 7s - loss: 4.8553 - acc: 0.020 - ETA: 7s - loss: 4.8550 - acc: 0.020 - ETA: 7s - loss: 4.8532 - acc: 0.020 - ETA: 7s - loss: 4.8532 - acc: 0.020 - ETA: 6s - loss: 4.8529 - acc: 0.020 - ETA: 6s - loss: 4.8525 - acc: 0.020 - ETA: 6s - loss: 4.8521 - acc: 0.020 - ETA: 6s - loss: 4.8516 - acc: 0.020 - ETA: 6s - loss: 4.8509 - acc: 0.019 - ETA: 6s - loss: 4.8507 - acc: 0.019 - ETA: 5s - loss: 4.8510 - acc: 0.019 - ETA: 5s - loss: 4.8514 - acc: 0.019 - ETA: 5s - loss: 4.8507 - acc: 0.020 - ETA: 5s - loss: 4.8504 - acc: 0.019 - ETA: 5s - loss: 4.8492 - acc: 0.019 - ETA: 4s - loss: 4.8492 - acc: 0.019 - ETA: 4s - loss: 4.8494 - acc: 0.019 - ETA: 4s - loss: 4.8492 - acc: 0.019 - ETA: 4s - loss: 4.8480 - acc: 0.019 - ETA: 4s - loss: 4.8492 - acc: 0.019 - ETA: 3s - loss: 4.8489 - acc: 0.019 - ETA: 3s - loss: 4.8480 - acc: 0.019 - ETA: 3s - loss: 4.8474 - acc: 0.019 - ETA: 3s - loss: 4.8464 - acc: 0.019 - ETA: 3s - loss: 4.8455 - acc: 0.020 - ETA: 3s - loss: 4.8453 - acc: 0.020 - ETA: 2s - loss: 4.8447 - acc: 0.020 - ETA: 2s - loss: 4.8451 - acc: 0.020 - ETA: 2s - loss: 4.8449 - acc: 0.020 - ETA: 2s - loss: 4.8448 - acc: 0.020 - ETA: 2s - loss: 4.8450 - acc: 0.020 - ETA: 1s - loss: 4.8449 - acc: 0.020 - ETA: 1s - loss: 4.8455 - acc: 0.020 - ETA: 1s - loss: 4.8448 - acc: 0.019 - ETA: 1s - loss: 4.8442 - acc: 0.020 - ETA: 1s - loss: 4.8431 - acc: 0.020 - ETA: 0s - loss: 4.8436 - acc: 0.020 - ETA: 0s - loss: 4.8424 - acc: 0.020 - ETA: 0s - loss: 4.8418 - acc: 0.020 - ETA: 0s - loss: 4.8416 - acc: 0.020 - ETA: 0s - loss: 4.8411 - acc: 0.020 - 65s 10ms/step - loss: 4.8408 - acc: 0.0204 - val_loss: 4.6827 - val_acc: 0.0431\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4280/6680 [==================>...........] - ETA: 55s - loss: 4.1147 - acc: 0.20 - ETA: 1:02 - loss: 4.0596 - acc: 0.125 - ETA: 1:01 - loss: 4.1203 - acc: 0.100 - ETA: 1:04 - loss: 4.1028 - acc: 0.112 - ETA: 1:05 - loss: 4.1351 - acc: 0.090 - ETA: 1:05 - loss: 4.1364 - acc: 0.083 - ETA: 1:07 - loss: 4.1485 - acc: 0.071 - ETA: 1:06 - loss: 4.1568 - acc: 0.087 - ETA: 1:05 - loss: 4.0975 - acc: 0.100 - ETA: 1:04 - loss: 4.1336 - acc: 0.105 - ETA: 1:04 - loss: 4.1358 - acc: 0.104 - ETA: 1:03 - loss: 4.1230 - acc: 0.108 - ETA: 1:02 - loss: 4.1162 - acc: 0.103 - ETA: 1:01 - loss: 4.1293 - acc: 0.103 - ETA: 1:01 - loss: 4.1397 - acc: 0.100 - ETA: 1:00 - loss: 4.1191 - acc: 0.100 - ETA: 59s - loss: 4.0961 - acc: 0.105 - ETA: 58s - loss: 4.0761 - acc: 0.10 - ETA: 58s - loss: 4.0712 - acc: 0.11 - ETA: 57s - loss: 4.0732 - acc: 0.11 - ETA: 57s - loss: 4.0602 - acc: 0.10 - ETA: 56s - loss: 4.0579 - acc: 0.10 - ETA: 56s - loss: 4.0542 - acc: 0.11 - ETA: 56s - loss: 4.0698 - acc: 0.10 - ETA: 55s - loss: 4.0727 - acc: 0.10 - ETA: 55s - loss: 4.0722 - acc: 0.10 - ETA: 54s - loss: 4.0731 - acc: 0.10 - ETA: 54s - loss: 4.0664 - acc: 0.10 - ETA: 53s - loss: 4.0558 - acc: 0.11 - ETA: 53s - loss: 4.0460 - acc: 0.11 - ETA: 53s - loss: 4.0407 - acc: 0.11 - ETA: 53s - loss: 4.0323 - acc: 0.11 - ETA: 53s - loss: 4.0307 - acc: 0.11 - ETA: 53s - loss: 4.0270 - acc: 0.12 - ETA: 53s - loss: 4.0096 - acc: 0.13 - ETA: 53s - loss: 4.0005 - acc: 0.13 - ETA: 53s - loss: 4.0031 - acc: 0.13 - ETA: 53s - loss: 3.9959 - acc: 0.13 - ETA: 53s - loss: 4.0036 - acc: 0.13 - ETA: 53s - loss: 3.9984 - acc: 0.13 - ETA: 53s - loss: 3.9955 - acc: 0.13 - ETA: 53s - loss: 3.9837 - acc: 0.14 - ETA: 54s - loss: 3.9806 - acc: 0.13 - ETA: 54s - loss: 3.9738 - acc: 0.14 - ETA: 54s - loss: 3.9566 - acc: 0.14 - ETA: 54s - loss: 3.9445 - acc: 0.14 - ETA: 54s - loss: 3.9594 - acc: 0.14 - ETA: 54s - loss: 3.9536 - acc: 0.14 - ETA: 54s - loss: 3.9593 - acc: 0.14 - ETA: 54s - loss: 3.9631 - acc: 0.14 - ETA: 54s - loss: 3.9582 - acc: 0.14 - ETA: 53s - loss: 3.9461 - acc: 0.15 - ETA: 53s - loss: 3.9415 - acc: 0.15 - ETA: 53s - loss: 3.9414 - acc: 0.14 - ETA: 53s - loss: 3.9409 - acc: 0.15 - ETA: 54s - loss: 3.9390 - acc: 0.15 - ETA: 53s - loss: 3.9326 - acc: 0.15 - ETA: 53s - loss: 3.9250 - acc: 0.15 - ETA: 53s - loss: 3.9244 - acc: 0.15 - ETA: 52s - loss: 3.9241 - acc: 0.15 - ETA: 53s - loss: 3.9316 - acc: 0.15 - ETA: 53s - loss: 3.9287 - acc: 0.15 - ETA: 53s - loss: 3.9281 - acc: 0.15 - ETA: 53s - loss: 3.9255 - acc: 0.15 - ETA: 53s - loss: 3.9175 - acc: 0.15 - ETA: 53s - loss: 3.9099 - acc: 0.16 - ETA: 53s - loss: 3.9074 - acc: 0.15 - ETA: 52s - loss: 3.8986 - acc: 0.16 - ETA: 52s - loss: 3.8944 - acc: 0.16 - ETA: 52s - loss: 3.8959 - acc: 0.16 - ETA: 51s - loss: 3.8896 - acc: 0.16 - ETA: 51s - loss: 3.8856 - acc: 0.16 - ETA: 51s - loss: 3.8857 - acc: 0.16 - ETA: 51s - loss: 3.8867 - acc: 0.16 - ETA: 50s - loss: 3.8797 - acc: 0.16 - ETA: 50s - loss: 3.8796 - acc: 0.16 - ETA: 50s - loss: 3.8739 - acc: 0.17 - ETA: 49s - loss: 3.8704 - acc: 0.17 - ETA: 49s - loss: 3.8720 - acc: 0.17 - ETA: 49s - loss: 3.8709 - acc: 0.17 - ETA: 49s - loss: 3.8716 - acc: 0.17 - ETA: 48s - loss: 3.8626 - acc: 0.17 - ETA: 48s - loss: 3.8632 - acc: 0.17 - ETA: 48s - loss: 3.8586 - acc: 0.17 - ETA: 48s - loss: 3.8585 - acc: 0.17 - ETA: 47s - loss: 3.8517 - acc: 0.17 - ETA: 47s - loss: 3.8470 - acc: 0.17 - ETA: 47s - loss: 3.8422 - acc: 0.17 - ETA: 47s - loss: 3.8329 - acc: 0.18 - ETA: 46s - loss: 3.8310 - acc: 0.18 - ETA: 46s - loss: 3.8300 - acc: 0.18 - ETA: 46s - loss: 3.8310 - acc: 0.18 - ETA: 45s - loss: 3.8340 - acc: 0.18 - ETA: 45s - loss: 3.8320 - acc: 0.18 - ETA: 45s - loss: 3.8272 - acc: 0.18 - ETA: 45s - loss: 3.8233 - acc: 0.18 - ETA: 44s - loss: 3.8160 - acc: 0.18 - ETA: 44s - loss: 3.8156 - acc: 0.18 - ETA: 44s - loss: 3.8141 - acc: 0.18 - ETA: 44s - loss: 3.8219 - acc: 0.18 - ETA: 44s - loss: 3.8236 - acc: 0.18 - ETA: 43s - loss: 3.8183 - acc: 0.18 - ETA: 43s - loss: 3.8167 - acc: 0.18 - ETA: 43s - loss: 3.8198 - acc: 0.18 - ETA: 43s - loss: 3.8233 - acc: 0.18 - ETA: 43s - loss: 3.8219 - acc: 0.18 - ETA: 43s - loss: 3.8176 - acc: 0.18 - ETA: 43s - loss: 3.8114 - acc: 0.18 - ETA: 42s - loss: 3.8114 - acc: 0.18 - ETA: 42s - loss: 3.8130 - acc: 0.18 - ETA: 42s - loss: 3.8160 - acc: 0.18 - ETA: 42s - loss: 3.8119 - acc: 0.18 - ETA: 42s - loss: 3.8125 - acc: 0.18 - ETA: 42s - loss: 3.8114 - acc: 0.18 - ETA: 42s - loss: 3.8130 - acc: 0.18 - ETA: 41s - loss: 3.8094 - acc: 0.18 - ETA: 41s - loss: 3.8065 - acc: 0.18 - ETA: 41s - loss: 3.8075 - acc: 0.18 - ETA: 41s - loss: 3.8034 - acc: 0.18 - ETA: 41s - loss: 3.8003 - acc: 0.18 - ETA: 41s - loss: 3.8007 - acc: 0.18 - ETA: 40s - loss: 3.8054 - acc: 0.18 - ETA: 40s - loss: 3.8027 - acc: 0.18 - ETA: 40s - loss: 3.8048 - acc: 0.18 - ETA: 40s - loss: 3.8050 - acc: 0.18 - ETA: 40s - loss: 3.8041 - acc: 0.18 - ETA: 39s - loss: 3.8005 - acc: 0.18 - ETA: 39s - loss: 3.8055 - acc: 0.18 - ETA: 39s - loss: 3.8030 - acc: 0.18 - ETA: 39s - loss: 3.8033 - acc: 0.18 - ETA: 39s - loss: 3.8021 - acc: 0.18 - ETA: 38s - loss: 3.8024 - acc: 0.18 - ETA: 38s - loss: 3.8004 - acc: 0.18 - ETA: 38s - loss: 3.7966 - acc: 0.18 - ETA: 38s - loss: 3.7947 - acc: 0.18 - ETA: 38s - loss: 3.7975 - acc: 0.18 - ETA: 38s - loss: 3.7925 - acc: 0.18 - ETA: 37s - loss: 3.7909 - acc: 0.18 - ETA: 37s - loss: 3.7864 - acc: 0.18 - ETA: 37s - loss: 3.7848 - acc: 0.18 - ETA: 37s - loss: 3.7828 - acc: 0.19 - ETA: 36s - loss: 3.7811 - acc: 0.19 - ETA: 36s - loss: 3.7780 - acc: 0.19 - ETA: 36s - loss: 3.7757 - acc: 0.19 - ETA: 36s - loss: 3.7712 - acc: 0.19 - ETA: 36s - loss: 3.7722 - acc: 0.19 - ETA: 36s - loss: 3.7707 - acc: 0.19 - ETA: 35s - loss: 3.7654 - acc: 0.19 - ETA: 35s - loss: 3.7647 - acc: 0.19 - ETA: 35s - loss: 3.7625 - acc: 0.19 - ETA: 35s - loss: 3.7602 - acc: 0.19 - ETA: 35s - loss: 3.7652 - acc: 0.19 - ETA: 34s - loss: 3.7662 - acc: 0.19 - ETA: 34s - loss: 3.7640 - acc: 0.19 - ETA: 34s - loss: 3.7650 - acc: 0.19 - ETA: 34s - loss: 3.7641 - acc: 0.19 - ETA: 34s - loss: 3.7598 - acc: 0.19 - ETA: 33s - loss: 3.7588 - acc: 0.19 - ETA: 33s - loss: 3.7550 - acc: 0.19 - ETA: 33s - loss: 3.7581 - acc: 0.19 - ETA: 33s - loss: 3.7571 - acc: 0.19 - ETA: 33s - loss: 3.7607 - acc: 0.19 - ETA: 32s - loss: 3.7635 - acc: 0.19 - ETA: 32s - loss: 3.7638 - acc: 0.19 - ETA: 32s - loss: 3.7655 - acc: 0.18 - ETA: 32s - loss: 3.7663 - acc: 0.18 - ETA: 32s - loss: 3.7636 - acc: 0.19 - ETA: 31s - loss: 3.7649 - acc: 0.19 - ETA: 31s - loss: 3.7607 - acc: 0.19 - ETA: 31s - loss: 3.7593 - acc: 0.19 - ETA: 31s - loss: 3.7591 - acc: 0.19 - ETA: 31s - loss: 3.7576 - acc: 0.19 - ETA: 30s - loss: 3.7568 - acc: 0.19 - ETA: 30s - loss: 3.7534 - acc: 0.19 - ETA: 30s - loss: 3.7527 - acc: 0.19 - ETA: 30s - loss: 3.7508 - acc: 0.19 - ETA: 30s - loss: 3.7472 - acc: 0.19 - ETA: 29s - loss: 3.7450 - acc: 0.19 - ETA: 29s - loss: 3.7441 - acc: 0.19 - ETA: 29s - loss: 3.7434 - acc: 0.19 - ETA: 29s - loss: 3.7400 - acc: 0.19 - ETA: 29s - loss: 3.7398 - acc: 0.19 - ETA: 28s - loss: 3.7396 - acc: 0.19 - ETA: 28s - loss: 3.7450 - acc: 0.19 - ETA: 28s - loss: 3.7454 - acc: 0.19 - ETA: 28s - loss: 3.7460 - acc: 0.19 - ETA: 27s - loss: 3.7450 - acc: 0.19 - ETA: 27s - loss: 3.7422 - acc: 0.19 - ETA: 27s - loss: 3.7411 - acc: 0.19 - ETA: 27s - loss: 3.7387 - acc: 0.19 - ETA: 27s - loss: 3.7393 - acc: 0.19 - ETA: 26s - loss: 3.7370 - acc: 0.19 - ETA: 26s - loss: 3.7355 - acc: 0.19 - ETA: 26s - loss: 3.7334 - acc: 0.19 - ETA: 26s - loss: 3.7282 - acc: 0.19 - ETA: 26s - loss: 3.7273 - acc: 0.19 - ETA: 25s - loss: 3.7271 - acc: 0.19 - ETA: 25s - loss: 3.7226 - acc: 0.20 - ETA: 25s - loss: 3.7225 - acc: 0.19 - ETA: 25s - loss: 3.7178 - acc: 0.20 - ETA: 25s - loss: 3.7145 - acc: 0.20 - ETA: 24s - loss: 3.7147 - acc: 0.20 - ETA: 24s - loss: 3.7115 - acc: 0.20 - ETA: 24s - loss: 3.7086 - acc: 0.20 - ETA: 24s - loss: 3.7049 - acc: 0.20 - ETA: 24s - loss: 3.6979 - acc: 0.20 - ETA: 23s - loss: 3.7023 - acc: 0.20 - ETA: 23s - loss: 3.7023 - acc: 0.20 - ETA: 23s - loss: 3.7031 - acc: 0.20 - ETA: 23s - loss: 3.7019 - acc: 0.20 - ETA: 23s - loss: 3.6995 - acc: 0.20 - ETA: 22s - loss: 3.7000 - acc: 0.20 - ETA: 22s - loss: 3.7028 - acc: 0.20 - ETA: 22s - loss: 3.7002 - acc: 0.2033"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 22s - loss: 3.6999 - acc: 0.20 - ETA: 22s - loss: 3.6965 - acc: 0.20 - ETA: 21s - loss: 3.6956 - acc: 0.20 - ETA: 21s - loss: 3.6954 - acc: 0.20 - ETA: 21s - loss: 3.6950 - acc: 0.20 - ETA: 21s - loss: 3.6949 - acc: 0.20 - ETA: 21s - loss: 3.6945 - acc: 0.20 - ETA: 20s - loss: 3.6907 - acc: 0.20 - ETA: 20s - loss: 3.6863 - acc: 0.20 - ETA: 20s - loss: 3.6847 - acc: 0.20 - ETA: 20s - loss: 3.6796 - acc: 0.20 - ETA: 20s - loss: 3.6792 - acc: 0.20 - ETA: 19s - loss: 3.6770 - acc: 0.20 - ETA: 19s - loss: 3.6721 - acc: 0.20 - ETA: 19s - loss: 3.6733 - acc: 0.20 - ETA: 19s - loss: 3.6722 - acc: 0.20 - ETA: 19s - loss: 3.6730 - acc: 0.20 - ETA: 19s - loss: 3.6754 - acc: 0.20 - ETA: 18s - loss: 3.6733 - acc: 0.20 - ETA: 18s - loss: 3.6807 - acc: 0.20 - ETA: 18s - loss: 3.6805 - acc: 0.20 - ETA: 18s - loss: 3.6783 - acc: 0.20 - ETA: 18s - loss: 3.6813 - acc: 0.20 - ETA: 17s - loss: 3.6814 - acc: 0.20 - ETA: 17s - loss: 3.6798 - acc: 0.20 - ETA: 17s - loss: 3.6809 - acc: 0.20 - ETA: 17s - loss: 3.6820 - acc: 0.20 - ETA: 17s - loss: 3.6812 - acc: 0.20 - ETA: 16s - loss: 3.6798 - acc: 0.20 - ETA: 16s - loss: 3.6787 - acc: 0.20 - ETA: 16s - loss: 3.6765 - acc: 0.20 - ETA: 16s - loss: 3.6788 - acc: 0.20 - ETA: 16s - loss: 3.6813 - acc: 0.20 - ETA: 16s - loss: 3.6794 - acc: 0.20 - ETA: 15s - loss: 3.6780 - acc: 0.20 - ETA: 15s - loss: 3.6760 - acc: 0.20 - ETA: 15s - loss: 3.6720 - acc: 0.20 - ETA: 15s - loss: 3.6732 - acc: 0.20 - ETA: 15s - loss: 3.6729 - acc: 0.20 - ETA: 14s - loss: 3.6741 - acc: 0.20 - ETA: 14s - loss: 3.6739 - acc: 0.20 - ETA: 14s - loss: 3.6716 - acc: 0.20 - ETA: 14s - loss: 3.6704 - acc: 0.20 - ETA: 14s - loss: 3.6697 - acc: 0.20 - ETA: 13s - loss: 3.6694 - acc: 0.20 - ETA: 13s - loss: 3.6707 - acc: 0.20 - ETA: 13s - loss: 3.6702 - acc: 0.20 - ETA: 13s - loss: 3.6706 - acc: 0.20 - ETA: 13s - loss: 3.6703 - acc: 0.20 - ETA: 12s - loss: 3.6693 - acc: 0.20 - ETA: 12s - loss: 3.6661 - acc: 0.20 - ETA: 12s - loss: 3.6634 - acc: 0.20 - ETA: 12s - loss: 3.6632 - acc: 0.20 - ETA: 12s - loss: 3.6651 - acc: 0.20 - ETA: 12s - loss: 3.6646 - acc: 0.20 - ETA: 11s - loss: 3.6630 - acc: 0.20 - ETA: 11s - loss: 3.6623 - acc: 0.20 - ETA: 11s - loss: 3.6624 - acc: 0.20 - ETA: 11s - loss: 3.6622 - acc: 0.20 - ETA: 11s - loss: 3.6578 - acc: 0.20 - ETA: 10s - loss: 3.6592 - acc: 0.20 - ETA: 10s - loss: 3.6587 - acc: 0.20 - ETA: 10s - loss: 3.6579 - acc: 0.20 - ETA: 10s - loss: 3.6583 - acc: 0.20 - ETA: 10s - loss: 3.6581 - acc: 0.20 - ETA: 10s - loss: 3.6580 - acc: 0.20 - ETA: 9s - loss: 3.6579 - acc: 0.2077 - ETA: 9s - loss: 3.6575 - acc: 0.208 - ETA: 9s - loss: 3.6574 - acc: 0.208 - ETA: 9s - loss: 3.6590 - acc: 0.207 - ETA: 9s - loss: 3.6575 - acc: 0.208 - ETA: 8s - loss: 3.6580 - acc: 0.208 - ETA: 8s - loss: 3.6569 - acc: 0.208 - ETA: 8s - loss: 3.6579 - acc: 0.208 - ETA: 8s - loss: 3.6574 - acc: 0.208 - ETA: 8s - loss: 3.6576 - acc: 0.208 - ETA: 8s - loss: 3.6563 - acc: 0.207 - ETA: 7s - loss: 3.6532 - acc: 0.208 - ETA: 7s - loss: 3.6532 - acc: 0.208 - ETA: 7s - loss: 3.6532 - acc: 0.208 - ETA: 7s - loss: 3.6565 - acc: 0.208 - ETA: 7s - loss: 3.6538 - acc: 0.209 - ETA: 6s - loss: 3.6513 - acc: 0.209 - ETA: 6s - loss: 3.6518 - acc: 0.208 - ETA: 6s - loss: 3.6505 - acc: 0.208 - ETA: 6s - loss: 3.6507 - acc: 0.208 - ETA: 6s - loss: 3.6486 - acc: 0.209 - ETA: 5s - loss: 3.6469 - acc: 0.209 - ETA: 5s - loss: 3.6466 - acc: 0.209 - ETA: 5s - loss: 3.6453 - acc: 0.209 - ETA: 5s - loss: 3.6456 - acc: 0.209 - ETA: 5s - loss: 3.6456 - acc: 0.209 - ETA: 5s - loss: 3.6438 - acc: 0.209 - ETA: 4s - loss: 3.6434 - acc: 0.209 - ETA: 4s - loss: 3.6433 - acc: 0.209 - ETA: 4s - loss: 3.6419 - acc: 0.210 - ETA: 4s - loss: 3.6414 - acc: 0.210 - ETA: 4s - loss: 3.6419 - acc: 0.209 - ETA: 3s - loss: 3.6410 - acc: 0.209 - ETA: 3s - loss: 3.6391 - acc: 0.209 - ETA: 3s - loss: 3.6399 - acc: 0.209 - ETA: 3s - loss: 3.6403 - acc: 0.210 - ETA: 3s - loss: 3.6431 - acc: 0.209 - ETA: 2s - loss: 3.6432 - acc: 0.209 - ETA: 2s - loss: 3.6424 - acc: 0.210 - ETA: 2s - loss: 3.6420 - acc: 0.210 - ETA: 2s - loss: 3.6425 - acc: 0.209 - ETA: 2s - loss: 3.6411 - acc: 0.210 - ETA: 2s - loss: 3.6421 - acc: 0.209 - ETA: 1s - loss: 3.6420 - acc: 0.209 - ETA: 1s - loss: 3.6409 - acc: 0.209 - ETA: 1s - loss: 3.6415 - acc: 0.209 - ETA: 1s - loss: 3.6406 - acc: 0.209 - ETA: 1s - loss: 3.6431 - acc: 0.209 - ETA: 0s - loss: 3.6433 - acc: 0.209 - ETA: 0s - loss: 3.6428 - acc: 0.209 - ETA: 0s - loss: 3.6425 - acc: 0.209 - ETA: 0s - loss: 3.6405 - acc: 0.209 - ETA: 0s - loss: 3.6417 - acc: 0.209 - 64s 10ms/step - loss: 3.6397 - acc: 0.2099 - val_loss: 4.9835 - val_acc: 0.0647\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4300/6680 [==================>...........] - ETA: 49s - loss: 2.4953 - acc: 0.60 - ETA: 50s - loss: 2.1756 - acc: 0.60 - ETA: 51s - loss: 1.9312 - acc: 0.65 - ETA: 51s - loss: 1.9635 - acc: 0.62 - ETA: 50s - loss: 1.9174 - acc: 0.65 - ETA: 50s - loss: 1.8683 - acc: 0.65 - ETA: 50s - loss: 2.0190 - acc: 0.61 - ETA: 50s - loss: 2.0333 - acc: 0.61 - ETA: 51s - loss: 2.0547 - acc: 0.60 - ETA: 51s - loss: 1.9792 - acc: 0.62 - ETA: 52s - loss: 2.0364 - acc: 0.60 - ETA: 53s - loss: 2.0015 - acc: 0.61 - ETA: 54s - loss: 1.9431 - acc: 0.62 - ETA: 54s - loss: 1.8940 - acc: 0.63 - ETA: 54s - loss: 1.9154 - acc: 0.62 - ETA: 54s - loss: 1.8979 - acc: 0.61 - ETA: 54s - loss: 1.9047 - acc: 0.60 - ETA: 54s - loss: 1.8949 - acc: 0.61 - ETA: 53s - loss: 1.8699 - acc: 0.62 - ETA: 53s - loss: 1.8640 - acc: 0.62 - ETA: 53s - loss: 1.8524 - acc: 0.62 - ETA: 52s - loss: 1.8310 - acc: 0.62 - ETA: 52s - loss: 1.8172 - acc: 0.62 - ETA: 53s - loss: 1.8164 - acc: 0.62 - ETA: 53s - loss: 1.8078 - acc: 0.61 - ETA: 53s - loss: 1.7947 - acc: 0.61 - ETA: 53s - loss: 1.8190 - acc: 0.60 - ETA: 53s - loss: 1.8124 - acc: 0.60 - ETA: 54s - loss: 1.8090 - acc: 0.60 - ETA: 54s - loss: 1.8038 - acc: 0.60 - ETA: 53s - loss: 1.8009 - acc: 0.60 - ETA: 53s - loss: 1.7823 - acc: 0.60 - ETA: 53s - loss: 1.7828 - acc: 0.60 - ETA: 52s - loss: 1.7988 - acc: 0.60 - ETA: 52s - loss: 1.7989 - acc: 0.60 - ETA: 52s - loss: 1.8112 - acc: 0.59 - ETA: 51s - loss: 1.7922 - acc: 0.60 - ETA: 51s - loss: 1.7900 - acc: 0.60 - ETA: 51s - loss: 1.7948 - acc: 0.59 - ETA: 51s - loss: 1.7969 - acc: 0.59 - ETA: 50s - loss: 1.7970 - acc: 0.59 - ETA: 50s - loss: 1.7951 - acc: 0.60 - ETA: 50s - loss: 1.7796 - acc: 0.60 - ETA: 50s - loss: 1.8031 - acc: 0.60 - ETA: 49s - loss: 1.8229 - acc: 0.59 - ETA: 49s - loss: 1.8067 - acc: 0.59 - ETA: 49s - loss: 1.7985 - acc: 0.59 - ETA: 49s - loss: 1.7926 - acc: 0.59 - ETA: 49s - loss: 1.8186 - acc: 0.59 - ETA: 49s - loss: 1.8056 - acc: 0.59 - ETA: 50s - loss: 1.8158 - acc: 0.59 - ETA: 50s - loss: 1.8128 - acc: 0.59 - ETA: 50s - loss: 1.8122 - acc: 0.59 - ETA: 51s - loss: 1.8092 - acc: 0.59 - ETA: 51s - loss: 1.8123 - acc: 0.59 - ETA: 51s - loss: 1.8108 - acc: 0.59 - ETA: 51s - loss: 1.8000 - acc: 0.59 - ETA: 50s - loss: 1.8054 - acc: 0.59 - ETA: 50s - loss: 1.8034 - acc: 0.59 - ETA: 50s - loss: 1.8042 - acc: 0.59 - ETA: 49s - loss: 1.8140 - acc: 0.59 - ETA: 49s - loss: 1.8080 - acc: 0.59 - ETA: 49s - loss: 1.8049 - acc: 0.59 - ETA: 49s - loss: 1.8016 - acc: 0.59 - ETA: 49s - loss: 1.8084 - acc: 0.59 - ETA: 49s - loss: 1.8001 - acc: 0.59 - ETA: 48s - loss: 1.7920 - acc: 0.59 - ETA: 48s - loss: 1.7881 - acc: 0.59 - ETA: 48s - loss: 1.7811 - acc: 0.59 - ETA: 48s - loss: 1.7727 - acc: 0.59 - ETA: 48s - loss: 1.7697 - acc: 0.59 - ETA: 47s - loss: 1.7586 - acc: 0.60 - ETA: 47s - loss: 1.7517 - acc: 0.60 - ETA: 47s - loss: 1.7553 - acc: 0.60 - ETA: 47s - loss: 1.7576 - acc: 0.59 - ETA: 46s - loss: 1.7604 - acc: 0.59 - ETA: 46s - loss: 1.7601 - acc: 0.59 - ETA: 46s - loss: 1.7548 - acc: 0.60 - ETA: 46s - loss: 1.7615 - acc: 0.60 - ETA: 45s - loss: 1.7701 - acc: 0.59 - ETA: 45s - loss: 1.7642 - acc: 0.60 - ETA: 45s - loss: 1.7605 - acc: 0.60 - ETA: 45s - loss: 1.7549 - acc: 0.60 - ETA: 45s - loss: 1.7456 - acc: 0.60 - ETA: 44s - loss: 1.7503 - acc: 0.60 - ETA: 44s - loss: 1.7521 - acc: 0.60 - ETA: 44s - loss: 1.7510 - acc: 0.60 - ETA: 44s - loss: 1.7471 - acc: 0.60 - ETA: 43s - loss: 1.7469 - acc: 0.60 - ETA: 43s - loss: 1.7489 - acc: 0.60 - ETA: 43s - loss: 1.7426 - acc: 0.60 - ETA: 43s - loss: 1.7479 - acc: 0.60 - ETA: 43s - loss: 1.7535 - acc: 0.59 - ETA: 42s - loss: 1.7462 - acc: 0.60 - ETA: 42s - loss: 1.7407 - acc: 0.60 - ETA: 42s - loss: 1.7375 - acc: 0.60 - ETA: 42s - loss: 1.7350 - acc: 0.60 - ETA: 42s - loss: 1.7302 - acc: 0.60 - ETA: 42s - loss: 1.7294 - acc: 0.60 - ETA: 41s - loss: 1.7333 - acc: 0.60 - ETA: 41s - loss: 1.7430 - acc: 0.60 - ETA: 41s - loss: 1.7433 - acc: 0.60 - ETA: 41s - loss: 1.7446 - acc: 0.60 - ETA: 40s - loss: 1.7437 - acc: 0.60 - ETA: 40s - loss: 1.7395 - acc: 0.60 - ETA: 40s - loss: 1.7375 - acc: 0.60 - ETA: 40s - loss: 1.7353 - acc: 0.60 - ETA: 40s - loss: 1.7303 - acc: 0.60 - ETA: 39s - loss: 1.7283 - acc: 0.60 - ETA: 39s - loss: 1.7219 - acc: 0.60 - ETA: 39s - loss: 1.7182 - acc: 0.60 - ETA: 39s - loss: 1.7158 - acc: 0.60 - ETA: 39s - loss: 1.7140 - acc: 0.60 - ETA: 38s - loss: 1.7184 - acc: 0.60 - ETA: 38s - loss: 1.7211 - acc: 0.60 - ETA: 38s - loss: 1.7192 - acc: 0.60 - ETA: 38s - loss: 1.7215 - acc: 0.60 - ETA: 38s - loss: 1.7184 - acc: 0.60 - ETA: 37s - loss: 1.7284 - acc: 0.60 - ETA: 37s - loss: 1.7334 - acc: 0.60 - ETA: 37s - loss: 1.7351 - acc: 0.60 - ETA: 37s - loss: 1.7324 - acc: 0.60 - ETA: 37s - loss: 1.7297 - acc: 0.60 - ETA: 36s - loss: 1.7299 - acc: 0.60 - ETA: 36s - loss: 1.7300 - acc: 0.60 - ETA: 36s - loss: 1.7268 - acc: 0.60 - ETA: 36s - loss: 1.7278 - acc: 0.60 - ETA: 36s - loss: 1.7216 - acc: 0.60 - ETA: 36s - loss: 1.7217 - acc: 0.60 - ETA: 35s - loss: 1.7214 - acc: 0.60 - ETA: 35s - loss: 1.7235 - acc: 0.60 - ETA: 35s - loss: 1.7325 - acc: 0.60 - ETA: 35s - loss: 1.7415 - acc: 0.60 - ETA: 35s - loss: 1.7329 - acc: 0.60 - ETA: 34s - loss: 1.7340 - acc: 0.60 - ETA: 34s - loss: 1.7415 - acc: 0.60 - ETA: 34s - loss: 1.7402 - acc: 0.60 - ETA: 34s - loss: 1.7419 - acc: 0.60 - ETA: 34s - loss: 1.7391 - acc: 0.60 - ETA: 33s - loss: 1.7352 - acc: 0.60 - ETA: 33s - loss: 1.7304 - acc: 0.60 - ETA: 33s - loss: 1.7307 - acc: 0.60 - ETA: 33s - loss: 1.7259 - acc: 0.60 - ETA: 33s - loss: 1.7234 - acc: 0.60 - ETA: 32s - loss: 1.7243 - acc: 0.60 - ETA: 32s - loss: 1.7179 - acc: 0.61 - ETA: 32s - loss: 1.7172 - acc: 0.60 - ETA: 32s - loss: 1.7125 - acc: 0.61 - ETA: 32s - loss: 1.7143 - acc: 0.61 - ETA: 32s - loss: 1.7131 - acc: 0.61 - ETA: 31s - loss: 1.7161 - acc: 0.60 - ETA: 31s - loss: 1.7144 - acc: 0.60 - ETA: 31s - loss: 1.7118 - acc: 0.60 - ETA: 31s - loss: 1.7117 - acc: 0.60 - ETA: 31s - loss: 1.7177 - acc: 0.60 - ETA: 30s - loss: 1.7154 - acc: 0.60 - ETA: 30s - loss: 1.7107 - acc: 0.61 - ETA: 30s - loss: 1.7136 - acc: 0.60 - ETA: 30s - loss: 1.7126 - acc: 0.60 - ETA: 30s - loss: 1.7120 - acc: 0.61 - ETA: 29s - loss: 1.7100 - acc: 0.60 - ETA: 29s - loss: 1.7114 - acc: 0.60 - ETA: 29s - loss: 1.7149 - acc: 0.60 - ETA: 29s - loss: 1.7121 - acc: 0.60 - ETA: 29s - loss: 1.7155 - acc: 0.60 - ETA: 29s - loss: 1.7134 - acc: 0.60 - ETA: 28s - loss: 1.7142 - acc: 0.60 - ETA: 28s - loss: 1.7092 - acc: 0.60 - ETA: 28s - loss: 1.7095 - acc: 0.60 - ETA: 28s - loss: 1.7086 - acc: 0.60 - ETA: 28s - loss: 1.7089 - acc: 0.60 - ETA: 27s - loss: 1.7122 - acc: 0.60 - ETA: 27s - loss: 1.7230 - acc: 0.60 - ETA: 27s - loss: 1.7301 - acc: 0.60 - ETA: 27s - loss: 1.7309 - acc: 0.60 - ETA: 27s - loss: 1.7295 - acc: 0.60 - ETA: 27s - loss: 1.7328 - acc: 0.60 - ETA: 26s - loss: 1.7300 - acc: 0.60 - ETA: 26s - loss: 1.7283 - acc: 0.60 - ETA: 26s - loss: 1.7278 - acc: 0.60 - ETA: 26s - loss: 1.7209 - acc: 0.60 - ETA: 26s - loss: 1.7192 - acc: 0.60 - ETA: 25s - loss: 1.7198 - acc: 0.60 - ETA: 25s - loss: 1.7201 - acc: 0.60 - ETA: 25s - loss: 1.7160 - acc: 0.60 - ETA: 25s - loss: 1.7143 - acc: 0.60 - ETA: 25s - loss: 1.7104 - acc: 0.61 - ETA: 25s - loss: 1.7101 - acc: 0.61 - ETA: 24s - loss: 1.7120 - acc: 0.60 - ETA: 24s - loss: 1.7086 - acc: 0.61 - ETA: 24s - loss: 1.7070 - acc: 0.60 - ETA: 24s - loss: 1.7074 - acc: 0.60 - ETA: 24s - loss: 1.7086 - acc: 0.60 - ETA: 24s - loss: 1.7087 - acc: 0.60 - ETA: 23s - loss: 1.7088 - acc: 0.60 - ETA: 23s - loss: 1.7066 - acc: 0.60 - ETA: 23s - loss: 1.7075 - acc: 0.60 - ETA: 23s - loss: 1.7088 - acc: 0.60 - ETA: 23s - loss: 1.7088 - acc: 0.60 - ETA: 23s - loss: 1.7081 - acc: 0.60 - ETA: 22s - loss: 1.7064 - acc: 0.60 - ETA: 22s - loss: 1.7081 - acc: 0.60 - ETA: 22s - loss: 1.7069 - acc: 0.60 - ETA: 22s - loss: 1.7026 - acc: 0.61 - ETA: 22s - loss: 1.7029 - acc: 0.60 - ETA: 21s - loss: 1.7021 - acc: 0.61 - ETA: 21s - loss: 1.7050 - acc: 0.60 - ETA: 21s - loss: 1.7068 - acc: 0.60 - ETA: 21s - loss: 1.7054 - acc: 0.60 - ETA: 21s - loss: 1.7080 - acc: 0.60 - ETA: 21s - loss: 1.7076 - acc: 0.60 - ETA: 20s - loss: 1.7083 - acc: 0.60 - ETA: 20s - loss: 1.7059 - acc: 0.60 - ETA: 20s - loss: 1.7064 - acc: 0.60 - ETA: 20s - loss: 1.7078 - acc: 0.6079"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 20s - loss: 1.7056 - acc: 0.60 - ETA: 20s - loss: 1.7058 - acc: 0.60 - ETA: 19s - loss: 1.7054 - acc: 0.60 - ETA: 19s - loss: 1.7081 - acc: 0.60 - ETA: 19s - loss: 1.7093 - acc: 0.60 - ETA: 19s - loss: 1.7059 - acc: 0.60 - ETA: 19s - loss: 1.7036 - acc: 0.60 - ETA: 18s - loss: 1.7032 - acc: 0.60 - ETA: 18s - loss: 1.7027 - acc: 0.60 - ETA: 18s - loss: 1.7049 - acc: 0.60 - ETA: 18s - loss: 1.7029 - acc: 0.60 - ETA: 18s - loss: 1.7057 - acc: 0.60 - ETA: 18s - loss: 1.7055 - acc: 0.60 - ETA: 17s - loss: 1.7052 - acc: 0.60 - ETA: 17s - loss: 1.7063 - acc: 0.60 - ETA: 17s - loss: 1.7058 - acc: 0.60 - ETA: 17s - loss: 1.7060 - acc: 0.60 - ETA: 17s - loss: 1.7061 - acc: 0.60 - ETA: 17s - loss: 1.7075 - acc: 0.60 - ETA: 16s - loss: 1.7073 - acc: 0.60 - ETA: 16s - loss: 1.7079 - acc: 0.60 - ETA: 16s - loss: 1.7056 - acc: 0.60 - ETA: 16s - loss: 1.7035 - acc: 0.60 - ETA: 16s - loss: 1.7018 - acc: 0.60 - ETA: 16s - loss: 1.7012 - acc: 0.60 - ETA: 15s - loss: 1.7005 - acc: 0.60 - ETA: 15s - loss: 1.7000 - acc: 0.60 - ETA: 15s - loss: 1.6967 - acc: 0.60 - ETA: 15s - loss: 1.7009 - acc: 0.60 - ETA: 15s - loss: 1.6982 - acc: 0.61 - ETA: 15s - loss: 1.6977 - acc: 0.61 - ETA: 14s - loss: 1.6981 - acc: 0.60 - ETA: 14s - loss: 1.6970 - acc: 0.60 - ETA: 14s - loss: 1.7002 - acc: 0.60 - ETA: 14s - loss: 1.7043 - acc: 0.60 - ETA: 14s - loss: 1.7036 - acc: 0.60 - ETA: 14s - loss: 1.7008 - acc: 0.60 - ETA: 13s - loss: 1.7009 - acc: 0.60 - ETA: 13s - loss: 1.6994 - acc: 0.60 - ETA: 13s - loss: 1.6999 - acc: 0.60 - ETA: 13s - loss: 1.7025 - acc: 0.60 - ETA: 13s - loss: 1.7050 - acc: 0.60 - ETA: 13s - loss: 1.7044 - acc: 0.60 - ETA: 12s - loss: 1.7057 - acc: 0.60 - ETA: 12s - loss: 1.7060 - acc: 0.60 - ETA: 12s - loss: 1.7075 - acc: 0.60 - ETA: 12s - loss: 1.7104 - acc: 0.60 - ETA: 12s - loss: 1.7108 - acc: 0.60 - ETA: 12s - loss: 1.7112 - acc: 0.60 - ETA: 11s - loss: 1.7118 - acc: 0.60 - ETA: 11s - loss: 1.7130 - acc: 0.60 - ETA: 11s - loss: 1.7102 - acc: 0.60 - ETA: 11s - loss: 1.7082 - acc: 0.60 - ETA: 11s - loss: 1.7078 - acc: 0.60 - ETA: 10s - loss: 1.7087 - acc: 0.60 - ETA: 10s - loss: 1.7082 - acc: 0.60 - ETA: 10s - loss: 1.7074 - acc: 0.60 - ETA: 10s - loss: 1.7069 - acc: 0.60 - ETA: 10s - loss: 1.7091 - acc: 0.60 - ETA: 10s - loss: 1.7094 - acc: 0.60 - ETA: 9s - loss: 1.7104 - acc: 0.6053 - ETA: 9s - loss: 1.7149 - acc: 0.604 - ETA: 9s - loss: 1.7158 - acc: 0.604 - ETA: 9s - loss: 1.7128 - acc: 0.604 - ETA: 9s - loss: 1.7160 - acc: 0.603 - ETA: 9s - loss: 1.7135 - acc: 0.604 - ETA: 8s - loss: 1.7162 - acc: 0.604 - ETA: 8s - loss: 1.7134 - acc: 0.604 - ETA: 8s - loss: 1.7107 - acc: 0.605 - ETA: 8s - loss: 1.7085 - acc: 0.606 - ETA: 8s - loss: 1.7121 - acc: 0.605 - ETA: 8s - loss: 1.7115 - acc: 0.605 - ETA: 7s - loss: 1.7122 - acc: 0.605 - ETA: 7s - loss: 1.7119 - acc: 0.605 - ETA: 7s - loss: 1.7138 - acc: 0.605 - ETA: 7s - loss: 1.7149 - acc: 0.605 - ETA: 7s - loss: 1.7153 - acc: 0.605 - ETA: 7s - loss: 1.7138 - acc: 0.605 - ETA: 6s - loss: 1.7141 - acc: 0.605 - ETA: 6s - loss: 1.7139 - acc: 0.605 - ETA: 6s - loss: 1.7126 - acc: 0.605 - ETA: 6s - loss: 1.7107 - acc: 0.605 - ETA: 6s - loss: 1.7102 - acc: 0.606 - ETA: 5s - loss: 1.7117 - acc: 0.605 - ETA: 5s - loss: 1.7125 - acc: 0.605 - ETA: 5s - loss: 1.7121 - acc: 0.605 - ETA: 5s - loss: 1.7126 - acc: 0.605 - ETA: 5s - loss: 1.7127 - acc: 0.605 - ETA: 5s - loss: 1.7137 - acc: 0.605 - ETA: 4s - loss: 1.7116 - acc: 0.605 - ETA: 4s - loss: 1.7114 - acc: 0.605 - ETA: 4s - loss: 1.7112 - acc: 0.605 - ETA: 4s - loss: 1.7121 - acc: 0.605 - ETA: 4s - loss: 1.7147 - acc: 0.604 - ETA: 4s - loss: 1.7141 - acc: 0.604 - ETA: 3s - loss: 1.7157 - acc: 0.604 - ETA: 3s - loss: 1.7166 - acc: 0.604 - ETA: 3s - loss: 1.7173 - acc: 0.603 - ETA: 3s - loss: 1.7160 - acc: 0.604 - ETA: 3s - loss: 1.7160 - acc: 0.604 - ETA: 3s - loss: 1.7170 - acc: 0.603 - ETA: 2s - loss: 1.7162 - acc: 0.604 - ETA: 2s - loss: 1.7141 - acc: 0.604 - ETA: 2s - loss: 1.7131 - acc: 0.604 - ETA: 2s - loss: 1.7133 - acc: 0.604 - ETA: 2s - loss: 1.7117 - acc: 0.604 - ETA: 2s - loss: 1.7121 - acc: 0.604 - ETA: 1s - loss: 1.7110 - acc: 0.604 - ETA: 1s - loss: 1.7106 - acc: 0.604 - ETA: 1s - loss: 1.7085 - acc: 0.605 - ETA: 1s - loss: 1.7071 - acc: 0.605 - ETA: 1s - loss: 1.7060 - acc: 0.605 - ETA: 1s - loss: 1.7059 - acc: 0.605 - ETA: 0s - loss: 1.7052 - acc: 0.605 - ETA: 0s - loss: 1.7042 - acc: 0.605 - ETA: 0s - loss: 1.7040 - acc: 0.606 - ETA: 0s - loss: 1.7062 - acc: 0.605 - ETA: 0s - loss: 1.7064 - acc: 0.605 - 59s 9ms/step - loss: 1.7089 - acc: 0.6049 - val_loss: 6.0971 - val_acc: 0.0527\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4300/6680 [==================>...........] - ETA: 52s - loss: 0.6659 - acc: 0.80 - ETA: 51s - loss: 0.8109 - acc: 0.80 - ETA: 51s - loss: 0.8246 - acc: 0.81 - ETA: 54s - loss: 0.8983 - acc: 0.80 - ETA: 53s - loss: 0.9374 - acc: 0.77 - ETA: 53s - loss: 0.8804 - acc: 0.79 - ETA: 53s - loss: 0.8303 - acc: 0.80 - ETA: 52s - loss: 0.7908 - acc: 0.82 - ETA: 52s - loss: 0.7919 - acc: 0.82 - ETA: 52s - loss: 0.8259 - acc: 0.83 - ETA: 52s - loss: 0.8419 - acc: 0.82 - ETA: 53s - loss: 0.8379 - acc: 0.82 - ETA: 52s - loss: 0.8376 - acc: 0.82 - ETA: 52s - loss: 0.7984 - acc: 0.83 - ETA: 52s - loss: 0.7678 - acc: 0.83 - ETA: 52s - loss: 0.7939 - acc: 0.82 - ETA: 52s - loss: 0.7846 - acc: 0.82 - ETA: 51s - loss: 0.7662 - acc: 0.83 - ETA: 51s - loss: 0.7619 - acc: 0.83 - ETA: 51s - loss: 0.7427 - acc: 0.83 - ETA: 50s - loss: 0.7269 - acc: 0.83 - ETA: 50s - loss: 0.7234 - acc: 0.83 - ETA: 50s - loss: 0.7293 - acc: 0.83 - ETA: 50s - loss: 0.7329 - acc: 0.83 - ETA: 50s - loss: 0.7207 - acc: 0.83 - ETA: 49s - loss: 0.7042 - acc: 0.84 - ETA: 49s - loss: 0.6873 - acc: 0.84 - ETA: 49s - loss: 0.6787 - acc: 0.84 - ETA: 49s - loss: 0.6794 - acc: 0.84 - ETA: 49s - loss: 0.6642 - acc: 0.84 - ETA: 49s - loss: 0.6594 - acc: 0.84 - ETA: 49s - loss: 0.6562 - acc: 0.84 - ETA: 49s - loss: 0.6540 - acc: 0.84 - ETA: 49s - loss: 0.6507 - acc: 0.84 - ETA: 48s - loss: 0.6403 - acc: 0.84 - ETA: 48s - loss: 0.6530 - acc: 0.84 - ETA: 48s - loss: 0.6731 - acc: 0.84 - ETA: 48s - loss: 0.6941 - acc: 0.83 - ETA: 48s - loss: 0.6980 - acc: 0.83 - ETA: 48s - loss: 0.7057 - acc: 0.83 - ETA: 48s - loss: 0.7204 - acc: 0.83 - ETA: 47s - loss: 0.7192 - acc: 0.83 - ETA: 47s - loss: 0.7068 - acc: 0.83 - ETA: 47s - loss: 0.7121 - acc: 0.83 - ETA: 47s - loss: 0.7034 - acc: 0.83 - ETA: 47s - loss: 0.6962 - acc: 0.83 - ETA: 47s - loss: 0.6959 - acc: 0.83 - ETA: 46s - loss: 0.6888 - acc: 0.84 - ETA: 46s - loss: 0.6832 - acc: 0.84 - ETA: 46s - loss: 0.6792 - acc: 0.84 - ETA: 46s - loss: 0.6774 - acc: 0.84 - ETA: 46s - loss: 0.6730 - acc: 0.84 - ETA: 46s - loss: 0.6674 - acc: 0.84 - ETA: 45s - loss: 0.6671 - acc: 0.84 - ETA: 45s - loss: 0.6723 - acc: 0.84 - ETA: 45s - loss: 0.6698 - acc: 0.84 - ETA: 45s - loss: 0.6717 - acc: 0.84 - ETA: 45s - loss: 0.6813 - acc: 0.84 - ETA: 45s - loss: 0.6770 - acc: 0.84 - ETA: 45s - loss: 0.6776 - acc: 0.84 - ETA: 44s - loss: 0.6766 - acc: 0.84 - ETA: 44s - loss: 0.6767 - acc: 0.84 - ETA: 44s - loss: 0.6742 - acc: 0.84 - ETA: 44s - loss: 0.6719 - acc: 0.84 - ETA: 44s - loss: 0.6669 - acc: 0.84 - ETA: 44s - loss: 0.6650 - acc: 0.84 - ETA: 43s - loss: 0.6560 - acc: 0.84 - ETA: 43s - loss: 0.6520 - acc: 0.84 - ETA: 43s - loss: 0.6487 - acc: 0.84 - ETA: 43s - loss: 0.6585 - acc: 0.84 - ETA: 44s - loss: 0.6542 - acc: 0.84 - ETA: 44s - loss: 0.6537 - acc: 0.84 - ETA: 44s - loss: 0.6489 - acc: 0.85 - ETA: 44s - loss: 0.6466 - acc: 0.85 - ETA: 44s - loss: 0.6438 - acc: 0.85 - ETA: 43s - loss: 0.6423 - acc: 0.85 - ETA: 43s - loss: 0.6371 - acc: 0.85 - ETA: 43s - loss: 0.6337 - acc: 0.85 - ETA: 43s - loss: 0.6387 - acc: 0.85 - ETA: 43s - loss: 0.6350 - acc: 0.85 - ETA: 43s - loss: 0.6356 - acc: 0.85 - ETA: 42s - loss: 0.6375 - acc: 0.85 - ETA: 42s - loss: 0.6357 - acc: 0.85 - ETA: 42s - loss: 0.6349 - acc: 0.85 - ETA: 42s - loss: 0.6340 - acc: 0.85 - ETA: 42s - loss: 0.6318 - acc: 0.85 - ETA: 41s - loss: 0.6278 - acc: 0.85 - ETA: 41s - loss: 0.6260 - acc: 0.85 - ETA: 41s - loss: 0.6265 - acc: 0.85 - ETA: 41s - loss: 0.6313 - acc: 0.84 - ETA: 41s - loss: 0.6400 - acc: 0.84 - ETA: 41s - loss: 0.6392 - acc: 0.84 - ETA: 40s - loss: 0.6390 - acc: 0.84 - ETA: 40s - loss: 0.6378 - acc: 0.84 - ETA: 40s - loss: 0.6331 - acc: 0.84 - ETA: 40s - loss: 0.6307 - acc: 0.84 - ETA: 40s - loss: 0.6313 - acc: 0.84 - ETA: 39s - loss: 0.6335 - acc: 0.84 - ETA: 39s - loss: 0.6306 - acc: 0.84 - ETA: 39s - loss: 0.6281 - acc: 0.85 - ETA: 39s - loss: 0.6270 - acc: 0.85 - ETA: 39s - loss: 0.6254 - acc: 0.85 - ETA: 39s - loss: 0.6273 - acc: 0.85 - ETA: 38s - loss: 0.6282 - acc: 0.85 - ETA: 38s - loss: 0.6244 - acc: 0.85 - ETA: 38s - loss: 0.6285 - acc: 0.85 - ETA: 38s - loss: 0.6257 - acc: 0.85 - ETA: 38s - loss: 0.6275 - acc: 0.85 - ETA: 38s - loss: 0.6374 - acc: 0.84 - ETA: 37s - loss: 0.6482 - acc: 0.84 - ETA: 37s - loss: 0.6458 - acc: 0.84 - ETA: 37s - loss: 0.6500 - acc: 0.84 - ETA: 37s - loss: 0.6459 - acc: 0.84 - ETA: 37s - loss: 0.6451 - acc: 0.84 - ETA: 36s - loss: 0.6429 - acc: 0.84 - ETA: 36s - loss: 0.6407 - acc: 0.85 - ETA: 36s - loss: 0.6394 - acc: 0.85 - ETA: 36s - loss: 0.6360 - acc: 0.85 - ETA: 36s - loss: 0.6350 - acc: 0.85 - ETA: 36s - loss: 0.6395 - acc: 0.85 - ETA: 35s - loss: 0.6383 - acc: 0.85 - ETA: 35s - loss: 0.6364 - acc: 0.85 - ETA: 35s - loss: 0.6412 - acc: 0.84 - ETA: 35s - loss: 0.6391 - acc: 0.85 - ETA: 35s - loss: 0.6353 - acc: 0.85 - ETA: 35s - loss: 0.6327 - acc: 0.85 - ETA: 35s - loss: 0.6336 - acc: 0.85 - ETA: 34s - loss: 0.6360 - acc: 0.85 - ETA: 34s - loss: 0.6329 - acc: 0.85 - ETA: 34s - loss: 0.6324 - acc: 0.85 - ETA: 34s - loss: 0.6335 - acc: 0.85 - ETA: 34s - loss: 0.6323 - acc: 0.85 - ETA: 34s - loss: 0.6380 - acc: 0.84 - ETA: 33s - loss: 0.6394 - acc: 0.84 - ETA: 33s - loss: 0.6390 - acc: 0.84 - ETA: 33s - loss: 0.6380 - acc: 0.84 - ETA: 33s - loss: 0.6378 - acc: 0.84 - ETA: 33s - loss: 0.6371 - acc: 0.84 - ETA: 33s - loss: 0.6370 - acc: 0.84 - ETA: 33s - loss: 0.6360 - acc: 0.84 - ETA: 32s - loss: 0.6337 - acc: 0.84 - ETA: 32s - loss: 0.6325 - acc: 0.84 - ETA: 32s - loss: 0.6340 - acc: 0.84 - ETA: 32s - loss: 0.6386 - acc: 0.84 - ETA: 32s - loss: 0.6399 - acc: 0.84 - ETA: 32s - loss: 0.6390 - acc: 0.84 - ETA: 31s - loss: 0.6421 - acc: 0.84 - ETA: 31s - loss: 0.6424 - acc: 0.84 - ETA: 31s - loss: 0.6405 - acc: 0.84 - ETA: 31s - loss: 0.6425 - acc: 0.84 - ETA: 31s - loss: 0.6408 - acc: 0.84 - ETA: 30s - loss: 0.6406 - acc: 0.84 - ETA: 30s - loss: 0.6406 - acc: 0.84 - ETA: 30s - loss: 0.6423 - acc: 0.84 - ETA: 30s - loss: 0.6421 - acc: 0.84 - ETA: 30s - loss: 0.6458 - acc: 0.84 - ETA: 30s - loss: 0.6453 - acc: 0.84 - ETA: 29s - loss: 0.6456 - acc: 0.84 - ETA: 29s - loss: 0.6474 - acc: 0.84 - ETA: 29s - loss: 0.6481 - acc: 0.84 - ETA: 29s - loss: 0.6462 - acc: 0.84 - ETA: 29s - loss: 0.6465 - acc: 0.84 - ETA: 29s - loss: 0.6480 - acc: 0.84 - ETA: 28s - loss: 0.6492 - acc: 0.84 - ETA: 28s - loss: 0.6515 - acc: 0.84 - ETA: 28s - loss: 0.6519 - acc: 0.84 - ETA: 28s - loss: 0.6502 - acc: 0.84 - ETA: 28s - loss: 0.6489 - acc: 0.84 - ETA: 28s - loss: 0.6502 - acc: 0.84 - ETA: 27s - loss: 0.6537 - acc: 0.84 - ETA: 27s - loss: 0.6561 - acc: 0.84 - ETA: 27s - loss: 0.6553 - acc: 0.84 - ETA: 27s - loss: 0.6525 - acc: 0.84 - ETA: 27s - loss: 0.6522 - acc: 0.84 - ETA: 27s - loss: 0.6512 - acc: 0.84 - ETA: 26s - loss: 0.6496 - acc: 0.84 - ETA: 26s - loss: 0.6476 - acc: 0.84 - ETA: 26s - loss: 0.6503 - acc: 0.84 - ETA: 26s - loss: 0.6492 - acc: 0.84 - ETA: 26s - loss: 0.6499 - acc: 0.84 - ETA: 25s - loss: 0.6506 - acc: 0.84 - ETA: 25s - loss: 0.6505 - acc: 0.84 - ETA: 25s - loss: 0.6538 - acc: 0.84 - ETA: 25s - loss: 0.6538 - acc: 0.84 - ETA: 25s - loss: 0.6539 - acc: 0.84 - ETA: 25s - loss: 0.6525 - acc: 0.84 - ETA: 24s - loss: 0.6511 - acc: 0.84 - ETA: 24s - loss: 0.6491 - acc: 0.84 - ETA: 24s - loss: 0.6522 - acc: 0.84 - ETA: 24s - loss: 0.6500 - acc: 0.84 - ETA: 24s - loss: 0.6474 - acc: 0.84 - ETA: 24s - loss: 0.6466 - acc: 0.84 - ETA: 23s - loss: 0.6447 - acc: 0.84 - ETA: 23s - loss: 0.6440 - acc: 0.84 - ETA: 23s - loss: 0.6424 - acc: 0.84 - ETA: 23s - loss: 0.6435 - acc: 0.84 - ETA: 23s - loss: 0.6447 - acc: 0.84 - ETA: 22s - loss: 0.6459 - acc: 0.84 - ETA: 22s - loss: 0.6477 - acc: 0.84 - ETA: 22s - loss: 0.6474 - acc: 0.84 - ETA: 22s - loss: 0.6466 - acc: 0.84 - ETA: 22s - loss: 0.6495 - acc: 0.84 - ETA: 22s - loss: 0.6487 - acc: 0.84 - ETA: 21s - loss: 0.6486 - acc: 0.84 - ETA: 21s - loss: 0.6494 - acc: 0.84 - ETA: 21s - loss: 0.6496 - acc: 0.84 - ETA: 21s - loss: 0.6489 - acc: 0.84 - ETA: 21s - loss: 0.6494 - acc: 0.84 - ETA: 21s - loss: 0.6493 - acc: 0.84 - ETA: 20s - loss: 0.6477 - acc: 0.84 - ETA: 20s - loss: 0.6458 - acc: 0.84 - ETA: 20s - loss: 0.6444 - acc: 0.84 - ETA: 20s - loss: 0.6438 - acc: 0.84 - ETA: 20s - loss: 0.6428 - acc: 0.84 - ETA: 20s - loss: 0.6408 - acc: 0.8433"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 19s - loss: 0.6418 - acc: 0.84 - ETA: 19s - loss: 0.6433 - acc: 0.84 - ETA: 19s - loss: 0.6440 - acc: 0.84 - ETA: 19s - loss: 0.6563 - acc: 0.84 - ETA: 19s - loss: 0.6558 - acc: 0.83 - ETA: 19s - loss: 0.6564 - acc: 0.83 - ETA: 18s - loss: 0.6567 - acc: 0.83 - ETA: 18s - loss: 0.6581 - acc: 0.83 - ETA: 18s - loss: 0.6584 - acc: 0.83 - ETA: 18s - loss: 0.6620 - acc: 0.83 - ETA: 18s - loss: 0.6618 - acc: 0.83 - ETA: 18s - loss: 0.6597 - acc: 0.83 - ETA: 17s - loss: 0.6602 - acc: 0.83 - ETA: 17s - loss: 0.6599 - acc: 0.83 - ETA: 17s - loss: 0.6589 - acc: 0.83 - ETA: 17s - loss: 0.6596 - acc: 0.83 - ETA: 17s - loss: 0.6584 - acc: 0.83 - ETA: 17s - loss: 0.6576 - acc: 0.83 - ETA: 16s - loss: 0.6566 - acc: 0.83 - ETA: 16s - loss: 0.6611 - acc: 0.83 - ETA: 16s - loss: 0.6613 - acc: 0.83 - ETA: 16s - loss: 0.6659 - acc: 0.83 - ETA: 16s - loss: 0.6662 - acc: 0.83 - ETA: 16s - loss: 0.6652 - acc: 0.83 - ETA: 15s - loss: 0.6689 - acc: 0.83 - ETA: 15s - loss: 0.6685 - acc: 0.83 - ETA: 15s - loss: 0.6681 - acc: 0.83 - ETA: 15s - loss: 0.6680 - acc: 0.83 - ETA: 15s - loss: 0.6673 - acc: 0.83 - ETA: 15s - loss: 0.6657 - acc: 0.83 - ETA: 14s - loss: 0.6678 - acc: 0.83 - ETA: 14s - loss: 0.6682 - acc: 0.83 - ETA: 14s - loss: 0.6675 - acc: 0.83 - ETA: 14s - loss: 0.6690 - acc: 0.83 - ETA: 14s - loss: 0.6694 - acc: 0.83 - ETA: 14s - loss: 0.6683 - acc: 0.83 - ETA: 13s - loss: 0.6664 - acc: 0.83 - ETA: 13s - loss: 0.6668 - acc: 0.83 - ETA: 13s - loss: 0.6684 - acc: 0.83 - ETA: 13s - loss: 0.6710 - acc: 0.83 - ETA: 13s - loss: 0.6718 - acc: 0.83 - ETA: 12s - loss: 0.6700 - acc: 0.83 - ETA: 12s - loss: 0.6696 - acc: 0.83 - ETA: 12s - loss: 0.6683 - acc: 0.83 - ETA: 12s - loss: 0.6668 - acc: 0.83 - ETA: 12s - loss: 0.6660 - acc: 0.83 - ETA: 12s - loss: 0.6672 - acc: 0.83 - ETA: 11s - loss: 0.6673 - acc: 0.83 - ETA: 11s - loss: 0.6668 - acc: 0.83 - ETA: 11s - loss: 0.6652 - acc: 0.83 - ETA: 11s - loss: 0.6685 - acc: 0.83 - ETA: 11s - loss: 0.6686 - acc: 0.83 - ETA: 11s - loss: 0.6693 - acc: 0.83 - ETA: 10s - loss: 0.6709 - acc: 0.83 - ETA: 10s - loss: 0.6694 - acc: 0.83 - ETA: 10s - loss: 0.6680 - acc: 0.83 - ETA: 10s - loss: 0.6680 - acc: 0.83 - ETA: 10s - loss: 0.6682 - acc: 0.83 - ETA: 10s - loss: 0.6672 - acc: 0.83 - ETA: 9s - loss: 0.6660 - acc: 0.8376 - ETA: 9s - loss: 0.6668 - acc: 0.837 - ETA: 9s - loss: 0.6670 - acc: 0.837 - ETA: 9s - loss: 0.6663 - acc: 0.837 - ETA: 9s - loss: 0.6650 - acc: 0.837 - ETA: 9s - loss: 0.6656 - acc: 0.838 - ETA: 8s - loss: 0.6660 - acc: 0.838 - ETA: 8s - loss: 0.6660 - acc: 0.837 - ETA: 8s - loss: 0.6659 - acc: 0.837 - ETA: 8s - loss: 0.6657 - acc: 0.837 - ETA: 8s - loss: 0.6676 - acc: 0.837 - ETA: 8s - loss: 0.6709 - acc: 0.836 - ETA: 7s - loss: 0.6731 - acc: 0.835 - ETA: 7s - loss: 0.6724 - acc: 0.835 - ETA: 7s - loss: 0.6723 - acc: 0.835 - ETA: 7s - loss: 0.6747 - acc: 0.835 - ETA: 7s - loss: 0.6759 - acc: 0.834 - ETA: 7s - loss: 0.6759 - acc: 0.834 - ETA: 6s - loss: 0.6754 - acc: 0.835 - ETA: 6s - loss: 0.6748 - acc: 0.835 - ETA: 6s - loss: 0.6749 - acc: 0.834 - ETA: 6s - loss: 0.6755 - acc: 0.834 - ETA: 6s - loss: 0.6745 - acc: 0.834 - ETA: 6s - loss: 0.6757 - acc: 0.834 - ETA: 5s - loss: 0.6747 - acc: 0.834 - ETA: 5s - loss: 0.6746 - acc: 0.834 - ETA: 5s - loss: 0.6748 - acc: 0.834 - ETA: 5s - loss: 0.6755 - acc: 0.833 - ETA: 5s - loss: 0.6757 - acc: 0.833 - ETA: 5s - loss: 0.6775 - acc: 0.833 - ETA: 4s - loss: 0.6807 - acc: 0.832 - ETA: 4s - loss: 0.6795 - acc: 0.833 - ETA: 4s - loss: 0.6791 - acc: 0.833 - ETA: 4s - loss: 0.6788 - acc: 0.833 - ETA: 4s - loss: 0.6787 - acc: 0.833 - ETA: 4s - loss: 0.6789 - acc: 0.832 - ETA: 3s - loss: 0.6797 - acc: 0.832 - ETA: 3s - loss: 0.6797 - acc: 0.832 - ETA: 3s - loss: 0.6788 - acc: 0.832 - ETA: 3s - loss: 0.6773 - acc: 0.833 - ETA: 3s - loss: 0.6779 - acc: 0.833 - ETA: 3s - loss: 0.6780 - acc: 0.832 - ETA: 2s - loss: 0.6775 - acc: 0.832 - ETA: 2s - loss: 0.6779 - acc: 0.832 - ETA: 2s - loss: 0.6773 - acc: 0.832 - ETA: 2s - loss: 0.6784 - acc: 0.832 - ETA: 2s - loss: 0.6808 - acc: 0.832 - ETA: 2s - loss: 0.6801 - acc: 0.832 - ETA: 1s - loss: 0.6803 - acc: 0.832 - ETA: 1s - loss: 0.6813 - acc: 0.832 - ETA: 1s - loss: 0.6811 - acc: 0.832 - ETA: 1s - loss: 0.6801 - acc: 0.832 - ETA: 1s - loss: 0.6820 - acc: 0.831 - ETA: 1s - loss: 0.6850 - acc: 0.831 - ETA: 0s - loss: 0.6855 - acc: 0.831 - ETA: 0s - loss: 0.6849 - acc: 0.831 - ETA: 0s - loss: 0.6841 - acc: 0.831 - ETA: 0s - loss: 0.6837 - acc: 0.831 - ETA: 0s - loss: 0.6834 - acc: 0.831 - 59s 9ms/step - loss: 0.6833 - acc: 0.8314 - val_loss: 8.6391 - val_acc: 0.0743\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4300/6680 [==================>...........] - ETA: 52s - loss: 0.1985 - acc: 0.95 - ETA: 52s - loss: 0.2315 - acc: 0.95 - ETA: 52s - loss: 0.2057 - acc: 0.95 - ETA: 53s - loss: 0.1570 - acc: 0.96 - ETA: 53s - loss: 0.1777 - acc: 0.96 - ETA: 53s - loss: 0.1746 - acc: 0.95 - ETA: 53s - loss: 0.1612 - acc: 0.95 - ETA: 53s - loss: 0.1668 - acc: 0.95 - ETA: 53s - loss: 0.1602 - acc: 0.95 - ETA: 52s - loss: 0.1448 - acc: 0.96 - ETA: 52s - loss: 0.1338 - acc: 0.96 - ETA: 52s - loss: 0.1362 - acc: 0.96 - ETA: 52s - loss: 0.1445 - acc: 0.96 - ETA: 52s - loss: 0.1662 - acc: 0.95 - ETA: 52s - loss: 0.2126 - acc: 0.93 - ETA: 52s - loss: 0.2202 - acc: 0.93 - ETA: 52s - loss: 0.2133 - acc: 0.94 - ETA: 52s - loss: 0.2119 - acc: 0.94 - ETA: 52s - loss: 0.2042 - acc: 0.94 - ETA: 52s - loss: 0.2027 - acc: 0.94 - ETA: 52s - loss: 0.2060 - acc: 0.94 - ETA: 51s - loss: 0.2001 - acc: 0.94 - ETA: 51s - loss: 0.1990 - acc: 0.94 - ETA: 51s - loss: 0.1933 - acc: 0.94 - ETA: 51s - loss: 0.1882 - acc: 0.95 - ETA: 50s - loss: 0.1861 - acc: 0.95 - ETA: 50s - loss: 0.1793 - acc: 0.95 - ETA: 50s - loss: 0.1868 - acc: 0.95 - ETA: 50s - loss: 0.1821 - acc: 0.95 - ETA: 50s - loss: 0.1798 - acc: 0.95 - ETA: 49s - loss: 0.1809 - acc: 0.95 - ETA: 49s - loss: 0.1817 - acc: 0.95 - ETA: 49s - loss: 0.1765 - acc: 0.95 - ETA: 49s - loss: 0.1989 - acc: 0.95 - ETA: 49s - loss: 0.2044 - acc: 0.94 - ETA: 49s - loss: 0.2065 - acc: 0.94 - ETA: 49s - loss: 0.2076 - acc: 0.94 - ETA: 48s - loss: 0.2153 - acc: 0.94 - ETA: 48s - loss: 0.2264 - acc: 0.94 - ETA: 48s - loss: 0.2532 - acc: 0.93 - ETA: 48s - loss: 0.2536 - acc: 0.93 - ETA: 48s - loss: 0.2519 - acc: 0.93 - ETA: 48s - loss: 0.2474 - acc: 0.93 - ETA: 47s - loss: 0.2469 - acc: 0.93 - ETA: 47s - loss: 0.2482 - acc: 0.93 - ETA: 47s - loss: 0.2443 - acc: 0.93 - ETA: 47s - loss: 0.2431 - acc: 0.93 - ETA: 47s - loss: 0.2507 - acc: 0.93 - ETA: 47s - loss: 0.2521 - acc: 0.93 - ETA: 46s - loss: 0.2546 - acc: 0.93 - ETA: 46s - loss: 0.2575 - acc: 0.93 - ETA: 46s - loss: 0.2570 - acc: 0.93 - ETA: 46s - loss: 0.2599 - acc: 0.93 - ETA: 46s - loss: 0.2616 - acc: 0.93 - ETA: 45s - loss: 0.2590 - acc: 0.93 - ETA: 45s - loss: 0.2581 - acc: 0.93 - ETA: 45s - loss: 0.2615 - acc: 0.93 - ETA: 45s - loss: 0.2603 - acc: 0.93 - ETA: 45s - loss: 0.2569 - acc: 0.93 - ETA: 45s - loss: 0.2546 - acc: 0.93 - ETA: 45s - loss: 0.2536 - acc: 0.93 - ETA: 44s - loss: 0.2546 - acc: 0.93 - ETA: 44s - loss: 0.2576 - acc: 0.93 - ETA: 44s - loss: 0.2612 - acc: 0.93 - ETA: 44s - loss: 0.2590 - acc: 0.93 - ETA: 44s - loss: 0.2631 - acc: 0.93 - ETA: 44s - loss: 0.3268 - acc: 0.92 - ETA: 43s - loss: 0.3446 - acc: 0.92 - ETA: 43s - loss: 0.3401 - acc: 0.92 - ETA: 43s - loss: 0.3431 - acc: 0.92 - ETA: 43s - loss: 0.3396 - acc: 0.92 - ETA: 43s - loss: 0.3416 - acc: 0.92 - ETA: 43s - loss: 0.3377 - acc: 0.92 - ETA: 42s - loss: 0.3389 - acc: 0.92 - ETA: 42s - loss: 0.3363 - acc: 0.92 - ETA: 42s - loss: 0.3341 - acc: 0.92 - ETA: 42s - loss: 0.3305 - acc: 0.92 - ETA: 42s - loss: 0.3277 - acc: 0.92 - ETA: 42s - loss: 0.3284 - acc: 0.92 - ETA: 42s - loss: 0.3283 - acc: 0.92 - ETA: 41s - loss: 0.3276 - acc: 0.92 - ETA: 41s - loss: 0.3356 - acc: 0.92 - ETA: 41s - loss: 0.3339 - acc: 0.92 - ETA: 41s - loss: 0.3334 - acc: 0.92 - ETA: 41s - loss: 0.3297 - acc: 0.92 - ETA: 41s - loss: 0.3260 - acc: 0.92 - ETA: 42s - loss: 0.3226 - acc: 0.92 - ETA: 42s - loss: 0.3214 - acc: 0.92 - ETA: 42s - loss: 0.3188 - acc: 0.93 - ETA: 42s - loss: 0.3160 - acc: 0.93 - ETA: 42s - loss: 0.3137 - acc: 0.93 - ETA: 42s - loss: 0.3132 - acc: 0.93 - ETA: 41s - loss: 0.3114 - acc: 0.93 - ETA: 41s - loss: 0.3095 - acc: 0.93 - ETA: 41s - loss: 0.3096 - acc: 0.93 - ETA: 41s - loss: 0.3072 - acc: 0.93 - ETA: 41s - loss: 0.3072 - acc: 0.93 - ETA: 41s - loss: 0.3099 - acc: 0.93 - ETA: 41s - loss: 0.3176 - acc: 0.92 - ETA: 41s - loss: 0.3227 - acc: 0.92 - ETA: 41s - loss: 0.3200 - acc: 0.92 - ETA: 40s - loss: 0.3173 - acc: 0.92 - ETA: 40s - loss: 0.3144 - acc: 0.92 - ETA: 40s - loss: 0.3121 - acc: 0.92 - ETA: 40s - loss: 0.3096 - acc: 0.92 - ETA: 40s - loss: 0.3082 - acc: 0.92 - ETA: 39s - loss: 0.3059 - acc: 0.93 - ETA: 39s - loss: 0.3050 - acc: 0.93 - ETA: 39s - loss: 0.3043 - acc: 0.92 - ETA: 39s - loss: 0.3018 - acc: 0.93 - ETA: 39s - loss: 0.2999 - acc: 0.93 - ETA: 38s - loss: 0.2975 - acc: 0.93 - ETA: 38s - loss: 0.2973 - acc: 0.93 - ETA: 38s - loss: 0.2984 - acc: 0.93 - ETA: 38s - loss: 0.2968 - acc: 0.93 - ETA: 38s - loss: 0.2945 - acc: 0.93 - ETA: 37s - loss: 0.2935 - acc: 0.93 - ETA: 37s - loss: 0.2924 - acc: 0.93 - ETA: 37s - loss: 0.2913 - acc: 0.93 - ETA: 37s - loss: 0.2895 - acc: 0.93 - ETA: 37s - loss: 0.2875 - acc: 0.93 - ETA: 36s - loss: 0.2878 - acc: 0.93 - ETA: 36s - loss: 0.2856 - acc: 0.93 - ETA: 36s - loss: 0.2840 - acc: 0.93 - ETA: 36s - loss: 0.2880 - acc: 0.93 - ETA: 36s - loss: 0.2883 - acc: 0.93 - ETA: 35s - loss: 0.2871 - acc: 0.93 - ETA: 35s - loss: 0.2861 - acc: 0.93 - ETA: 35s - loss: 0.2849 - acc: 0.93 - ETA: 35s - loss: 0.2837 - acc: 0.93 - ETA: 35s - loss: 0.2874 - acc: 0.93 - ETA: 35s - loss: 0.2860 - acc: 0.93 - ETA: 34s - loss: 0.2864 - acc: 0.93 - ETA: 34s - loss: 0.2859 - acc: 0.93 - ETA: 34s - loss: 0.2846 - acc: 0.93 - ETA: 34s - loss: 0.2853 - acc: 0.93 - ETA: 34s - loss: 0.2863 - acc: 0.93 - ETA: 33s - loss: 0.2894 - acc: 0.93 - ETA: 33s - loss: 0.2891 - acc: 0.93 - ETA: 33s - loss: 0.2874 - acc: 0.93 - ETA: 33s - loss: 0.2874 - acc: 0.93 - ETA: 33s - loss: 0.2869 - acc: 0.93 - ETA: 33s - loss: 0.2860 - acc: 0.93 - ETA: 32s - loss: 0.2848 - acc: 0.93 - ETA: 32s - loss: 0.2840 - acc: 0.93 - ETA: 32s - loss: 0.2832 - acc: 0.93 - ETA: 32s - loss: 0.2815 - acc: 0.93 - ETA: 32s - loss: 0.2799 - acc: 0.93 - ETA: 31s - loss: 0.2802 - acc: 0.93 - ETA: 31s - loss: 0.2788 - acc: 0.93 - ETA: 31s - loss: 0.2778 - acc: 0.93 - ETA: 31s - loss: 0.2770 - acc: 0.93 - ETA: 31s - loss: 0.2759 - acc: 0.93 - ETA: 31s - loss: 0.2766 - acc: 0.93 - ETA: 30s - loss: 0.2790 - acc: 0.93 - ETA: 30s - loss: 0.2792 - acc: 0.93 - ETA: 30s - loss: 0.2783 - acc: 0.93 - ETA: 30s - loss: 0.2786 - acc: 0.93 - ETA: 30s - loss: 0.2792 - acc: 0.93 - ETA: 29s - loss: 0.2816 - acc: 0.93 - ETA: 29s - loss: 0.2812 - acc: 0.93 - ETA: 29s - loss: 0.2798 - acc: 0.93 - ETA: 29s - loss: 0.2792 - acc: 0.93 - ETA: 29s - loss: 0.2783 - acc: 0.93 - ETA: 29s - loss: 0.2783 - acc: 0.93 - ETA: 28s - loss: 0.2795 - acc: 0.93 - ETA: 28s - loss: 0.2784 - acc: 0.93 - ETA: 28s - loss: 0.2770 - acc: 0.93 - ETA: 28s - loss: 0.2778 - acc: 0.93 - ETA: 28s - loss: 0.2775 - acc: 0.93 - ETA: 28s - loss: 0.2788 - acc: 0.93 - ETA: 27s - loss: 0.2778 - acc: 0.93 - ETA: 27s - loss: 0.2780 - acc: 0.93 - ETA: 27s - loss: 0.2779 - acc: 0.93 - ETA: 27s - loss: 0.2774 - acc: 0.93 - ETA: 27s - loss: 0.2760 - acc: 0.93 - ETA: 26s - loss: 0.2748 - acc: 0.93 - ETA: 26s - loss: 0.2739 - acc: 0.93 - ETA: 26s - loss: 0.2745 - acc: 0.93 - ETA: 26s - loss: 0.2755 - acc: 0.93 - ETA: 26s - loss: 0.2768 - acc: 0.93 - ETA: 26s - loss: 0.2754 - acc: 0.93 - ETA: 25s - loss: 0.2750 - acc: 0.93 - ETA: 25s - loss: 0.2751 - acc: 0.93 - ETA: 25s - loss: 0.2751 - acc: 0.93 - ETA: 25s - loss: 0.2747 - acc: 0.93 - ETA: 25s - loss: 0.2746 - acc: 0.93 - ETA: 24s - loss: 0.2736 - acc: 0.93 - ETA: 24s - loss: 0.2723 - acc: 0.93 - ETA: 24s - loss: 0.2710 - acc: 0.93 - ETA: 24s - loss: 0.2709 - acc: 0.93 - ETA: 24s - loss: 0.2710 - acc: 0.93 - ETA: 24s - loss: 0.2706 - acc: 0.93 - ETA: 23s - loss: 0.2700 - acc: 0.93 - ETA: 23s - loss: 0.2705 - acc: 0.93 - ETA: 23s - loss: 0.2699 - acc: 0.93 - ETA: 23s - loss: 0.2693 - acc: 0.93 - ETA: 23s - loss: 0.2693 - acc: 0.93 - ETA: 23s - loss: 0.2692 - acc: 0.93 - ETA: 22s - loss: 0.2702 - acc: 0.93 - ETA: 22s - loss: 0.2699 - acc: 0.93 - ETA: 22s - loss: 0.2687 - acc: 0.93 - ETA: 22s - loss: 0.2689 - acc: 0.93 - ETA: 22s - loss: 0.2694 - acc: 0.93 - ETA: 22s - loss: 0.2692 - acc: 0.93 - ETA: 21s - loss: 0.2690 - acc: 0.93 - ETA: 21s - loss: 0.2678 - acc: 0.93 - ETA: 21s - loss: 0.2681 - acc: 0.93 - ETA: 21s - loss: 0.2709 - acc: 0.93 - ETA: 21s - loss: 0.2713 - acc: 0.93 - ETA: 20s - loss: 0.2722 - acc: 0.93 - ETA: 20s - loss: 0.2745 - acc: 0.93 - ETA: 20s - loss: 0.2750 - acc: 0.93 - ETA: 20s - loss: 0.2752 - acc: 0.93 - ETA: 20s - loss: 0.2749 - acc: 0.9340"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 20s - loss: 0.2753 - acc: 0.93 - ETA: 19s - loss: 0.2742 - acc: 0.93 - ETA: 19s - loss: 0.2732 - acc: 0.93 - ETA: 19s - loss: 0.2737 - acc: 0.93 - ETA: 19s - loss: 0.2735 - acc: 0.93 - ETA: 19s - loss: 0.2724 - acc: 0.93 - ETA: 19s - loss: 0.2719 - acc: 0.93 - ETA: 18s - loss: 0.2710 - acc: 0.93 - ETA: 18s - loss: 0.2715 - acc: 0.93 - ETA: 18s - loss: 0.2708 - acc: 0.93 - ETA: 18s - loss: 0.2704 - acc: 0.93 - ETA: 18s - loss: 0.2765 - acc: 0.93 - ETA: 18s - loss: 0.2773 - acc: 0.93 - ETA: 17s - loss: 0.2766 - acc: 0.93 - ETA: 17s - loss: 0.2770 - acc: 0.93 - ETA: 17s - loss: 0.2770 - acc: 0.93 - ETA: 17s - loss: 0.2770 - acc: 0.93 - ETA: 17s - loss: 0.2764 - acc: 0.93 - ETA: 16s - loss: 0.2788 - acc: 0.93 - ETA: 16s - loss: 0.2816 - acc: 0.93 - ETA: 16s - loss: 0.2805 - acc: 0.93 - ETA: 16s - loss: 0.2798 - acc: 0.93 - ETA: 16s - loss: 0.2802 - acc: 0.93 - ETA: 16s - loss: 0.2806 - acc: 0.93 - ETA: 15s - loss: 0.2815 - acc: 0.93 - ETA: 15s - loss: 0.2815 - acc: 0.93 - ETA: 15s - loss: 0.2809 - acc: 0.93 - ETA: 15s - loss: 0.2801 - acc: 0.93 - ETA: 15s - loss: 0.2800 - acc: 0.93 - ETA: 15s - loss: 0.2796 - acc: 0.93 - ETA: 14s - loss: 0.2787 - acc: 0.93 - ETA: 14s - loss: 0.2778 - acc: 0.93 - ETA: 14s - loss: 0.2796 - acc: 0.93 - ETA: 14s - loss: 0.2798 - acc: 0.93 - ETA: 14s - loss: 0.2800 - acc: 0.93 - ETA: 14s - loss: 0.2823 - acc: 0.93 - ETA: 13s - loss: 0.2838 - acc: 0.93 - ETA: 13s - loss: 0.2841 - acc: 0.93 - ETA: 13s - loss: 0.2839 - acc: 0.93 - ETA: 13s - loss: 0.2835 - acc: 0.93 - ETA: 13s - loss: 0.2829 - acc: 0.93 - ETA: 13s - loss: 0.2827 - acc: 0.93 - ETA: 12s - loss: 0.2817 - acc: 0.93 - ETA: 12s - loss: 0.2813 - acc: 0.93 - ETA: 12s - loss: 0.2817 - acc: 0.93 - ETA: 12s - loss: 0.2808 - acc: 0.93 - ETA: 12s - loss: 0.2799 - acc: 0.93 - ETA: 12s - loss: 0.2795 - acc: 0.93 - ETA: 11s - loss: 0.2787 - acc: 0.93 - ETA: 11s - loss: 0.2786 - acc: 0.93 - ETA: 11s - loss: 0.2780 - acc: 0.93 - ETA: 11s - loss: 0.2788 - acc: 0.93 - ETA: 11s - loss: 0.2780 - acc: 0.93 - ETA: 11s - loss: 0.2770 - acc: 0.93 - ETA: 10s - loss: 0.2770 - acc: 0.93 - ETA: 10s - loss: 0.2773 - acc: 0.93 - ETA: 10s - loss: 0.2778 - acc: 0.93 - ETA: 10s - loss: 0.2770 - acc: 0.93 - ETA: 10s - loss: 0.2779 - acc: 0.93 - ETA: 10s - loss: 0.2775 - acc: 0.93 - ETA: 9s - loss: 0.2770 - acc: 0.9335 - ETA: 9s - loss: 0.2764 - acc: 0.933 - ETA: 9s - loss: 0.2770 - acc: 0.933 - ETA: 9s - loss: 0.2782 - acc: 0.933 - ETA: 9s - loss: 0.2784 - acc: 0.933 - ETA: 8s - loss: 0.2777 - acc: 0.933 - ETA: 8s - loss: 0.2771 - acc: 0.933 - ETA: 8s - loss: 0.2786 - acc: 0.933 - ETA: 8s - loss: 0.2780 - acc: 0.933 - ETA: 8s - loss: 0.2800 - acc: 0.933 - ETA: 8s - loss: 0.2795 - acc: 0.933 - ETA: 7s - loss: 0.2792 - acc: 0.933 - ETA: 7s - loss: 0.2783 - acc: 0.933 - ETA: 7s - loss: 0.2775 - acc: 0.933 - ETA: 7s - loss: 0.2774 - acc: 0.933 - ETA: 7s - loss: 0.2774 - acc: 0.933 - ETA: 7s - loss: 0.2768 - acc: 0.933 - ETA: 6s - loss: 0.2763 - acc: 0.933 - ETA: 6s - loss: 0.2763 - acc: 0.933 - ETA: 6s - loss: 0.2764 - acc: 0.933 - ETA: 6s - loss: 0.2764 - acc: 0.932 - ETA: 6s - loss: 0.2759 - acc: 0.933 - ETA: 6s - loss: 0.2784 - acc: 0.932 - ETA: 5s - loss: 0.2777 - acc: 0.933 - ETA: 5s - loss: 0.2774 - acc: 0.933 - ETA: 5s - loss: 0.2779 - acc: 0.933 - ETA: 5s - loss: 0.2777 - acc: 0.932 - ETA: 5s - loss: 0.2782 - acc: 0.932 - ETA: 5s - loss: 0.2777 - acc: 0.932 - ETA: 4s - loss: 0.2769 - acc: 0.933 - ETA: 4s - loss: 0.2765 - acc: 0.933 - ETA: 4s - loss: 0.2757 - acc: 0.933 - ETA: 4s - loss: 0.2756 - acc: 0.933 - ETA: 4s - loss: 0.2767 - acc: 0.933 - ETA: 4s - loss: 0.2758 - acc: 0.933 - ETA: 3s - loss: 0.2759 - acc: 0.933 - ETA: 3s - loss: 0.2751 - acc: 0.933 - ETA: 3s - loss: 0.2749 - acc: 0.933 - ETA: 3s - loss: 0.2759 - acc: 0.933 - ETA: 3s - loss: 0.2776 - acc: 0.932 - ETA: 3s - loss: 0.2781 - acc: 0.932 - ETA: 2s - loss: 0.2775 - acc: 0.932 - ETA: 2s - loss: 0.2768 - acc: 0.932 - ETA: 2s - loss: 0.2770 - acc: 0.932 - ETA: 2s - loss: 0.2762 - acc: 0.933 - ETA: 2s - loss: 0.2763 - acc: 0.932 - ETA: 2s - loss: 0.2761 - acc: 0.932 - ETA: 1s - loss: 0.2755 - acc: 0.933 - ETA: 1s - loss: 0.2754 - acc: 0.933 - ETA: 1s - loss: 0.2748 - acc: 0.933 - ETA: 1s - loss: 0.2750 - acc: 0.933 - ETA: 1s - loss: 0.2746 - acc: 0.933 - ETA: 1s - loss: 0.2742 - acc: 0.933 - ETA: 0s - loss: 0.2747 - acc: 0.932 - ETA: 0s - loss: 0.2744 - acc: 0.932 - ETA: 0s - loss: 0.2745 - acc: 0.932 - ETA: 0s - loss: 0.2750 - acc: 0.932 - ETA: 0s - loss: 0.2752 - acc: 0.932 - 59s 9ms/step - loss: 0.2755 - acc: 0.9325 - val_loss: 11.5494 - val_acc: 0.0503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1deb1b34828>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "### TODO: specify the number of epochs that you would like to use to train the model.\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "### Do NOT modify the code below this line.\n",
    "\n",
    "#The below fits the CNN and saves the weights for the epoch which yields the ebst results\n",
    "checkpointer = ModelCheckpoint(filepath='model/notebook/weights.best.from_scratch.hdf5', \n",
    "                               verbose=0, save_best_only=True)\n",
    "\n",
    "model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads the weights of the highest performing epoch\n",
    "model.load_weights('model/notebook/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 3.3493%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step4'></a>\n",
    "## Step 4: Use a CNN to Classify Dog Breeds\n",
    "\n",
    "To reduce training time without sacrificing accuracy, we show you how to train a CNN using transfer learning.  In the following step, you will get a chance to use transfer learning to train your own CNN.\n",
    "\n",
    "### Obtain Bottleneck Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads in pre-processed VGG16 features for test, train and validation images.\n",
    "bottleneck_features = np.load('bottleneck_features/DogVGG16Data.npz')\n",
    "train_VGG16 = bottleneck_features['train']\n",
    "valid_VGG16 = bottleneck_features['valid']\n",
    "test_VGG16 = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "The model uses the the pre-trained VGG-16 model as a fixed feature extractor, where the last convolutional output of VGG-16 is fed as input to our model.  We only add a global average pooling layer and a fully connected layer, where the latter contains one node for each dog category and is equipped with a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 68,229\n",
      "Trainable params: 68,229\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Defines the model architecture\n",
    "VGG16_model = Sequential()\n",
    "VGG16_model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[1:]))\n",
    "VGG16_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "#Prints model architecture\n",
    "VGG16_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defines the metric to measure against, the optimiser and the loss function used for training the model\n",
    "VGG16_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6680/6680 [==============================] - ETA: 4:21 - loss: 14.5280 - acc: 0.05 - ETA: 39s - loss: 14.7354 - acc: 0.0357 - ETA: 17s - loss: 14.6177 - acc: 0.028 - ETA: 12s - loss: 14.4402 - acc: 0.021 - ETA: 9s - loss: 14.2133 - acc: 0.024 - ETA: 7s - loss: 14.0636 - acc: 0.02 - ETA: 6s - loss: 13.9383 - acc: 0.03 - ETA: 5s - loss: 13.8575 - acc: 0.04 - ETA: 5s - loss: 13.7627 - acc: 0.04 - ETA: 4s - loss: 13.6909 - acc: 0.04 - ETA: 4s - loss: 13.6671 - acc: 0.04 - ETA: 3s - loss: 13.5335 - acc: 0.05 - ETA: 3s - loss: 13.4513 - acc: 0.05 - ETA: 3s - loss: 13.3452 - acc: 0.06 - ETA: 3s - loss: 13.2901 - acc: 0.06 - ETA: 3s - loss: 13.1739 - acc: 0.06 - ETA: 2s - loss: 13.1324 - acc: 0.06 - ETA: 2s - loss: 13.0878 - acc: 0.07 - ETA: 2s - loss: 13.0100 - acc: 0.07 - ETA: 2s - loss: 12.9471 - acc: 0.07 - ETA: 2s - loss: 12.8770 - acc: 0.08 - ETA: 2s - loss: 12.7883 - acc: 0.08 - ETA: 2s - loss: 12.7082 - acc: 0.08 - ETA: 1s - loss: 12.6576 - acc: 0.09 - ETA: 1s - loss: 12.5963 - acc: 0.09 - ETA: 1s - loss: 12.5560 - acc: 0.09 - ETA: 1s - loss: 12.5083 - acc: 0.09 - ETA: 1s - loss: 12.4421 - acc: 0.09 - ETA: 1s - loss: 12.3954 - acc: 0.10 - ETA: 1s - loss: 12.3139 - acc: 0.10 - ETA: 1s - loss: 12.2667 - acc: 0.10 - ETA: 1s - loss: 12.1899 - acc: 0.11 - ETA: 0s - loss: 12.1206 - acc: 0.11 - ETA: 0s - loss: 12.0537 - acc: 0.12 - ETA: 0s - loss: 11.9751 - acc: 0.12 - ETA: 0s - loss: 11.9150 - acc: 0.12 - ETA: 0s - loss: 11.8575 - acc: 0.13 - ETA: 0s - loss: 11.7886 - acc: 0.13 - ETA: 0s - loss: 11.7937 - acc: 0.13 - ETA: 0s - loss: 11.7656 - acc: 0.14 - ETA: 0s - loss: 11.7317 - acc: 0.14 - ETA: 0s - loss: 11.7136 - acc: 0.14 - ETA: 0s - loss: 11.6824 - acc: 0.14 - ETA: 0s - loss: 11.6584 - acc: 0.14 - ETA: 0s - loss: 11.6605 - acc: 0.14 - ETA: 0s - loss: 11.6283 - acc: 0.14 - ETA: 0s - loss: 11.6073 - acc: 0.14 - ETA: 0s - loss: 11.5955 - acc: 0.14 - 4s 548us/step - loss: 11.5849 - acc: 0.1503 - val_loss: 9.9479 - val_acc: 0.2611\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 9.94793, saving model to model/notebook/weights.best.VGG16.hdf5\n",
      "Epoch 2/20\n",
      "6680/6680 [==============================] - ETA: 3s - loss: 6.9899 - acc: 0.500 - ETA: 2s - loss: 9.2721 - acc: 0.305 - ETA: 2s - loss: 9.4701 - acc: 0.291 - ETA: 2s - loss: 9.3986 - acc: 0.293 - ETA: 2s - loss: 9.6583 - acc: 0.288 - ETA: 2s - loss: 9.6255 - acc: 0.298 - ETA: 2s - loss: 9.7517 - acc: 0.289 - ETA: 2s - loss: 9.7399 - acc: 0.290 - ETA: 2s - loss: 9.7595 - acc: 0.289 - ETA: 2s - loss: 9.5616 - acc: 0.303 - ETA: 2s - loss: 9.6149 - acc: 0.300 - ETA: 2s - loss: 9.5801 - acc: 0.297 - ETA: 2s - loss: 9.5960 - acc: 0.293 - ETA: 2s - loss: 9.6320 - acc: 0.291 - ETA: 1s - loss: 9.6221 - acc: 0.294 - ETA: 1s - loss: 9.6480 - acc: 0.293 - ETA: 1s - loss: 9.5670 - acc: 0.299 - ETA: 1s - loss: 9.4830 - acc: 0.305 - ETA: 1s - loss: 9.5190 - acc: 0.303 - ETA: 1s - loss: 9.4825 - acc: 0.306 - ETA: 1s - loss: 9.5073 - acc: 0.305 - ETA: 1s - loss: 9.5427 - acc: 0.303 - ETA: 1s - loss: 9.4892 - acc: 0.305 - ETA: 1s - loss: 9.4903 - acc: 0.304 - ETA: 1s - loss: 9.4703 - acc: 0.306 - ETA: 1s - loss: 9.4328 - acc: 0.309 - ETA: 1s - loss: 9.4349 - acc: 0.309 - ETA: 1s - loss: 9.4485 - acc: 0.310 - ETA: 0s - loss: 9.4221 - acc: 0.311 - ETA: 0s - loss: 9.4098 - acc: 0.312 - ETA: 0s - loss: 9.3994 - acc: 0.313 - ETA: 0s - loss: 9.4011 - acc: 0.312 - ETA: 0s - loss: 9.4058 - acc: 0.312 - ETA: 0s - loss: 9.4159 - acc: 0.311 - ETA: 0s - loss: 9.4104 - acc: 0.313 - ETA: 0s - loss: 9.4120 - acc: 0.314 - ETA: 0s - loss: 9.3704 - acc: 0.316 - ETA: 0s - loss: 9.3462 - acc: 0.317 - ETA: 0s - loss: 9.3455 - acc: 0.317 - ETA: 0s - loss: 9.3401 - acc: 0.317 - ETA: 0s - loss: 9.3237 - acc: 0.318 - ETA: 0s - loss: 9.3203 - acc: 0.319 - ETA: 0s - loss: 9.3000 - acc: 0.320 - ETA: 0s - loss: 9.3122 - acc: 0.320 - ETA: 0s - loss: 9.3118 - acc: 0.321 - 3s 393us/step - loss: 9.3011 - acc: 0.3225 - val_loss: 9.1352 - val_acc: 0.3413\n",
      "\n",
      "Epoch 00002: val_loss improved from 9.94793 to 9.13525, saving model to model/notebook/weights.best.VGG16.hdf5\n",
      "Epoch 3/20\n",
      "6680/6680 [==============================] - ETA: 3s - loss: 8.3850 - acc: 0.400 - ETA: 2s - loss: 8.0417 - acc: 0.406 - ETA: 2s - loss: 8.5972 - acc: 0.381 - ETA: 2s - loss: 8.4289 - acc: 0.395 - ETA: 2s - loss: 8.3923 - acc: 0.397 - ETA: 1s - loss: 8.5466 - acc: 0.389 - ETA: 2s - loss: 8.5019 - acc: 0.392 - ETA: 1s - loss: 8.5316 - acc: 0.393 - ETA: 1s - loss: 8.5431 - acc: 0.392 - ETA: 1s - loss: 8.5350 - acc: 0.390 - ETA: 1s - loss: 8.5788 - acc: 0.384 - ETA: 1s - loss: 8.6205 - acc: 0.382 - ETA: 1s - loss: 8.5770 - acc: 0.387 - ETA: 1s - loss: 8.5813 - acc: 0.385 - ETA: 1s - loss: 8.5610 - acc: 0.388 - ETA: 1s - loss: 8.6106 - acc: 0.384 - ETA: 1s - loss: 8.6209 - acc: 0.385 - ETA: 1s - loss: 8.6717 - acc: 0.384 - ETA: 1s - loss: 8.6693 - acc: 0.385 - ETA: 1s - loss: 8.6680 - acc: 0.386 - ETA: 1s - loss: 8.6715 - acc: 0.386 - ETA: 1s - loss: 8.6895 - acc: 0.385 - ETA: 1s - loss: 8.7386 - acc: 0.383 - ETA: 1s - loss: 8.7174 - acc: 0.383 - ETA: 1s - loss: 8.7384 - acc: 0.381 - ETA: 0s - loss: 8.6860 - acc: 0.385 - ETA: 0s - loss: 8.6523 - acc: 0.386 - ETA: 0s - loss: 8.6620 - acc: 0.385 - ETA: 0s - loss: 8.6769 - acc: 0.385 - ETA: 0s - loss: 8.6520 - acc: 0.386 - ETA: 0s - loss: 8.6759 - acc: 0.385 - ETA: 0s - loss: 8.6827 - acc: 0.385 - ETA: 0s - loss: 8.6671 - acc: 0.386 - ETA: 0s - loss: 8.6804 - acc: 0.385 - ETA: 0s - loss: 8.6854 - acc: 0.386 - ETA: 0s - loss: 8.6855 - acc: 0.386 - ETA: 0s - loss: 8.6798 - acc: 0.386 - ETA: 0s - loss: 8.6901 - acc: 0.385 - ETA: 0s - loss: 8.6723 - acc: 0.385 - ETA: 0s - loss: 8.6810 - acc: 0.385 - ETA: 0s - loss: 8.6623 - acc: 0.385 - ETA: 0s - loss: 8.6870 - acc: 0.384 - ETA: 0s - loss: 8.6905 - acc: 0.384 - ETA: 0s - loss: 8.6612 - acc: 0.386 - ETA: 0s - loss: 8.6648 - acc: 0.385 - 3s 390us/step - loss: 8.6614 - acc: 0.3858 - val_loss: 8.8180 - val_acc: 0.3581\n",
      "\n",
      "Epoch 00003: val_loss improved from 9.13525 to 8.81799, saving model to model/notebook/weights.best.VGG16.hdf5\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 2s - loss: 8.3799 - acc: 0.400 - ETA: 6s - loss: 8.6672 - acc: 0.387 - ETA: 4s - loss: 8.3492 - acc: 0.416 - ETA: 4s - loss: 8.4250 - acc: 0.425 - ETA: 4s - loss: 8.2913 - acc: 0.426 - ETA: 3s - loss: 8.1860 - acc: 0.431 - ETA: 3s - loss: 7.9451 - acc: 0.444 - ETA: 3s - loss: 7.9364 - acc: 0.446 - ETA: 3s - loss: 7.9776 - acc: 0.450 - ETA: 3s - loss: 7.9413 - acc: 0.452 - ETA: 3s - loss: 8.1176 - acc: 0.442 - ETA: 3s - loss: 8.0150 - acc: 0.450 - ETA: 3s - loss: 8.0074 - acc: 0.450 - ETA: 3s - loss: 7.9980 - acc: 0.450 - ETA: 3s - loss: 8.0388 - acc: 0.447 - ETA: 3s - loss: 8.0073 - acc: 0.449 - ETA: 3s - loss: 8.0771 - acc: 0.442 - ETA: 3s - loss: 8.1011 - acc: 0.440 - ETA: 3s - loss: 8.1783 - acc: 0.435 - ETA: 3s - loss: 8.1635 - acc: 0.437 - ETA: 3s - loss: 8.1560 - acc: 0.436 - ETA: 3s - loss: 8.2263 - acc: 0.433 - ETA: 3s - loss: 8.2515 - acc: 0.432 - ETA: 3s - loss: 8.2408 - acc: 0.430 - ETA: 3s - loss: 8.1877 - acc: 0.430 - ETA: 3s - loss: 8.2011 - acc: 0.428 - ETA: 3s - loss: 8.1883 - acc: 0.430 - ETA: 3s - loss: 8.2244 - acc: 0.426 - ETA: 3s - loss: 8.2729 - acc: 0.424 - ETA: 3s - loss: 8.3207 - acc: 0.419 - ETA: 3s - loss: 8.3207 - acc: 0.419 - ETA: 3s - loss: 8.2986 - acc: 0.421 - ETA: 2s - loss: 8.3013 - acc: 0.422 - ETA: 2s - loss: 8.3046 - acc: 0.420 - ETA: 2s - loss: 8.2989 - acc: 0.421 - ETA: 2s - loss: 8.2858 - acc: 0.423 - ETA: 2s - loss: 8.3204 - acc: 0.421 - ETA: 2s - loss: 8.3060 - acc: 0.422 - ETA: 2s - loss: 8.2581 - acc: 0.424 - ETA: 2s - loss: 8.2301 - acc: 0.425 - ETA: 2s - loss: 8.2555 - acc: 0.423 - ETA: 2s - loss: 8.2804 - acc: 0.421 - ETA: 1s - loss: 8.2840 - acc: 0.420 - ETA: 1s - loss: 8.2550 - acc: 0.421 - ETA: 1s - loss: 8.2591 - acc: 0.421 - ETA: 1s - loss: 8.2347 - acc: 0.422 - ETA: 1s - loss: 8.2312 - acc: 0.422 - ETA: 1s - loss: 8.2464 - acc: 0.419 - ETA: 1s - loss: 8.2513 - acc: 0.419 - ETA: 1s - loss: 8.2375 - acc: 0.420 - ETA: 1s - loss: 8.2313 - acc: 0.420 - ETA: 0s - loss: 8.2198 - acc: 0.422 - ETA: 0s - loss: 8.2243 - acc: 0.422 - ETA: 0s - loss: 8.2440 - acc: 0.421 - ETA: 0s - loss: 8.2136 - acc: 0.423 - ETA: 0s - loss: 8.2261 - acc: 0.421 - ETA: 0s - loss: 8.2114 - acc: 0.421 - ETA: 0s - loss: 8.1926 - acc: 0.421 - ETA: 0s - loss: 8.1885 - acc: 0.422 - ETA: 0s - loss: 8.1996 - acc: 0.422 - ETA: 0s - loss: 8.1995 - acc: 0.423 - ETA: 0s - loss: 8.1954 - acc: 0.424 - ETA: 0s - loss: 8.1848 - acc: 0.424 - 4s 530us/step - loss: 8.1863 - acc: 0.4244 - val_loss: 8.3334 - val_acc: 0.3952\n",
      "\n",
      "Epoch 00004: val_loss improved from 8.81799 to 8.33344, saving model to model/notebook/weights.best.VGG16.hdf5\n",
      "Epoch 5/20\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 5.7841 - acc: 0.600 - ETA: 3s - loss: 7.5184 - acc: 0.510 - ETA: 2s - loss: 7.2328 - acc: 0.519 - ETA: 2s - loss: 7.4423 - acc: 0.486 - ETA: 2s - loss: 7.6240 - acc: 0.463 - ETA: 2s - loss: 7.3698 - acc: 0.480 - ETA: 2s - loss: 7.4638 - acc: 0.478 - ETA: 2s - loss: 7.4169 - acc: 0.484 - ETA: 2s - loss: 7.5327 - acc: 0.477 - ETA: 2s - loss: 7.5949 - acc: 0.472 - ETA: 2s - loss: 7.6524 - acc: 0.469 - ETA: 1s - loss: 7.5196 - acc: 0.472 - ETA: 1s - loss: 7.4797 - acc: 0.472 - ETA: 1s - loss: 7.5142 - acc: 0.466 - ETA: 1s - loss: 7.4942 - acc: 0.470 - ETA: 1s - loss: 7.5814 - acc: 0.466 - ETA: 1s - loss: 7.6123 - acc: 0.464 - ETA: 1s - loss: 7.6356 - acc: 0.463 - ETA: 1s - loss: 7.7139 - acc: 0.457 - ETA: 1s - loss: 7.6755 - acc: 0.458 - ETA: 1s - loss: 7.6652 - acc: 0.457 - ETA: 1s - loss: 7.6166 - acc: 0.461 - ETA: 1s - loss: 7.6471 - acc: 0.459 - ETA: 1s - loss: 7.6440 - acc: 0.459 - ETA: 1s - loss: 7.6565 - acc: 0.457 - ETA: 1s - loss: 7.6468 - acc: 0.457 - ETA: 1s - loss: 7.6387 - acc: 0.458 - ETA: 1s - loss: 7.7148 - acc: 0.454 - ETA: 0s - loss: 7.6811 - acc: 0.455 - ETA: 0s - loss: 7.7100 - acc: 0.454 - ETA: 0s - loss: 7.7353 - acc: 0.452 - ETA: 0s - loss: 7.7405 - acc: 0.451 - ETA: 0s - loss: 7.7406 - acc: 0.451 - ETA: 0s - loss: 7.7319 - acc: 0.451 - ETA: 0s - loss: 7.7279 - acc: 0.451 - ETA: 0s - loss: 7.7156 - acc: 0.451 - ETA: 0s - loss: 7.7163 - acc: 0.451 - ETA: 0s - loss: 7.6899 - acc: 0.453 - ETA: 0s - loss: 7.6631 - acc: 0.455 - ETA: 0s - loss: 7.6607 - acc: 0.455 - ETA: 0s - loss: 7.6676 - acc: 0.455 - ETA: 0s - loss: 7.6915 - acc: 0.453 - ETA: 0s - loss: 7.6884 - acc: 0.453 - ETA: 0s - loss: 7.6665 - acc: 0.455 - ETA: 0s - loss: 7.6630 - acc: 0.455 - ETA: 0s - loss: 7.6487 - acc: 0.457 - ETA: 0s - loss: 7.6284 - acc: 0.459 - ETA: 0s - loss: 7.6625 - acc: 0.457 - ETA: 0s - loss: 7.6457 - acc: 0.458 - ETA: 0s - loss: 7.6330 - acc: 0.459 - 3s 430us/step - loss: 7.6054 - acc: 0.4612 - val_loss: 7.8908 - val_acc: 0.4144\n",
      "\n",
      "Epoch 00005: val_loss improved from 8.33344 to 7.89078, saving model to model/notebook/weights.best.VGG16.hdf5\n",
      "Epoch 6/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 6.3902 - acc: 0.550 - ETA: 6s - loss: 7.9953 - acc: 0.450 - ETA: 6s - loss: 8.4097 - acc: 0.428 - ETA: 5s - loss: 7.7928 - acc: 0.463 - ETA: 4s - loss: 7.4062 - acc: 0.485 - ETA: 4s - loss: 7.8018 - acc: 0.454 - ETA: 3s - loss: 7.6535 - acc: 0.466 - ETA: 3s - loss: 7.4412 - acc: 0.470 - ETA: 2s - loss: 7.3286 - acc: 0.478 - ETA: 2s - loss: 7.2850 - acc: 0.482 - ETA: 2s - loss: 7.2963 - acc: 0.483 - ETA: 2s - loss: 7.2191 - acc: 0.490 - ETA: 2s - loss: 7.1480 - acc: 0.496 - ETA: 2s - loss: 7.0415 - acc: 0.502 - ETA: 2s - loss: 7.0272 - acc: 0.503 - ETA: 2s - loss: 7.0436 - acc: 0.503 - ETA: 1s - loss: 7.0996 - acc: 0.502 - ETA: 1s - loss: 7.1088 - acc: 0.501 - ETA: 1s - loss: 7.0717 - acc: 0.502 - ETA: 1s - loss: 7.0557 - acc: 0.505 - ETA: 1s - loss: 7.1013 - acc: 0.503 - ETA: 1s - loss: 7.1069 - acc: 0.503 - ETA: 1s - loss: 7.1282 - acc: 0.502 - ETA: 1s - loss: 7.1359 - acc: 0.501 - ETA: 1s - loss: 7.1664 - acc: 0.500 - ETA: 1s - loss: 7.1451 - acc: 0.502 - ETA: 1s - loss: 7.2135 - acc: 0.497 - ETA: 1s - loss: 7.2295 - acc: 0.496 - ETA: 1s - loss: 7.2111 - acc: 0.498 - ETA: 0s - loss: 7.2260 - acc: 0.497 - ETA: 0s - loss: 7.2325 - acc: 0.496 - ETA: 0s - loss: 7.2317 - acc: 0.497 - ETA: 0s - loss: 7.2483 - acc: 0.496 - ETA: 0s - loss: 7.2575 - acc: 0.495 - ETA: 0s - loss: 7.2308 - acc: 0.496 - ETA: 0s - loss: 7.2184 - acc: 0.497 - ETA: 0s - loss: 7.2332 - acc: 0.496 - ETA: 0s - loss: 7.2427 - acc: 0.496 - ETA: 0s - loss: 7.2476 - acc: 0.495 - ETA: 0s - loss: 7.2647 - acc: 0.494 - ETA: 0s - loss: 7.2700 - acc: 0.494 - ETA: 0s - loss: 7.2561 - acc: 0.495 - ETA: 0s - loss: 7.2585 - acc: 0.495 - ETA: 0s - loss: 7.2652 - acc: 0.495 - ETA: 0s - loss: 7.2645 - acc: 0.495 - 3s 381us/step - loss: 7.2564 - acc: 0.4952 - val_loss: 7.6765 - val_acc: 0.4240\n",
      "\n",
      "Epoch 00006: val_loss improved from 7.89078 to 7.67651, saving model to model/notebook/weights.best.VGG16.hdf5\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 1s - loss: 6.7473 - acc: 0.550 - ETA: 2s - loss: 7.5536 - acc: 0.500 - ETA: 2s - loss: 7.5535 - acc: 0.503 - ETA: 2s - loss: 7.3845 - acc: 0.515 - ETA: 2s - loss: 7.1182 - acc: 0.530 - ETA: 2s - loss: 6.9913 - acc: 0.537 - ETA: 2s - loss: 6.9351 - acc: 0.542 - ETA: 2s - loss: 6.7817 - acc: 0.552 - ETA: 2s - loss: 6.7665 - acc: 0.550 - ETA: 2s - loss: 6.7413 - acc: 0.553 - ETA: 2s - loss: 6.7285 - acc: 0.553 - ETA: 2s - loss: 6.8183 - acc: 0.548 - ETA: 1s - loss: 6.8377 - acc: 0.543 - ETA: 1s - loss: 6.9621 - acc: 0.533 - ETA: 1s - loss: 6.9525 - acc: 0.532 - ETA: 1s - loss: 6.9711 - acc: 0.531 - ETA: 1s - loss: 7.0431 - acc: 0.528 - ETA: 1s - loss: 7.0197 - acc: 0.531 - ETA: 1s - loss: 7.0597 - acc: 0.527 - ETA: 1s - loss: 7.0549 - acc: 0.527 - ETA: 1s - loss: 7.0884 - acc: 0.525 - ETA: 1s - loss: 7.1568 - acc: 0.520 - ETA: 1s - loss: 7.1375 - acc: 0.521 - ETA: 1s - loss: 7.1227 - acc: 0.522 - ETA: 1s - loss: 7.1234 - acc: 0.522 - ETA: 1s - loss: 7.0711 - acc: 0.525 - ETA: 1s - loss: 7.0814 - acc: 0.525 - ETA: 1s - loss: 7.0533 - acc: 0.527 - ETA: 0s - loss: 7.0723 - acc: 0.526 - ETA: 0s - loss: 7.0666 - acc: 0.527 - ETA: 0s - loss: 7.0717 - acc: 0.526 - ETA: 0s - loss: 7.1223 - acc: 0.523 - ETA: 0s - loss: 7.0915 - acc: 0.525 - ETA: 0s - loss: 7.1217 - acc: 0.523 - ETA: 0s - loss: 7.1055 - acc: 0.523 - ETA: 0s - loss: 7.1026 - acc: 0.523 - ETA: 0s - loss: 7.1017 - acc: 0.522 - ETA: 0s - loss: 7.1028 - acc: 0.522 - ETA: 0s - loss: 7.0951 - acc: 0.521 - ETA: 0s - loss: 7.0814 - acc: 0.522 - ETA: 0s - loss: 7.0809 - acc: 0.521 - ETA: 0s - loss: 7.0859 - acc: 0.520 - ETA: 0s - loss: 7.1031 - acc: 0.520 - ETA: 0s - loss: 7.1018 - acc: 0.520 - ETA: 0s - loss: 7.0939 - acc: 0.520 - 3s 382us/step - loss: 7.1069 - acc: 0.5193 - val_loss: 7.6108 - val_acc: 0.4407\n",
      "\n",
      "Epoch 00007: val_loss improved from 7.67651 to 7.61085, saving model to model/notebook/weights.best.VGG16.hdf5\n",
      "Epoch 8/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 7.3272 - acc: 0.500 - ETA: 2s - loss: 6.7350 - acc: 0.537 - ETA: 2s - loss: 6.4166 - acc: 0.562 - ETA: 2s - loss: 6.5279 - acc: 0.565 - ETA: 2s - loss: 6.7378 - acc: 0.554 - ETA: 2s - loss: 6.8326 - acc: 0.546 - ETA: 1s - loss: 6.8634 - acc: 0.543 - ETA: 1s - loss: 6.7298 - acc: 0.552 - ETA: 1s - loss: 6.7308 - acc: 0.550 - ETA: 1s - loss: 6.8184 - acc: 0.543 - ETA: 1s - loss: 6.8542 - acc: 0.542 - ETA: 1s - loss: 6.8369 - acc: 0.543 - ETA: 1s - loss: 6.9019 - acc: 0.539 - ETA: 1s - loss: 6.8835 - acc: 0.542 - ETA: 1s - loss: 6.8657 - acc: 0.542 - ETA: 1s - loss: 6.8880 - acc: 0.539 - ETA: 1s - loss: 6.8872 - acc: 0.538 - ETA: 1s - loss: 6.8279 - acc: 0.541 - ETA: 1s - loss: 6.8202 - acc: 0.540 - ETA: 1s - loss: 6.8524 - acc: 0.538 - ETA: 1s - loss: 6.8518 - acc: 0.538 - ETA: 1s - loss: 6.8681 - acc: 0.535 - ETA: 0s - loss: 6.8851 - acc: 0.534 - ETA: 0s - loss: 6.8908 - acc: 0.533 - ETA: 0s - loss: 6.8478 - acc: 0.535 - ETA: 0s - loss: 6.8679 - acc: 0.533 - ETA: 0s - loss: 6.9063 - acc: 0.529 - ETA: 0s - loss: 6.9332 - acc: 0.526 - ETA: 0s - loss: 6.9253 - acc: 0.526 - ETA: 0s - loss: 6.9283 - acc: 0.525 - ETA: 0s - loss: 6.9001 - acc: 0.526 - ETA: 0s - loss: 6.8896 - acc: 0.526 - ETA: 0s - loss: 6.8571 - acc: 0.527 - ETA: 0s - loss: 6.8553 - acc: 0.528 - ETA: 0s - loss: 6.8826 - acc: 0.527 - ETA: 0s - loss: 6.8961 - acc: 0.526 - ETA: 0s - loss: 6.9030 - acc: 0.526 - ETA: 0s - loss: 6.8804 - acc: 0.528 - ETA: 0s - loss: 6.8863 - acc: 0.528 - ETA: 0s - loss: 6.8561 - acc: 0.529 - 2s 338us/step - loss: 6.8640 - acc: 0.5293 - val_loss: 7.3490 - val_acc: 0.4695\n",
      "\n",
      "Epoch 00008: val_loss improved from 7.61085 to 7.34901, saving model to model/notebook/weights.best.VGG16.hdf5\n",
      "Epoch 9/20\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 8.0618 - acc: 0.500 - ETA: 1s - loss: 6.3229 - acc: 0.580 - ETA: 1s - loss: 6.0793 - acc: 0.592 - ETA: 1s - loss: 6.0071 - acc: 0.601 - ETA: 1s - loss: 6.0568 - acc: 0.600 - ETA: 1s - loss: 6.1276 - acc: 0.590 - ETA: 1s - loss: 6.2685 - acc: 0.579 - ETA: 1s - loss: 6.3617 - acc: 0.573 - ETA: 1s - loss: 6.3025 - acc: 0.580 - ETA: 1s - loss: 6.4551 - acc: 0.569 - ETA: 1s - loss: 6.4947 - acc: 0.566 - ETA: 1s - loss: 6.4210 - acc: 0.571 - ETA: 1s - loss: 6.4646 - acc: 0.567 - ETA: 1s - loss: 6.4578 - acc: 0.567 - ETA: 1s - loss: 6.5052 - acc: 0.565 - ETA: 1s - loss: 6.5368 - acc: 0.563 - ETA: 1s - loss: 6.5800 - acc: 0.562 - ETA: 1s - loss: 6.5896 - acc: 0.561 - ETA: 1s - loss: 6.5342 - acc: 0.563 - ETA: 1s - loss: 6.5086 - acc: 0.564 - ETA: 1s - loss: 6.4834 - acc: 0.565 - ETA: 1s - loss: 6.4765 - acc: 0.566 - ETA: 1s - loss: 6.4608 - acc: 0.567 - ETA: 1s - loss: 6.4193 - acc: 0.569 - ETA: 1s - loss: 6.4120 - acc: 0.569 - ETA: 1s - loss: 6.4720 - acc: 0.566 - ETA: 0s - loss: 6.4608 - acc: 0.567 - ETA: 0s - loss: 6.4438 - acc: 0.568 - ETA: 0s - loss: 6.4509 - acc: 0.567 - ETA: 0s - loss: 6.4823 - acc: 0.564 - ETA: 0s - loss: 6.4471 - acc: 0.566 - ETA: 0s - loss: 6.4600 - acc: 0.565 - ETA: 0s - loss: 6.4891 - acc: 0.564 - ETA: 0s - loss: 6.5222 - acc: 0.562 - ETA: 0s - loss: 6.5605 - acc: 0.559 - ETA: 0s - loss: 6.5977 - acc: 0.557 - ETA: 0s - loss: 6.6131 - acc: 0.556 - ETA: 0s - loss: 6.5949 - acc: 0.557 - ETA: 0s - loss: 6.5987 - acc: 0.556 - ETA: 0s - loss: 6.5984 - acc: 0.556 - ETA: 0s - loss: 6.6040 - acc: 0.555 - ETA: 0s - loss: 6.6337 - acc: 0.554 - ETA: 0s - loss: 6.6467 - acc: 0.552 - 2s 359us/step - loss: 6.6533 - acc: 0.5518 - val_loss: 7.3949 - val_acc: 0.4467\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 7.34901\n",
      "Epoch 10/20\n",
      "6680/6680 [==============================] - ETA: 5s - loss: 6.5890 - acc: 0.500 - ETA: 2s - loss: 6.4111 - acc: 0.562 - ETA: 2s - loss: 6.3647 - acc: 0.575 - ETA: 2s - loss: 6.3154 - acc: 0.581 - ETA: 2s - loss: 6.4542 - acc: 0.575 - ETA: 2s - loss: 6.4859 - acc: 0.576 - ETA: 1s - loss: 6.5242 - acc: 0.573 - ETA: 1s - loss: 6.5180 - acc: 0.574 - ETA: 1s - loss: 6.5244 - acc: 0.571 - ETA: 1s - loss: 6.4981 - acc: 0.573 - ETA: 1s - loss: 6.4726 - acc: 0.574 - ETA: 1s - loss: 6.4213 - acc: 0.578 - ETA: 1s - loss: 6.4724 - acc: 0.576 - ETA: 1s - loss: 6.4962 - acc: 0.574 - ETA: 1s - loss: 6.4510 - acc: 0.577 - ETA: 1s - loss: 6.4913 - acc: 0.573 - ETA: 1s - loss: 6.4791 - acc: 0.574 - ETA: 1s - loss: 6.4185 - acc: 0.576 - ETA: 1s - loss: 6.4538 - acc: 0.574 - ETA: 1s - loss: 6.4866 - acc: 0.572 - ETA: 1s - loss: 6.5472 - acc: 0.569 - ETA: 1s - loss: 6.5685 - acc: 0.568 - ETA: 1s - loss: 6.5553 - acc: 0.568 - ETA: 0s - loss: 6.5773 - acc: 0.566 - ETA: 0s - loss: 6.5638 - acc: 0.566 - ETA: 0s - loss: 6.5575 - acc: 0.565 - ETA: 0s - loss: 6.5601 - acc: 0.565 - ETA: 0s - loss: 6.5664 - acc: 0.565 - ETA: 0s - loss: 6.5695 - acc: 0.565 - ETA: 0s - loss: 6.5636 - acc: 0.565 - ETA: 0s - loss: 6.5721 - acc: 0.563 - ETA: 0s - loss: 6.5419 - acc: 0.565 - ETA: 0s - loss: 6.5584 - acc: 0.564 - ETA: 0s - loss: 6.5367 - acc: 0.565 - ETA: 0s - loss: 6.5009 - acc: 0.567 - ETA: 0s - loss: 6.5120 - acc: 0.566 - ETA: 0s - loss: 6.5283 - acc: 0.564 - ETA: 0s - loss: 6.5489 - acc: 0.563 - ETA: 0s - loss: 6.5521 - acc: 0.562 - ETA: 0s - loss: 6.5203 - acc: 0.564 - ETA: 0s - loss: 6.5329 - acc: 0.563 - ETA: 0s - loss: 6.5242 - acc: 0.564 - 2s 355us/step - loss: 6.5209 - acc: 0.5647 - val_loss: 7.1224 - val_acc: 0.4743\n",
      "\n",
      "Epoch 00010: val_loss improved from 7.34901 to 7.12237, saving model to model/notebook/weights.best.VGG16.hdf5\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 1s - loss: 4.3393 - acc: 0.600 - ETA: 2s - loss: 6.6044 - acc: 0.568 - ETA: 2s - loss: 6.4729 - acc: 0.580 - ETA: 2s - loss: 6.3289 - acc: 0.588 - ETA: 2s - loss: 6.4473 - acc: 0.576 - ETA: 2s - loss: 6.3709 - acc: 0.583 - ETA: 2s - loss: 6.2388 - acc: 0.592 - ETA: 2s - loss: 6.0634 - acc: 0.601 - ETA: 1s - loss: 6.1557 - acc: 0.591 - ETA: 1s - loss: 6.1793 - acc: 0.588 - ETA: 1s - loss: 6.0837 - acc: 0.595 - ETA: 1s - loss: 6.0910 - acc: 0.592 - ETA: 1s - loss: 6.0805 - acc: 0.593 - ETA: 1s - loss: 6.1472 - acc: 0.591 - ETA: 1s - loss: 6.1285 - acc: 0.591 - ETA: 1s - loss: 6.2471 - acc: 0.583 - ETA: 1s - loss: 6.2642 - acc: 0.583 - ETA: 1s - loss: 6.2897 - acc: 0.580 - ETA: 1s - loss: 6.2987 - acc: 0.579 - ETA: 1s - loss: 6.3670 - acc: 0.574 - ETA: 1s - loss: 6.3228 - acc: 0.577 - ETA: 1s - loss: 6.2755 - acc: 0.579 - ETA: 1s - loss: 6.2800 - acc: 0.579 - ETA: 1s - loss: 6.2434 - acc: 0.581 - ETA: 0s - loss: 6.2321 - acc: 0.582 - ETA: 0s - loss: 6.2473 - acc: 0.580 - ETA: 0s - loss: 6.2193 - acc: 0.582 - ETA: 0s - loss: 6.2150 - acc: 0.582 - ETA: 0s - loss: 6.2276 - acc: 0.582 - ETA: 0s - loss: 6.2490 - acc: 0.581 - ETA: 0s - loss: 6.2300 - acc: 0.583 - ETA: 0s - loss: 6.2173 - acc: 0.584 - ETA: 0s - loss: 6.2476 - acc: 0.583 - ETA: 0s - loss: 6.2436 - acc: 0.583 - ETA: 0s - loss: 6.2429 - acc: 0.583 - ETA: 0s - loss: 6.2352 - acc: 0.583 - ETA: 0s - loss: 6.2217 - acc: 0.584 - ETA: 0s - loss: 6.2252 - acc: 0.584 - ETA: 0s - loss: 6.2447 - acc: 0.583 - ETA: 0s - loss: 6.2477 - acc: 0.583 - 2s 335us/step - loss: 6.2300 - acc: 0.5844 - val_loss: 6.9725 - val_acc: 0.4862\n",
      "\n",
      "Epoch 00011: val_loss improved from 7.12237 to 6.97255, saving model to model/notebook/weights.best.VGG16.hdf5\n",
      "Epoch 12/20\n",
      "6680/6680 [==============================] - ETA: 3s - loss: 4.0446 - acc: 0.750 - ETA: 2s - loss: 5.9963 - acc: 0.588 - ETA: 1s - loss: 6.1036 - acc: 0.597 - ETA: 1s - loss: 6.1840 - acc: 0.594 - ETA: 1s - loss: 6.0061 - acc: 0.602 - ETA: 1s - loss: 6.0442 - acc: 0.601 - ETA: 1s - loss: 6.0063 - acc: 0.604 - ETA: 1s - loss: 5.9915 - acc: 0.605 - ETA: 1s - loss: 5.9720 - acc: 0.608 - ETA: 1s - loss: 6.0501 - acc: 0.605 - ETA: 1s - loss: 6.1016 - acc: 0.600 - ETA: 1s - loss: 6.1098 - acc: 0.602 - ETA: 1s - loss: 6.1025 - acc: 0.601 - ETA: 1s - loss: 6.1222 - acc: 0.599 - ETA: 1s - loss: 6.0105 - acc: 0.605 - ETA: 1s - loss: 6.0434 - acc: 0.603 - ETA: 1s - loss: 6.0681 - acc: 0.602 - ETA: 1s - loss: 6.0358 - acc: 0.602 - ETA: 0s - loss: 6.0482 - acc: 0.602 - ETA: 0s - loss: 6.0420 - acc: 0.603 - ETA: 0s - loss: 6.0068 - acc: 0.605 - ETA: 0s - loss: 6.0400 - acc: 0.603 - ETA: 0s - loss: 6.0603 - acc: 0.603 - ETA: 0s - loss: 6.0634 - acc: 0.603 - ETA: 0s - loss: 6.0807 - acc: 0.602 - ETA: 0s - loss: 6.0858 - acc: 0.602 - ETA: 0s - loss: 6.1020 - acc: 0.601 - ETA: 0s - loss: 6.1106 - acc: 0.600 - ETA: 0s - loss: 6.1053 - acc: 0.600 - ETA: 0s - loss: 6.1131 - acc: 0.599 - ETA: 0s - loss: 6.0956 - acc: 0.601 - ETA: 0s - loss: 6.0838 - acc: 0.601 - ETA: 0s - loss: 6.0856 - acc: 0.601 - ETA: 0s - loss: 6.0882 - acc: 0.601 - ETA: 0s - loss: 6.1220 - acc: 0.599 - ETA: 0s - loss: 6.1006 - acc: 0.601 - ETA: 0s - loss: 6.1043 - acc: 0.601 - 2s 310us/step - loss: 6.1149 - acc: 0.6006 - val_loss: 6.9209 - val_acc: 0.5006\n",
      "\n",
      "Epoch 00012: val_loss improved from 6.97255 to 6.92088, saving model to model/notebook/weights.best.VGG16.hdf5\n",
      "Epoch 13/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 5.6414 - acc: 0.650 - ETA: 1s - loss: 5.8490 - acc: 0.631 - ETA: 1s - loss: 6.1851 - acc: 0.605 - ETA: 1s - loss: 5.9940 - acc: 0.607 - ETA: 1s - loss: 6.0194 - acc: 0.609 - ETA: 1s - loss: 5.9360 - acc: 0.615 - ETA: 1s - loss: 6.0480 - acc: 0.607 - ETA: 1s - loss: 6.0500 - acc: 0.609 - ETA: 1s - loss: 5.9946 - acc: 0.611 - ETA: 1s - loss: 5.8263 - acc: 0.621 - ETA: 1s - loss: 5.7285 - acc: 0.625 - ETA: 1s - loss: 5.7900 - acc: 0.621 - ETA: 1s - loss: 5.8286 - acc: 0.617 - ETA: 1s - loss: 5.8781 - acc: 0.614 - ETA: 1s - loss: 5.9403 - acc: 0.610 - ETA: 1s - loss: 5.9959 - acc: 0.606 - ETA: 1s - loss: 6.0132 - acc: 0.605 - ETA: 1s - loss: 6.0243 - acc: 0.604 - ETA: 1s - loss: 6.0322 - acc: 0.604 - ETA: 1s - loss: 6.0069 - acc: 0.606 - ETA: 0s - loss: 6.0065 - acc: 0.606 - ETA: 0s - loss: 6.0050 - acc: 0.607 - ETA: 0s - loss: 5.9922 - acc: 0.608 - ETA: 0s - loss: 5.9475 - acc: 0.611 - ETA: 0s - loss: 5.9136 - acc: 0.613 - ETA: 0s - loss: 5.9471 - acc: 0.611 - ETA: 0s - loss: 5.9895 - acc: 0.608 - ETA: 0s - loss: 6.0065 - acc: 0.607 - ETA: 0s - loss: 5.9810 - acc: 0.609 - ETA: 0s - loss: 5.9825 - acc: 0.608 - ETA: 0s - loss: 5.9754 - acc: 0.609 - ETA: 0s - loss: 5.9575 - acc: 0.610 - ETA: 0s - loss: 5.9807 - acc: 0.608 - ETA: 0s - loss: 6.0116 - acc: 0.606 - ETA: 0s - loss: 6.0199 - acc: 0.606 - ETA: 0s - loss: 6.0195 - acc: 0.606 - ETA: 0s - loss: 5.9998 - acc: 0.607 - ETA: 0s - loss: 6.0177 - acc: 0.606 - ETA: 0s - loss: 5.9880 - acc: 0.608 - ETA: 0s - loss: 5.9900 - acc: 0.607 - ETA: 0s - loss: 5.9919 - acc: 0.607 - ETA: 0s - loss: 5.9803 - acc: 0.607 - 2s 344us/step - loss: 5.9844 - acc: 0.6070 - val_loss: 6.8667 - val_acc: 0.4862\n",
      "\n",
      "Epoch 00013: val_loss improved from 6.92088 to 6.86673, saving model to model/notebook/weights.best.VGG16.hdf5\n",
      "Epoch 14/20\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 6.4510 - acc: 0.600 - ETA: 1s - loss: 6.2784 - acc: 0.585 - ETA: 1s - loss: 5.6681 - acc: 0.627 - ETA: 1s - loss: 5.5595 - acc: 0.638 - ETA: 1s - loss: 5.5364 - acc: 0.642 - ETA: 1s - loss: 5.7274 - acc: 0.630 - ETA: 1s - loss: 5.8303 - acc: 0.624 - ETA: 1s - loss: 5.8819 - acc: 0.620 - ETA: 1s - loss: 5.9037 - acc: 0.620 - ETA: 1s - loss: 5.8532 - acc: 0.623 - ETA: 1s - loss: 5.8814 - acc: 0.619 - ETA: 1s - loss: 5.7678 - acc: 0.627 - ETA: 1s - loss: 5.7467 - acc: 0.629 - ETA: 1s - loss: 5.8469 - acc: 0.622 - ETA: 1s - loss: 5.9034 - acc: 0.619 - ETA: 1s - loss: 5.8941 - acc: 0.619 - ETA: 1s - loss: 5.8175 - acc: 0.622 - ETA: 1s - loss: 5.7909 - acc: 0.624 - ETA: 1s - loss: 5.7818 - acc: 0.623 - ETA: 1s - loss: 5.7609 - acc: 0.625 - ETA: 1s - loss: 5.7852 - acc: 0.623 - ETA: 0s - loss: 5.8310 - acc: 0.620 - ETA: 0s - loss: 5.8332 - acc: 0.619 - ETA: 0s - loss: 5.8567 - acc: 0.618 - ETA: 0s - loss: 5.8717 - acc: 0.617 - ETA: 0s - loss: 5.8930 - acc: 0.615 - ETA: 0s - loss: 5.8822 - acc: 0.615 - ETA: 0s - loss: 5.8702 - acc: 0.615 - ETA: 0s - loss: 5.8587 - acc: 0.616 - ETA: 0s - loss: 5.8524 - acc: 0.616 - ETA: 0s - loss: 5.8554 - acc: 0.616 - ETA: 0s - loss: 5.7999 - acc: 0.620 - ETA: 0s - loss: 5.7830 - acc: 0.620 - ETA: 0s - loss: 5.7855 - acc: 0.620 - ETA: 0s - loss: 5.7897 - acc: 0.620 - ETA: 0s - loss: 5.7829 - acc: 0.620 - ETA: 0s - loss: 5.7854 - acc: 0.621 - ETA: 0s - loss: 5.8001 - acc: 0.619 - ETA: 0s - loss: 5.8173 - acc: 0.618 - ETA: 0s - loss: 5.8174 - acc: 0.618 - ETA: 0s - loss: 5.8017 - acc: 0.619 - ETA: 0s - loss: 5.8088 - acc: 0.619 - 2s 344us/step - loss: 5.7963 - acc: 0.6202 - val_loss: 6.6235 - val_acc: 0.4994\n",
      "\n",
      "Epoch 00014: val_loss improved from 6.86673 to 6.62355, saving model to model/notebook/weights.best.VGG16.hdf5\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 4s - loss: 4.9863 - acc: 0.600 - ETA: 2s - loss: 4.9519 - acc: 0.675 - ETA: 2s - loss: 5.5316 - acc: 0.639 - ETA: 2s - loss: 5.0089 - acc: 0.668 - ETA: 2s - loss: 5.3824 - acc: 0.646 - ETA: 2s - loss: 5.4824 - acc: 0.641 - ETA: 1s - loss: 5.4786 - acc: 0.643 - ETA: 1s - loss: 5.5887 - acc: 0.638 - ETA: 1s - loss: 5.5191 - acc: 0.644 - ETA: 1s - loss: 5.5599 - acc: 0.641 - ETA: 1s - loss: 5.5805 - acc: 0.641 - ETA: 1s - loss: 5.5682 - acc: 0.642 - ETA: 1s - loss: 5.5302 - acc: 0.645 - ETA: 1s - loss: 5.5084 - acc: 0.647 - ETA: 1s - loss: 5.5734 - acc: 0.641 - ETA: 1s - loss: 5.5925 - acc: 0.640 - ETA: 1s - loss: 5.6156 - acc: 0.638 - ETA: 1s - loss: 5.6235 - acc: 0.638 - ETA: 1s - loss: 5.6827 - acc: 0.634 - ETA: 1s - loss: 5.6370 - acc: 0.636 - ETA: 1s - loss: 5.6411 - acc: 0.635 - ETA: 1s - loss: 5.6433 - acc: 0.635 - ETA: 1s - loss: 5.6567 - acc: 0.634 - ETA: 1s - loss: 5.6873 - acc: 0.632 - ETA: 1s - loss: 5.7001 - acc: 0.632 - ETA: 0s - loss: 5.7146 - acc: 0.631 - ETA: 0s - loss: 5.6850 - acc: 0.633 - ETA: 0s - loss: 5.6815 - acc: 0.633 - ETA: 0s - loss: 5.6950 - acc: 0.632 - ETA: 0s - loss: 5.6841 - acc: 0.632 - ETA: 0s - loss: 5.6547 - acc: 0.634 - ETA: 0s - loss: 5.6550 - acc: 0.634 - ETA: 0s - loss: 5.6506 - acc: 0.633 - ETA: 0s - loss: 5.6438 - acc: 0.634 - ETA: 0s - loss: 5.6402 - acc: 0.634 - ETA: 0s - loss: 5.6545 - acc: 0.633 - ETA: 0s - loss: 5.6735 - acc: 0.632 - ETA: 0s - loss: 5.6977 - acc: 0.630 - ETA: 0s - loss: 5.6964 - acc: 0.630 - ETA: 0s - loss: 5.6875 - acc: 0.631 - ETA: 0s - loss: 5.6834 - acc: 0.631 - ETA: 0s - loss: 5.6601 - acc: 0.632 - ETA: 0s - loss: 5.6662 - acc: 0.632 - 2s 362us/step - loss: 5.6637 - acc: 0.6326 - val_loss: 6.5124 - val_acc: 0.4994\n",
      "\n",
      "Epoch 00015: val_loss improved from 6.62355 to 6.51238, saving model to model/notebook/weights.best.VGG16.hdf5\n",
      "Epoch 16/20\n",
      "6680/6680 [==============================] - ETA: 3s - loss: 8.5136 - acc: 0.450 - ETA: 2s - loss: 6.5066 - acc: 0.571 - ETA: 2s - loss: 6.6831 - acc: 0.567 - ETA: 2s - loss: 6.1911 - acc: 0.597 - ETA: 2s - loss: 6.0594 - acc: 0.606 - ETA: 2s - loss: 5.8570 - acc: 0.619 - ETA: 2s - loss: 5.8614 - acc: 0.619 - ETA: 1s - loss: 5.9241 - acc: 0.617 - ETA: 1s - loss: 5.7658 - acc: 0.628 - ETA: 1s - loss: 5.6977 - acc: 0.633 - ETA: 1s - loss: 5.6620 - acc: 0.636 - ETA: 1s - loss: 5.6270 - acc: 0.638 - ETA: 1s - loss: 5.6785 - acc: 0.634 - ETA: 1s - loss: 5.7020 - acc: 0.634 - ETA: 1s - loss: 5.6778 - acc: 0.635 - ETA: 1s - loss: 5.6566 - acc: 0.637 - ETA: 1s - loss: 5.7207 - acc: 0.634 - ETA: 1s - loss: 5.6748 - acc: 0.636 - ETA: 1s - loss: 5.6576 - acc: 0.637 - ETA: 1s - loss: 5.6867 - acc: 0.634 - ETA: 1s - loss: 5.6671 - acc: 0.635 - ETA: 1s - loss: 5.6426 - acc: 0.637 - ETA: 1s - loss: 5.6480 - acc: 0.637 - ETA: 1s - loss: 5.6165 - acc: 0.639 - ETA: 1s - loss: 5.6453 - acc: 0.637 - ETA: 0s - loss: 5.6433 - acc: 0.637 - ETA: 0s - loss: 5.5944 - acc: 0.640 - ETA: 0s - loss: 5.5589 - acc: 0.642 - ETA: 0s - loss: 5.5605 - acc: 0.642 - ETA: 0s - loss: 5.5755 - acc: 0.641 - ETA: 0s - loss: 5.5744 - acc: 0.641 - ETA: 0s - loss: 5.5842 - acc: 0.641 - ETA: 0s - loss: 5.5830 - acc: 0.641 - ETA: 0s - loss: 5.5637 - acc: 0.643 - ETA: 0s - loss: 5.5561 - acc: 0.643 - ETA: 0s - loss: 5.5347 - acc: 0.645 - ETA: 0s - loss: 5.5360 - acc: 0.645 - ETA: 0s - loss: 5.5488 - acc: 0.643 - ETA: 0s - loss: 5.5458 - acc: 0.643 - ETA: 0s - loss: 5.5649 - acc: 0.642 - ETA: 0s - loss: 5.5903 - acc: 0.640 - ETA: 0s - loss: 5.5918 - acc: 0.640 - 2s 347us/step - loss: 5.5876 - acc: 0.6406 - val_loss: 6.4817 - val_acc: 0.5090\n",
      "\n",
      "Epoch 00016: val_loss improved from 6.51238 to 6.48169, saving model to model/notebook/weights.best.VGG16.hdf5\n",
      "Epoch 17/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 6.5731 - acc: 0.550 - ETA: 2s - loss: 5.5608 - acc: 0.644 - ETA: 1s - loss: 5.9280 - acc: 0.625 - ETA: 2s - loss: 5.9449 - acc: 0.626 - ETA: 1s - loss: 5.9858 - acc: 0.623 - ETA: 1s - loss: 5.8591 - acc: 0.632 - ETA: 1s - loss: 5.7583 - acc: 0.638 - ETA: 1s - loss: 5.5957 - acc: 0.648 - ETA: 1s - loss: 5.4784 - acc: 0.655 - ETA: 1s - loss: 5.4893 - acc: 0.650 - ETA: 1s - loss: 5.4072 - acc: 0.655 - ETA: 1s - loss: 5.3774 - acc: 0.657 - ETA: 1s - loss: 5.3998 - acc: 0.655 - ETA: 1s - loss: 5.3064 - acc: 0.659 - ETA: 1s - loss: 5.3362 - acc: 0.656 - ETA: 1s - loss: 5.3174 - acc: 0.657 - ETA: 1s - loss: 5.3261 - acc: 0.656 - ETA: 1s - loss: 5.3285 - acc: 0.656 - ETA: 1s - loss: 5.3420 - acc: 0.656 - ETA: 1s - loss: 5.3587 - acc: 0.655 - ETA: 1s - loss: 5.3552 - acc: 0.655 - ETA: 1s - loss: 5.3359 - acc: 0.656 - ETA: 0s - loss: 5.3519 - acc: 0.655 - ETA: 0s - loss: 5.3837 - acc: 0.653 - ETA: 0s - loss: 5.3949 - acc: 0.651 - ETA: 0s - loss: 5.4179 - acc: 0.649 - ETA: 0s - loss: 5.4599 - acc: 0.647 - ETA: 0s - loss: 5.4630 - acc: 0.647 - ETA: 0s - loss: 5.4782 - acc: 0.645 - ETA: 0s - loss: 5.4359 - acc: 0.647 - ETA: 0s - loss: 5.4446 - acc: 0.647 - ETA: 0s - loss: 5.4701 - acc: 0.645 - ETA: 0s - loss: 5.4725 - acc: 0.645 - ETA: 0s - loss: 5.4695 - acc: 0.645 - ETA: 0s - loss: 5.4446 - acc: 0.646 - ETA: 0s - loss: 5.4406 - acc: 0.647 - ETA: 0s - loss: 5.4718 - acc: 0.645 - ETA: 0s - loss: 5.4802 - acc: 0.644 - ETA: 0s - loss: 5.4918 - acc: 0.644 - ETA: 0s - loss: 5.4553 - acc: 0.646 - ETA: 0s - loss: 5.4508 - acc: 0.647 - ETA: 0s - loss: 5.4725 - acc: 0.645 - ETA: 0s - loss: 5.4783 - acc: 0.645 - ETA: 0s - loss: 5.4945 - acc: 0.644 - ETA: 0s - loss: 5.5043 - acc: 0.643 - 3s 387us/step - loss: 5.4964 - acc: 0.6442 - val_loss: 6.3304 - val_acc: 0.5246\n",
      "\n",
      "Epoch 00017: val_loss improved from 6.48169 to 6.33039, saving model to model/notebook/weights.best.VGG16.hdf5\n",
      "Epoch 18/20\n",
      "6680/6680 [==============================] - ETA: 3s - loss: 8.0606 - acc: 0.500 - ETA: 2s - loss: 6.4484 - acc: 0.600 - ETA: 2s - loss: 5.6134 - acc: 0.646 - ETA: 2s - loss: 5.4298 - acc: 0.654 - ETA: 2s - loss: 5.4145 - acc: 0.653 - ETA: 2s - loss: 5.4145 - acc: 0.655 - ETA: 2s - loss: 5.4419 - acc: 0.653 - ETA: 2s - loss: 5.4087 - acc: 0.655 - ETA: 2s - loss: 5.5106 - acc: 0.647 - ETA: 1s - loss: 5.5654 - acc: 0.644 - ETA: 1s - loss: 5.5129 - acc: 0.647 - ETA: 1s - loss: 5.4240 - acc: 0.652 - ETA: 1s - loss: 5.4160 - acc: 0.652 - ETA: 1s - loss: 5.3614 - acc: 0.656 - ETA: 1s - loss: 5.4101 - acc: 0.653 - ETA: 1s - loss: 5.4284 - acc: 0.653 - ETA: 1s - loss: 5.4117 - acc: 0.654 - ETA: 1s - loss: 5.4054 - acc: 0.654 - ETA: 1s - loss: 5.4027 - acc: 0.654 - ETA: 1s - loss: 5.3798 - acc: 0.655 - ETA: 1s - loss: 5.3899 - acc: 0.655 - ETA: 1s - loss: 5.3893 - acc: 0.655 - ETA: 1s - loss: 5.4023 - acc: 0.654 - ETA: 1s - loss: 5.3952 - acc: 0.654 - ETA: 1s - loss: 5.3911 - acc: 0.654 - ETA: 0s - loss: 5.3950 - acc: 0.654 - ETA: 0s - loss: 5.3754 - acc: 0.655 - ETA: 0s - loss: 5.3720 - acc: 0.656 - ETA: 0s - loss: 5.3770 - acc: 0.655 - ETA: 0s - loss: 5.3505 - acc: 0.657 - ETA: 0s - loss: 5.3360 - acc: 0.658 - ETA: 0s - loss: 5.3651 - acc: 0.656 - ETA: 0s - loss: 5.3756 - acc: 0.655 - ETA: 0s - loss: 5.3931 - acc: 0.654 - ETA: 0s - loss: 5.3597 - acc: 0.656 - ETA: 0s - loss: 5.3710 - acc: 0.655 - ETA: 0s - loss: 5.4010 - acc: 0.654 - ETA: 0s - loss: 5.4050 - acc: 0.654 - ETA: 0s - loss: 5.4156 - acc: 0.653 - ETA: 0s - loss: 5.4085 - acc: 0.654 - ETA: 0s - loss: 5.4198 - acc: 0.653 - ETA: 0s - loss: 5.3886 - acc: 0.655 - ETA: 0s - loss: 5.3724 - acc: 0.656 - ETA: 0s - loss: 5.3886 - acc: 0.654 - ETA: 0s - loss: 5.3947 - acc: 0.654 - ETA: 0s - loss: 5.4017 - acc: 0.653 - ETA: 0s - loss: 5.3935 - acc: 0.654 - ETA: 0s - loss: 5.3929 - acc: 0.654 - ETA: 0s - loss: 5.3927 - acc: 0.654 - ETA: 0s - loss: 5.3991 - acc: 0.654 - ETA: 0s - loss: 5.4029 - acc: 0.654 - 3s 438us/step - loss: 5.4089 - acc: 0.6537 - val_loss: 6.3538 - val_acc: 0.5162\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 6.33039\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 6s - loss: 8.0592 - acc: 0.500 - ETA: 5s - loss: 5.0390 - acc: 0.687 - ETA: 4s - loss: 5.3746 - acc: 0.666 - ETA: 3s - loss: 5.4445 - acc: 0.662 - ETA: 3s - loss: 5.0291 - acc: 0.686 - ETA: 3s - loss: 5.1780 - acc: 0.672 - ETA: 3s - loss: 5.2143 - acc: 0.671 - ETA: 2s - loss: 5.1495 - acc: 0.675 - ETA: 2s - loss: 5.0009 - acc: 0.685 - ETA: 2s - loss: 5.0694 - acc: 0.681 - ETA: 2s - loss: 5.1101 - acc: 0.678 - ETA: 2s - loss: 5.1545 - acc: 0.675 - ETA: 2s - loss: 5.1152 - acc: 0.677 - ETA: 2s - loss: 5.3076 - acc: 0.665 - ETA: 2s - loss: 5.2284 - acc: 0.670 - ETA: 2s - loss: 5.3359 - acc: 0.663 - ETA: 2s - loss: 5.2954 - acc: 0.666 - ETA: 1s - loss: 5.2945 - acc: 0.666 - ETA: 1s - loss: 5.3145 - acc: 0.665 - ETA: 1s - loss: 5.3517 - acc: 0.662 - ETA: 1s - loss: 5.3595 - acc: 0.662 - ETA: 1s - loss: 5.3405 - acc: 0.662 - ETA: 1s - loss: 5.3469 - acc: 0.661 - ETA: 1s - loss: 5.4085 - acc: 0.658 - ETA: 1s - loss: 5.4172 - acc: 0.657 - ETA: 1s - loss: 5.4215 - acc: 0.657 - ETA: 1s - loss: 5.4544 - acc: 0.655 - ETA: 1s - loss: 5.4506 - acc: 0.655 - ETA: 1s - loss: 5.4526 - acc: 0.655 - ETA: 1s - loss: 5.4657 - acc: 0.654 - ETA: 1s - loss: 5.4370 - acc: 0.655 - ETA: 1s - loss: 5.4343 - acc: 0.656 - ETA: 1s - loss: 5.4276 - acc: 0.656 - ETA: 0s - loss: 5.3691 - acc: 0.659 - ETA: 0s - loss: 5.3572 - acc: 0.660 - ETA: 0s - loss: 5.3649 - acc: 0.660 - ETA: 0s - loss: 5.3209 - acc: 0.662 - ETA: 0s - loss: 5.3408 - acc: 0.661 - ETA: 0s - loss: 5.3306 - acc: 0.661 - ETA: 0s - loss: 5.3363 - acc: 0.661 - ETA: 0s - loss: 5.3566 - acc: 0.660 - ETA: 0s - loss: 5.3642 - acc: 0.659 - ETA: 0s - loss: 5.3690 - acc: 0.659 - ETA: 0s - loss: 5.3418 - acc: 0.660 - ETA: 0s - loss: 5.3559 - acc: 0.659 - ETA: 0s - loss: 5.3442 - acc: 0.660 - ETA: 0s - loss: 5.3678 - acc: 0.659 - ETA: 0s - loss: 5.3833 - acc: 0.658 - 3s 397us/step - loss: 5.3840 - acc: 0.6585 - val_loss: 6.3614 - val_acc: 0.5329\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 6.33039\n",
      "Epoch 20/20\n",
      "6680/6680 [==============================] - ETA: 4s - loss: 10.6091 - acc: 0.30 - ETA: 2s - loss: 5.8984 - acc: 0.6300 - ETA: 2s - loss: 5.3914 - acc: 0.655 - ETA: 2s - loss: 5.4996 - acc: 0.652 - ETA: 2s - loss: 5.5129 - acc: 0.651 - ETA: 2s - loss: 5.6401 - acc: 0.644 - ETA: 1s - loss: 5.6112 - acc: 0.646 - ETA: 1s - loss: 5.5236 - acc: 0.651 - ETA: 1s - loss: 5.5035 - acc: 0.653 - ETA: 1s - loss: 5.4421 - acc: 0.657 - ETA: 1s - loss: 5.4051 - acc: 0.659 - ETA: 1s - loss: 5.5389 - acc: 0.651 - ETA: 1s - loss: 5.4334 - acc: 0.657 - ETA: 1s - loss: 5.4489 - acc: 0.655 - ETA: 1s - loss: 5.3842 - acc: 0.659 - ETA: 1s - loss: 5.3597 - acc: 0.661 - ETA: 1s - loss: 5.4052 - acc: 0.659 - ETA: 1s - loss: 5.4096 - acc: 0.659 - ETA: 1s - loss: 5.4407 - acc: 0.657 - ETA: 1s - loss: 5.4197 - acc: 0.659 - ETA: 1s - loss: 5.4716 - acc: 0.655 - ETA: 1s - loss: 5.4692 - acc: 0.656 - ETA: 1s - loss: 5.4654 - acc: 0.655 - ETA: 0s - loss: 5.4478 - acc: 0.656 - ETA: 0s - loss: 5.3974 - acc: 0.660 - ETA: 0s - loss: 5.3643 - acc: 0.662 - ETA: 0s - loss: 5.3089 - acc: 0.665 - ETA: 0s - loss: 5.3067 - acc: 0.665 - ETA: 0s - loss: 5.3475 - acc: 0.663 - ETA: 0s - loss: 5.3547 - acc: 0.662 - ETA: 0s - loss: 5.3645 - acc: 0.662 - ETA: 0s - loss: 5.3531 - acc: 0.663 - ETA: 0s - loss: 5.3606 - acc: 0.662 - ETA: 0s - loss: 5.3505 - acc: 0.662 - ETA: 0s - loss: 5.3825 - acc: 0.660 - ETA: 0s - loss: 5.3905 - acc: 0.659 - ETA: 0s - loss: 5.4130 - acc: 0.658 - ETA: 0s - loss: 5.4183 - acc: 0.658 - ETA: 0s - loss: 5.4179 - acc: 0.658 - ETA: 0s - loss: 5.3990 - acc: 0.659 - ETA: 0s - loss: 5.3859 - acc: 0.660 - ETA: 0s - loss: 5.3675 - acc: 0.661 - 2s 343us/step - loss: 5.3708 - acc: 0.6608 - val_loss: 6.3694 - val_acc: 0.5257\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 6.33039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1deb255b0f0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The below fits the CNN and saves the weights for the epoch which yields the ebst results\n",
    "checkpointer = ModelCheckpoint(filepath='model/notebook/weights.best.VGG16.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "VGG16_model.fit(train_VGG16, train_targets, \n",
    "          validation_data=(valid_VGG16, valid_targets),\n",
    "          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads the weights of the best performing epoch\n",
    "VGG16_model.load_weights('model/notebook/weights.best.VGG16.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 51.4354%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Dog Breed with the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.extract_bottleneck_features import *\n",
    "\n",
    "def VGG16_predict_breed(img_path):\n",
    "    \"\"\"Takes a file path of an image, extracts features, classifies it then\n",
    "    states what breed the image mostly resembles  according to the VGG19 model.\n",
    "\n",
    "    Parameters:\n",
    "    img_path (String): Path to a file containing the image.\n",
    "\n",
    "    Returns:\n",
    "    (Boolean): The breed which the image submitted most resembles\n",
    "    \"\"\"\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = extract_VGG16(path_to_tensor(img_path))\n",
    "    # obtain predicted vector\n",
    "    predicted_vector = VGG16_model.predict(bottleneck_feature)\n",
    "    # return dog breed that is predicted by the model\n",
    "    return dog_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step5'></a>\n",
    "## Step 5: Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "\n",
    "You will now use transfer learning to create a CNN that can identify dog breed from images.  Your CNN must attain at least 60% accuracy on the test set.\n",
    "\n",
    "In Step 4, we used transfer learning to create a CNN using VGG-16 bottleneck features.  In this section, you must use the bottleneck features from a different pre-trained model.  To make things easier for you, we have pre-computed the features for all of the networks that are currently available in Keras:\n",
    "- [VGG-19](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogVGG19Data.npz) bottleneck features\n",
    "- [ResNet-50](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogResnet50Data.npz) bottleneck features\n",
    "- [Inception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogInceptionV3Data.npz) bottleneck features\n",
    "- [Xception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogXceptionData.npz) bottleneck features\n",
    "\n",
    "The files are encoded as such:\n",
    "\n",
    "    Dog{network}Data.npz\n",
    "    \n",
    "where `{network}`, in the above filename, can be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`.  Pick one of the above architectures, download the corresponding bottleneck features, and store the downloaded file in the `bottleneck_features/` folder in the repository.\n",
    "\n",
    "### (IMPLEMENTATION) Obtain Bottleneck Features\n",
    "\n",
    "In the code block below, extract the bottleneck features corresponding to the train, test, and validation sets by running the following:\n",
    "\n",
    "    bottleneck_features = np.load('bottleneck_features/Dog{network}Data.npz')\n",
    "    train_{network} = bottleneck_features['train']\n",
    "    valid_{network} = bottleneck_features['valid']\n",
    "    test_{network} = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS SHOULD HAVE BEEN DONE IN THE Preprocessing.py\n",
    "\n",
    "### TODO: Obtain bottleneck features from another pre-trained CNN.\n",
    "\n",
    "#import requests\n",
    "#url = r'https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogVGG19Data.npz'\n",
    "#print('processing')\n",
    "#myfile = requests.get(url)\n",
    "#print('received')\n",
    "\n",
    "#open('bottleneck_features/DogVGG19Data.npz', 'wb').write(myfile.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads in pre-processed VGG16 features for test, train and validation images.\n",
    "bottleneck_features = np.load('bottleneck_features/DogVGG19Data.npz')\n",
    "train_VGG19 = bottleneck_features['train']\n",
    "valid_VGG19 = bottleneck_features['valid']\n",
    "test_VGG19 = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        <your model's name>.summary()\n",
    "   \n",
    "__Question 5:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  Describe why you think the architecture is suitable for the current problem.\n",
    "\n",
    "__Answer:__ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_2 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 133)               34181     \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 133)               0         \n",
      "=================================================================\n",
      "Total params: 165,509\n",
      "Trainable params: 165,509\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### TODO: Define your architecture.\n",
    "from keras.layers import Activation, GlobalAveragePooling2D, Flatten, Activation, Dense, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "\n",
    "#Instatiates model and defines its architecture\n",
    "VGG19_model = Sequential()\n",
    "\n",
    "VGG19_model.add(GlobalAveragePooling2D(input_shape=train_VGG19.shape[1:]))\n",
    "\n",
    "VGG19_model.add(Dense(256))\n",
    "VGG19_model.add(Activation('relu'))\n",
    "\n",
    "VGG19_model.add(Dense(133))\n",
    "VGG19_model.add(Activation('softmax'))\n",
    "\n",
    "#Prints model architecture\n",
    "VGG19_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Compile the model.\n",
    "#Defines the metric to measure against, the optimiser and the loss function used for training the model\n",
    "VGG19_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.  \n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6680/6680 [==============================] - ETA: 5:59 - loss: 13.5141 - acc: 0.0000e+ - ETA: 1:14 - loss: 12.1577 - acc: 0.0300   - ETA: 38s - loss: 12.0415 - acc: 0.0350 - ETA: 23s - loss: 11.4062 - acc: 0.029 - ETA: 16s - loss: 10.7721 - acc: 0.041 - ETA: 15s - loss: 10.4875 - acc: 0.050 - ETA: 12s - loss: 9.8046 - acc: 0.059 - ETA: 11s - loss: 9.2482 - acc: 0.07 - ETA: 9s - loss: 8.6099 - acc: 0.0793 - ETA: 9s - loss: 8.3673 - acc: 0.086 - ETA: 8s - loss: 7.9581 - acc: 0.094 - ETA: 7s - loss: 7.6359 - acc: 0.096 - ETA: 6s - loss: 7.3302 - acc: 0.101 - ETA: 6s - loss: 7.0492 - acc: 0.111 - ETA: 5s - loss: 6.8106 - acc: 0.119 - ETA: 5s - loss: 6.6745 - acc: 0.125 - ETA: 5s - loss: 6.5919 - acc: 0.130 - ETA: 5s - loss: 6.4963 - acc: 0.131 - ETA: 5s - loss: 6.4300 - acc: 0.131 - ETA: 5s - loss: 6.3603 - acc: 0.133 - ETA: 5s - loss: 6.3263 - acc: 0.137 - ETA: 5s - loss: 6.2162 - acc: 0.142 - ETA: 5s - loss: 6.1262 - acc: 0.144 - ETA: 5s - loss: 5.9873 - acc: 0.151 - ETA: 5s - loss: 5.8463 - acc: 0.159 - ETA: 4s - loss: 5.7280 - acc: 0.167 - ETA: 4s - loss: 5.6257 - acc: 0.174 - ETA: 4s - loss: 5.5330 - acc: 0.182 - ETA: 4s - loss: 5.4587 - acc: 0.188 - ETA: 4s - loss: 5.3963 - acc: 0.193 - ETA: 4s - loss: 5.3423 - acc: 0.196 - ETA: 4s - loss: 5.2981 - acc: 0.198 - ETA: 4s - loss: 5.2488 - acc: 0.201 - ETA: 4s - loss: 5.1754 - acc: 0.206 - ETA: 4s - loss: 5.1240 - acc: 0.209 - ETA: 4s - loss: 5.0919 - acc: 0.209 - ETA: 3s - loss: 5.0339 - acc: 0.213 - ETA: 3s - loss: 4.9914 - acc: 0.218 - ETA: 3s - loss: 4.9390 - acc: 0.222 - ETA: 3s - loss: 4.8860 - acc: 0.226 - ETA: 3s - loss: 4.8029 - acc: 0.233 - ETA: 3s - loss: 4.7410 - acc: 0.238 - ETA: 3s - loss: 4.6784 - acc: 0.243 - ETA: 3s - loss: 4.6157 - acc: 0.248 - ETA: 3s - loss: 4.5472 - acc: 0.254 - ETA: 3s - loss: 4.5019 - acc: 0.259 - ETA: 2s - loss: 4.4346 - acc: 0.263 - ETA: 2s - loss: 4.3713 - acc: 0.269 - ETA: 2s - loss: 4.3195 - acc: 0.273 - ETA: 2s - loss: 4.2492 - acc: 0.279 - ETA: 2s - loss: 4.1802 - acc: 0.286 - ETA: 2s - loss: 4.1276 - acc: 0.291 - ETA: 2s - loss: 4.0572 - acc: 0.297 - ETA: 1s - loss: 3.9879 - acc: 0.304 - ETA: 1s - loss: 3.9590 - acc: 0.307 - ETA: 1s - loss: 3.9221 - acc: 0.310 - ETA: 1s - loss: 3.8762 - acc: 0.315 - ETA: 1s - loss: 3.8297 - acc: 0.320 - ETA: 1s - loss: 3.7906 - acc: 0.324 - ETA: 1s - loss: 3.7454 - acc: 0.329 - ETA: 1s - loss: 3.7003 - acc: 0.334 - ETA: 1s - loss: 3.6782 - acc: 0.336 - ETA: 1s - loss: 3.6352 - acc: 0.342 - ETA: 1s - loss: 3.5826 - acc: 0.347 - ETA: 0s - loss: 3.5498 - acc: 0.351 - ETA: 0s - loss: 3.5156 - acc: 0.354 - ETA: 0s - loss: 3.4778 - acc: 0.359 - ETA: 0s - loss: 3.4508 - acc: 0.361 - ETA: 0s - loss: 3.4033 - acc: 0.367 - ETA: 0s - loss: 3.3689 - acc: 0.370 - ETA: 0s - loss: 3.3288 - acc: 0.375 - ETA: 0s - loss: 3.3020 - acc: 0.378 - ETA: 0s - loss: 3.2657 - acc: 0.383 - ETA: 0s - loss: 3.2286 - acc: 0.387 - ETA: 0s - loss: 3.2061 - acc: 0.390 - 5s 819us/step - loss: 3.2027 - acc: 0.3907 - val_loss: 1.4831 - val_acc: 0.5964\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.48306, saving model to model/notebook/weights.best.VGG19.hdf5\n",
      "Epoch 2/20\n",
      "6680/6680 [==============================] - ETA: 7s - loss: 0.8760 - acc: 0.900 - ETA: 3s - loss: 0.8660 - acc: 0.743 - ETA: 2s - loss: 0.8612 - acc: 0.746 - ETA: 2s - loss: 0.8113 - acc: 0.752 - ETA: 2s - loss: 0.8334 - acc: 0.757 - ETA: 2s - loss: 0.9216 - acc: 0.750 - ETA: 2s - loss: 0.9041 - acc: 0.745 - ETA: 2s - loss: 0.8917 - acc: 0.745 - ETA: 2s - loss: 0.9013 - acc: 0.738 - ETA: 2s - loss: 0.8875 - acc: 0.737 - ETA: 2s - loss: 0.8693 - acc: 0.741 - ETA: 2s - loss: 0.8626 - acc: 0.745 - ETA: 2s - loss: 0.8832 - acc: 0.741 - ETA: 2s - loss: 0.8750 - acc: 0.741 - ETA: 2s - loss: 0.8752 - acc: 0.739 - ETA: 2s - loss: 0.8796 - acc: 0.742 - ETA: 2s - loss: 0.8678 - acc: 0.745 - ETA: 2s - loss: 0.8694 - acc: 0.744 - ETA: 2s - loss: 0.8771 - acc: 0.744 - ETA: 2s - loss: 0.8767 - acc: 0.745 - ETA: 2s - loss: 0.8654 - acc: 0.746 - ETA: 2s - loss: 0.8607 - acc: 0.747 - ETA: 2s - loss: 0.8491 - acc: 0.750 - ETA: 2s - loss: 0.8440 - acc: 0.751 - ETA: 2s - loss: 0.8462 - acc: 0.751 - ETA: 1s - loss: 0.8443 - acc: 0.751 - ETA: 1s - loss: 0.8385 - acc: 0.753 - ETA: 1s - loss: 0.8414 - acc: 0.753 - ETA: 1s - loss: 0.8389 - acc: 0.755 - ETA: 1s - loss: 0.8395 - acc: 0.754 - ETA: 1s - loss: 0.8332 - acc: 0.756 - ETA: 1s - loss: 0.8363 - acc: 0.754 - ETA: 1s - loss: 0.8420 - acc: 0.753 - ETA: 1s - loss: 0.8356 - acc: 0.755 - ETA: 1s - loss: 0.8387 - acc: 0.754 - ETA: 1s - loss: 0.8393 - acc: 0.754 - ETA: 1s - loss: 0.8438 - acc: 0.752 - ETA: 1s - loss: 0.8449 - acc: 0.751 - ETA: 1s - loss: 0.8425 - acc: 0.751 - ETA: 1s - loss: 0.8404 - acc: 0.752 - ETA: 1s - loss: 0.8392 - acc: 0.753 - ETA: 1s - loss: 0.8322 - acc: 0.754 - ETA: 0s - loss: 0.8277 - acc: 0.754 - ETA: 0s - loss: 0.8257 - acc: 0.754 - ETA: 0s - loss: 0.8293 - acc: 0.754 - ETA: 0s - loss: 0.8262 - acc: 0.754 - ETA: 0s - loss: 0.8275 - acc: 0.753 - ETA: 0s - loss: 0.8296 - acc: 0.752 - ETA: 0s - loss: 0.8303 - acc: 0.752 - ETA: 0s - loss: 0.8331 - acc: 0.752 - ETA: 0s - loss: 0.8318 - acc: 0.752 - ETA: 0s - loss: 0.8329 - acc: 0.752 - ETA: 0s - loss: 0.8334 - acc: 0.752 - ETA: 0s - loss: 0.8305 - acc: 0.753 - ETA: 0s - loss: 0.8347 - acc: 0.752 - ETA: 0s - loss: 0.8329 - acc: 0.753 - ETA: 0s - loss: 0.8338 - acc: 0.752 - ETA: 0s - loss: 0.8295 - acc: 0.753 - ETA: 0s - loss: 0.8236 - acc: 0.755 - ETA: 0s - loss: 0.8256 - acc: 0.754 - ETA: 0s - loss: 0.8264 - acc: 0.752 - ETA: 0s - loss: 0.8238 - acc: 0.753 - 4s 545us/step - loss: 0.8223 - acc: 0.7540 - val_loss: 1.2562 - val_acc: 0.6766\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.48306 to 1.25617, saving model to model/notebook/weights.best.VGG19.hdf5\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 2s - loss: 0.5782 - acc: 0.800 - ETA: 3s - loss: 0.5910 - acc: 0.816 - ETA: 2s - loss: 0.5893 - acc: 0.826 - ETA: 3s - loss: 0.5031 - acc: 0.850 - ETA: 2s - loss: 0.4817 - acc: 0.862 - ETA: 2s - loss: 0.4811 - acc: 0.850 - ETA: 2s - loss: 0.4750 - acc: 0.852 - ETA: 2s - loss: 0.4766 - acc: 0.850 - ETA: 2s - loss: 0.4658 - acc: 0.853 - ETA: 2s - loss: 0.4519 - acc: 0.857 - ETA: 2s - loss: 0.4568 - acc: 0.854 - ETA: 2s - loss: 0.4507 - acc: 0.857 - ETA: 2s - loss: 0.4488 - acc: 0.857 - ETA: 2s - loss: 0.4421 - acc: 0.857 - ETA: 2s - loss: 0.4322 - acc: 0.863 - ETA: 2s - loss: 0.4411 - acc: 0.864 - ETA: 2s - loss: 0.4395 - acc: 0.864 - ETA: 2s - loss: 0.4396 - acc: 0.863 - ETA: 2s - loss: 0.4424 - acc: 0.862 - ETA: 2s - loss: 0.4467 - acc: 0.861 - ETA: 2s - loss: 0.4482 - acc: 0.860 - ETA: 2s - loss: 0.4558 - acc: 0.860 - ETA: 2s - loss: 0.4510 - acc: 0.861 - ETA: 2s - loss: 0.4508 - acc: 0.860 - ETA: 2s - loss: 0.4509 - acc: 0.860 - ETA: 2s - loss: 0.4433 - acc: 0.862 - ETA: 2s - loss: 0.4379 - acc: 0.863 - ETA: 2s - loss: 0.4399 - acc: 0.863 - ETA: 2s - loss: 0.4412 - acc: 0.862 - ETA: 2s - loss: 0.4399 - acc: 0.862 - ETA: 2s - loss: 0.4412 - acc: 0.860 - ETA: 2s - loss: 0.4468 - acc: 0.859 - ETA: 2s - loss: 0.4468 - acc: 0.858 - ETA: 2s - loss: 0.4444 - acc: 0.859 - ETA: 2s - loss: 0.4440 - acc: 0.859 - ETA: 1s - loss: 0.4475 - acc: 0.859 - ETA: 1s - loss: 0.4440 - acc: 0.858 - ETA: 1s - loss: 0.4480 - acc: 0.857 - ETA: 1s - loss: 0.4500 - acc: 0.855 - ETA: 1s - loss: 0.4512 - acc: 0.856 - ETA: 1s - loss: 0.4529 - acc: 0.857 - ETA: 1s - loss: 0.4571 - acc: 0.855 - ETA: 1s - loss: 0.4593 - acc: 0.854 - ETA: 1s - loss: 0.4593 - acc: 0.854 - ETA: 1s - loss: 0.4591 - acc: 0.854 - ETA: 1s - loss: 0.4563 - acc: 0.854 - ETA: 1s - loss: 0.4534 - acc: 0.855 - ETA: 1s - loss: 0.4542 - acc: 0.856 - ETA: 1s - loss: 0.4546 - acc: 0.856 - ETA: 1s - loss: 0.4540 - acc: 0.856 - ETA: 1s - loss: 0.4548 - acc: 0.856 - ETA: 1s - loss: 0.4528 - acc: 0.856 - ETA: 1s - loss: 0.4544 - acc: 0.857 - ETA: 1s - loss: 0.4530 - acc: 0.857 - ETA: 1s - loss: 0.4527 - acc: 0.857 - ETA: 1s - loss: 0.4524 - acc: 0.857 - ETA: 1s - loss: 0.4513 - acc: 0.858 - ETA: 1s - loss: 0.4507 - acc: 0.859 - ETA: 1s - loss: 0.4530 - acc: 0.858 - ETA: 1s - loss: 0.4549 - acc: 0.858 - ETA: 1s - loss: 0.4547 - acc: 0.858 - ETA: 1s - loss: 0.4551 - acc: 0.858 - ETA: 1s - loss: 0.4555 - acc: 0.858 - ETA: 1s - loss: 0.4562 - acc: 0.858 - ETA: 1s - loss: 0.4563 - acc: 0.858 - ETA: 1s - loss: 0.4555 - acc: 0.858 - ETA: 1s - loss: 0.4561 - acc: 0.858 - ETA: 1s - loss: 0.4545 - acc: 0.858 - ETA: 1s - loss: 0.4543 - acc: 0.858 - ETA: 1s - loss: 0.4574 - acc: 0.857 - ETA: 0s - loss: 0.4572 - acc: 0.857 - ETA: 0s - loss: 0.4569 - acc: 0.857 - ETA: 0s - loss: 0.4582 - acc: 0.856 - ETA: 0s - loss: 0.4575 - acc: 0.857 - ETA: 0s - loss: 0.4578 - acc: 0.856 - ETA: 0s - loss: 0.4599 - acc: 0.856 - ETA: 0s - loss: 0.4595 - acc: 0.856 - ETA: 0s - loss: 0.4594 - acc: 0.856 - ETA: 0s - loss: 0.4591 - acc: 0.856 - ETA: 0s - loss: 0.4602 - acc: 0.856 - ETA: 0s - loss: 0.4592 - acc: 0.856 - ETA: 0s - loss: 0.4576 - acc: 0.856 - ETA: 0s - loss: 0.4587 - acc: 0.857 - ETA: 0s - loss: 0.4589 - acc: 0.856 - ETA: 0s - loss: 0.4610 - acc: 0.855 - ETA: 0s - loss: 0.4627 - acc: 0.854 - ETA: 0s - loss: 0.4636 - acc: 0.854 - ETA: 0s - loss: 0.4622 - acc: 0.854 - ETA: 0s - loss: 0.4668 - acc: 0.853 - ETA: 0s - loss: 0.4712 - acc: 0.852 - 6s 857us/step - loss: 0.4711 - acc: 0.8524 - val_loss: 1.1459 - val_acc: 0.7102\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.25617 to 1.14590, saving model to model/notebook/weights.best.VGG19.hdf5\n",
      "Epoch 4/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.0780 - acc: 1.000 - ETA: 5s - loss: 0.2455 - acc: 0.950 - ETA: 4s - loss: 0.2489 - acc: 0.938 - ETA: 3s - loss: 0.2572 - acc: 0.925 - ETA: 3s - loss: 0.2684 - acc: 0.922 - ETA: 3s - loss: 0.2582 - acc: 0.928 - ETA: 3s - loss: 0.2649 - acc: 0.925 - ETA: 2s - loss: 0.2602 - acc: 0.927 - ETA: 2s - loss: 0.2622 - acc: 0.922 - ETA: 2s - loss: 0.2528 - acc: 0.923 - ETA: 2s - loss: 0.2387 - acc: 0.926 - ETA: 2s - loss: 0.2395 - acc: 0.925 - ETA: 2s - loss: 0.2393 - acc: 0.927 - ETA: 2s - loss: 0.2367 - acc: 0.928 - ETA: 2s - loss: 0.2427 - acc: 0.924 - ETA: 2s - loss: 0.2349 - acc: 0.928 - ETA: 2s - loss: 0.2337 - acc: 0.927 - ETA: 2s - loss: 0.2409 - acc: 0.923 - ETA: 2s - loss: 0.2477 - acc: 0.918 - ETA: 2s - loss: 0.2502 - acc: 0.918 - ETA: 2s - loss: 0.2478 - acc: 0.919 - ETA: 2s - loss: 0.2533 - acc: 0.918 - ETA: 2s - loss: 0.2567 - acc: 0.918 - ETA: 2s - loss: 0.2562 - acc: 0.917 - ETA: 1s - loss: 0.2610 - acc: 0.916 - ETA: 1s - loss: 0.2604 - acc: 0.916 - ETA: 1s - loss: 0.2599 - acc: 0.916 - ETA: 1s - loss: 0.2615 - acc: 0.916 - ETA: 1s - loss: 0.2623 - acc: 0.916 - ETA: 1s - loss: 0.2659 - acc: 0.914 - ETA: 1s - loss: 0.2636 - acc: 0.915 - ETA: 1s - loss: 0.2653 - acc: 0.915 - ETA: 1s - loss: 0.2659 - acc: 0.914 - ETA: 1s - loss: 0.2667 - acc: 0.914 - ETA: 1s - loss: 0.2659 - acc: 0.914 - ETA: 1s - loss: 0.2637 - acc: 0.915 - ETA: 1s - loss: 0.2654 - acc: 0.914 - ETA: 1s - loss: 0.2651 - acc: 0.914 - ETA: 1s - loss: 0.2685 - acc: 0.913 - ETA: 1s - loss: 0.2684 - acc: 0.913 - ETA: 1s - loss: 0.2686 - acc: 0.913 - ETA: 1s - loss: 0.2688 - acc: 0.912 - ETA: 0s - loss: 0.2689 - acc: 0.912 - ETA: 0s - loss: 0.2724 - acc: 0.911 - ETA: 0s - loss: 0.2727 - acc: 0.911 - ETA: 0s - loss: 0.2721 - acc: 0.911 - ETA: 0s - loss: 0.2743 - acc: 0.910 - ETA: 0s - loss: 0.2772 - acc: 0.909 - ETA: 0s - loss: 0.2768 - acc: 0.909 - ETA: 0s - loss: 0.2756 - acc: 0.910 - ETA: 0s - loss: 0.2750 - acc: 0.910 - ETA: 0s - loss: 0.2774 - acc: 0.910 - ETA: 0s - loss: 0.2769 - acc: 0.910 - ETA: 0s - loss: 0.2815 - acc: 0.909 - ETA: 0s - loss: 0.2821 - acc: 0.909 - ETA: 0s - loss: 0.2835 - acc: 0.909 - ETA: 0s - loss: 0.2840 - acc: 0.908 - ETA: 0s - loss: 0.2842 - acc: 0.908 - ETA: 0s - loss: 0.2852 - acc: 0.907 - ETA: 0s - loss: 0.2876 - acc: 0.906 - 3s 515us/step - loss: 0.2876 - acc: 0.9070 - val_loss: 1.2264 - val_acc: 0.7126\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.14590\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 10s - loss: 0.1532 - acc: 0.95 - ETA: 4s - loss: 0.1291 - acc: 0.9500 - ETA: 4s - loss: 0.2097 - acc: 0.933 - ETA: 3s - loss: 0.1931 - acc: 0.938 - ETA: 3s - loss: 0.1846 - acc: 0.941 - ETA: 3s - loss: 0.1701 - acc: 0.945 - ETA: 3s - loss: 0.1617 - acc: 0.948 - ETA: 3s - loss: 0.1885 - acc: 0.948 - ETA: 3s - loss: 0.1802 - acc: 0.950 - ETA: 3s - loss: 0.1831 - acc: 0.947 - ETA: 3s - loss: 0.1783 - acc: 0.950 - ETA: 3s - loss: 0.1829 - acc: 0.948 - ETA: 3s - loss: 0.1760 - acc: 0.950 - ETA: 3s - loss: 0.1787 - acc: 0.950 - ETA: 3s - loss: 0.1750 - acc: 0.950 - ETA: 3s - loss: 0.1719 - acc: 0.951 - ETA: 3s - loss: 0.1746 - acc: 0.950 - ETA: 3s - loss: 0.1683 - acc: 0.952 - ETA: 3s - loss: 0.1719 - acc: 0.950 - ETA: 3s - loss: 0.1708 - acc: 0.950 - ETA: 3s - loss: 0.1783 - acc: 0.949 - ETA: 3s - loss: 0.1790 - acc: 0.949 - ETA: 3s - loss: 0.1814 - acc: 0.949 - ETA: 3s - loss: 0.1795 - acc: 0.949 - ETA: 2s - loss: 0.1771 - acc: 0.950 - ETA: 2s - loss: 0.1791 - acc: 0.949 - ETA: 2s - loss: 0.1777 - acc: 0.949 - ETA: 2s - loss: 0.1785 - acc: 0.948 - ETA: 2s - loss: 0.1792 - acc: 0.948 - ETA: 2s - loss: 0.1812 - acc: 0.947 - ETA: 2s - loss: 0.1854 - acc: 0.946 - ETA: 2s - loss: 0.1899 - acc: 0.945 - ETA: 2s - loss: 0.1937 - acc: 0.944 - ETA: 1s - loss: 0.1963 - acc: 0.942 - ETA: 1s - loss: 0.1961 - acc: 0.942 - ETA: 1s - loss: 0.1966 - acc: 0.942 - ETA: 1s - loss: 0.1946 - acc: 0.943 - ETA: 1s - loss: 0.1943 - acc: 0.943 - ETA: 1s - loss: 0.1978 - acc: 0.942 - ETA: 1s - loss: 0.1995 - acc: 0.942 - ETA: 1s - loss: 0.1973 - acc: 0.942 - ETA: 1s - loss: 0.1993 - acc: 0.942 - ETA: 1s - loss: 0.1985 - acc: 0.942 - ETA: 1s - loss: 0.1967 - acc: 0.942 - ETA: 1s - loss: 0.1991 - acc: 0.941 - ETA: 1s - loss: 0.1983 - acc: 0.941 - ETA: 0s - loss: 0.1956 - acc: 0.942 - ETA: 0s - loss: 0.1949 - acc: 0.942 - ETA: 0s - loss: 0.1946 - acc: 0.942 - ETA: 0s - loss: 0.1951 - acc: 0.942 - ETA: 0s - loss: 0.1940 - acc: 0.942 - ETA: 0s - loss: 0.1939 - acc: 0.942 - ETA: 0s - loss: 0.1919 - acc: 0.943 - ETA: 0s - loss: 0.1917 - acc: 0.943 - ETA: 0s - loss: 0.1916 - acc: 0.942 - ETA: 0s - loss: 0.1914 - acc: 0.942 - ETA: 0s - loss: 0.1924 - acc: 0.942 - ETA: 0s - loss: 0.1928 - acc: 0.942 - ETA: 0s - loss: 0.1936 - acc: 0.942 - ETA: 0s - loss: 0.1936 - acc: 0.942 - ETA: 0s - loss: 0.1921 - acc: 0.942 - ETA: 0s - loss: 0.1957 - acc: 0.942 - 4s 529us/step - loss: 0.1963 - acc: 0.9422 - val_loss: 1.1332 - val_acc: 0.7377\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.14590 to 1.13319, saving model to model/notebook/weights.best.VGG19.hdf5\n",
      "Epoch 6/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 0.1913 - acc: 0.950 - ETA: 4s - loss: 0.1196 - acc: 0.990 - ETA: 4s - loss: 0.0988 - acc: 0.988 - ETA: 3s - loss: 0.1052 - acc: 0.976 - ETA: 3s - loss: 0.1259 - acc: 0.959 - ETA: 3s - loss: 0.1280 - acc: 0.958 - ETA: 2s - loss: 0.1193 - acc: 0.961 - ETA: 2s - loss: 0.1141 - acc: 0.961 - ETA: 2s - loss: 0.1174 - acc: 0.958 - ETA: 2s - loss: 0.1228 - acc: 0.958 - ETA: 2s - loss: 0.1206 - acc: 0.960 - ETA: 2s - loss: 0.1196 - acc: 0.961 - ETA: 2s - loss: 0.1151 - acc: 0.963 - ETA: 2s - loss: 0.1120 - acc: 0.964 - ETA: 2s - loss: 0.1096 - acc: 0.965 - ETA: 2s - loss: 0.1091 - acc: 0.965 - ETA: 2s - loss: 0.1083 - acc: 0.966 - ETA: 2s - loss: 0.1073 - acc: 0.966 - ETA: 2s - loss: 0.1075 - acc: 0.966 - ETA: 2s - loss: 0.1051 - acc: 0.968 - ETA: 2s - loss: 0.1041 - acc: 0.968 - ETA: 2s - loss: 0.1039 - acc: 0.969 - ETA: 2s - loss: 0.1026 - acc: 0.969 - ETA: 1s - loss: 0.1027 - acc: 0.969 - ETA: 1s - loss: 0.1045 - acc: 0.969 - ETA: 1s - loss: 0.1029 - acc: 0.969 - ETA: 1s - loss: 0.1029 - acc: 0.969 - ETA: 1s - loss: 0.1049 - acc: 0.968 - ETA: 1s - loss: 0.1050 - acc: 0.968 - ETA: 1s - loss: 0.1077 - acc: 0.967 - ETA: 1s - loss: 0.1072 - acc: 0.967 - ETA: 1s - loss: 0.1099 - acc: 0.968 - ETA: 1s - loss: 0.1097 - acc: 0.968 - ETA: 1s - loss: 0.1107 - acc: 0.967 - ETA: 1s - loss: 0.1095 - acc: 0.967 - ETA: 1s - loss: 0.1108 - acc: 0.966 - ETA: 1s - loss: 0.1102 - acc: 0.967 - ETA: 1s - loss: 0.1098 - acc: 0.966 - ETA: 1s - loss: 0.1098 - acc: 0.967 - ETA: 0s - loss: 0.1107 - acc: 0.967 - ETA: 0s - loss: 0.1112 - acc: 0.966 - ETA: 0s - loss: 0.1125 - acc: 0.966 - ETA: 0s - loss: 0.1137 - acc: 0.965 - ETA: 0s - loss: 0.1148 - acc: 0.964 - ETA: 0s - loss: 0.1158 - acc: 0.963 - ETA: 0s - loss: 0.1182 - acc: 0.963 - ETA: 0s - loss: 0.1202 - acc: 0.962 - ETA: 0s - loss: 0.1236 - acc: 0.961 - ETA: 0s - loss: 0.1241 - acc: 0.961 - ETA: 0s - loss: 0.1251 - acc: 0.961 - ETA: 0s - loss: 0.1250 - acc: 0.961 - ETA: 0s - loss: 0.1262 - acc: 0.960 - ETA: 0s - loss: 0.1271 - acc: 0.960 - ETA: 0s - loss: 0.1284 - acc: 0.959 - ETA: 0s - loss: 0.1291 - acc: 0.959 - ETA: 0s - loss: 0.1313 - acc: 0.959 - ETA: 0s - loss: 0.1323 - acc: 0.958 - 3s 478us/step - loss: 0.1325 - acc: 0.9587 - val_loss: 1.1802 - val_acc: 0.7485\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.13319\n",
      "Epoch 7/20\n",
      "6680/6680 [==============================] - ETA: 5s - loss: 0.4336 - acc: 0.900 - ETA: 3s - loss: 0.2269 - acc: 0.942 - ETA: 3s - loss: 0.1599 - acc: 0.961 - ETA: 2s - loss: 0.1577 - acc: 0.950 - ETA: 2s - loss: 0.1367 - acc: 0.956 - ETA: 2s - loss: 0.1464 - acc: 0.957 - ETA: 2s - loss: 0.1401 - acc: 0.959 - ETA: 2s - loss: 0.1338 - acc: 0.960 - ETA: 2s - loss: 0.1289 - acc: 0.963 - ETA: 2s - loss: 0.1264 - acc: 0.962 - ETA: 2s - loss: 0.1200 - acc: 0.963 - ETA: 2s - loss: 0.1178 - acc: 0.964 - ETA: 2s - loss: 0.1142 - acc: 0.964 - ETA: 2s - loss: 0.1084 - acc: 0.966 - ETA: 2s - loss: 0.1094 - acc: 0.966 - ETA: 2s - loss: 0.1056 - acc: 0.967 - ETA: 2s - loss: 0.1047 - acc: 0.967 - ETA: 2s - loss: 0.1055 - acc: 0.967 - ETA: 1s - loss: 0.1027 - acc: 0.968 - ETA: 1s - loss: 0.1044 - acc: 0.968 - ETA: 1s - loss: 0.1054 - acc: 0.967 - ETA: 1s - loss: 0.1040 - acc: 0.967 - ETA: 1s - loss: 0.1031 - acc: 0.968 - ETA: 1s - loss: 0.1058 - acc: 0.967 - ETA: 1s - loss: 0.1060 - acc: 0.968 - ETA: 1s - loss: 0.1049 - acc: 0.968 - ETA: 1s - loss: 0.1033 - acc: 0.969 - ETA: 1s - loss: 0.1014 - acc: 0.970 - ETA: 1s - loss: 0.1002 - acc: 0.970 - ETA: 1s - loss: 0.1002 - acc: 0.969 - ETA: 1s - loss: 0.0981 - acc: 0.970 - ETA: 1s - loss: 0.0968 - acc: 0.971 - ETA: 1s - loss: 0.0973 - acc: 0.971 - ETA: 1s - loss: 0.0989 - acc: 0.970 - ETA: 1s - loss: 0.0983 - acc: 0.971 - ETA: 1s - loss: 0.0994 - acc: 0.970 - ETA: 1s - loss: 0.0995 - acc: 0.970 - ETA: 0s - loss: 0.1000 - acc: 0.970 - ETA: 0s - loss: 0.1016 - acc: 0.969 - ETA: 0s - loss: 0.1058 - acc: 0.968 - ETA: 0s - loss: 0.1087 - acc: 0.966 - ETA: 0s - loss: 0.1084 - acc: 0.966 - ETA: 0s - loss: 0.1094 - acc: 0.966 - ETA: 0s - loss: 0.1105 - acc: 0.966 - ETA: 0s - loss: 0.1135 - acc: 0.965 - ETA: 0s - loss: 0.1132 - acc: 0.965 - ETA: 0s - loss: 0.1175 - acc: 0.964 - ETA: 0s - loss: 0.1181 - acc: 0.963 - ETA: 0s - loss: 0.1190 - acc: 0.963 - ETA: 0s - loss: 0.1232 - acc: 0.962 - ETA: 0s - loss: 0.1233 - acc: 0.963 - ETA: 0s - loss: 0.1248 - acc: 0.962 - ETA: 0s - loss: 0.1248 - acc: 0.962 - ETA: 0s - loss: 0.1252 - acc: 0.961 - ETA: 0s - loss: 0.1253 - acc: 0.961 - ETA: 0s - loss: 0.1257 - acc: 0.961 - 3s 467us/step - loss: 0.1260 - acc: 0.9618 - val_loss: 1.3287 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.13319\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 7s - loss: 0.1962 - acc: 0.900 - ETA: 3s - loss: 0.1325 - acc: 0.935 - ETA: 3s - loss: 0.1320 - acc: 0.950 - ETA: 3s - loss: 0.1052 - acc: 0.960 - ETA: 3s - loss: 0.1002 - acc: 0.960 - ETA: 3s - loss: 0.1132 - acc: 0.963 - ETA: 2s - loss: 0.1054 - acc: 0.964 - ETA: 2s - loss: 0.0944 - acc: 0.969 - ETA: 2s - loss: 0.0903 - acc: 0.970 - ETA: 2s - loss: 0.0892 - acc: 0.970 - ETA: 2s - loss: 0.0887 - acc: 0.970 - ETA: 2s - loss: 0.0878 - acc: 0.969 - ETA: 2s - loss: 0.0857 - acc: 0.969 - ETA: 2s - loss: 0.0891 - acc: 0.968 - ETA: 2s - loss: 0.0915 - acc: 0.967 - ETA: 2s - loss: 0.0883 - acc: 0.968 - ETA: 2s - loss: 0.0893 - acc: 0.969 - ETA: 2s - loss: 0.0880 - acc: 0.969 - ETA: 2s - loss: 0.0913 - acc: 0.969 - ETA: 2s - loss: 0.0906 - acc: 0.970 - ETA: 2s - loss: 0.0905 - acc: 0.971 - ETA: 1s - loss: 0.0910 - acc: 0.971 - ETA: 1s - loss: 0.0897 - acc: 0.971 - ETA: 1s - loss: 0.0885 - acc: 0.972 - ETA: 1s - loss: 0.0904 - acc: 0.971 - ETA: 1s - loss: 0.0906 - acc: 0.971 - ETA: 1s - loss: 0.0914 - acc: 0.970 - ETA: 1s - loss: 0.0905 - acc: 0.970 - ETA: 1s - loss: 0.0907 - acc: 0.970 - ETA: 1s - loss: 0.0912 - acc: 0.970 - ETA: 1s - loss: 0.0907 - acc: 0.970 - ETA: 1s - loss: 0.0909 - acc: 0.970 - ETA: 1s - loss: 0.0978 - acc: 0.969 - ETA: 1s - loss: 0.0991 - acc: 0.968 - ETA: 1s - loss: 0.1014 - acc: 0.967 - ETA: 1s - loss: 0.1039 - acc: 0.965 - ETA: 1s - loss: 0.1059 - acc: 0.965 - ETA: 1s - loss: 0.1085 - acc: 0.963 - ETA: 0s - loss: 0.1123 - acc: 0.962 - ETA: 0s - loss: 0.1124 - acc: 0.962 - ETA: 0s - loss: 0.1141 - acc: 0.962 - ETA: 0s - loss: 0.1168 - acc: 0.962 - ETA: 0s - loss: 0.1181 - acc: 0.961 - ETA: 0s - loss: 0.1188 - acc: 0.960 - ETA: 0s - loss: 0.1220 - acc: 0.959 - ETA: 0s - loss: 0.1228 - acc: 0.959 - ETA: 0s - loss: 0.1228 - acc: 0.959 - ETA: 0s - loss: 0.1243 - acc: 0.959 - ETA: 0s - loss: 0.1244 - acc: 0.959 - ETA: 0s - loss: 0.1269 - acc: 0.958 - ETA: 0s - loss: 0.1275 - acc: 0.958 - ETA: 0s - loss: 0.1282 - acc: 0.957 - ETA: 0s - loss: 0.1277 - acc: 0.957 - ETA: 0s - loss: 0.1278 - acc: 0.957 - 3s 457us/step - loss: 0.1313 - acc: 0.9569 - val_loss: 1.2942 - val_acc: 0.7281\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.13319\n",
      "Epoch 9/20\n",
      "6680/6680 [==============================] - ETA: 5s - loss: 0.1535 - acc: 0.900 - ETA: 3s - loss: 0.2251 - acc: 0.925 - ETA: 2s - loss: 0.2217 - acc: 0.925 - ETA: 2s - loss: 0.2412 - acc: 0.935 - ETA: 2s - loss: 0.2279 - acc: 0.942 - ETA: 2s - loss: 0.1993 - acc: 0.950 - ETA: 2s - loss: 0.1950 - acc: 0.948 - ETA: 2s - loss: 0.1798 - acc: 0.952 - ETA: 2s - loss: 0.1715 - acc: 0.954 - ETA: 2s - loss: 0.1611 - acc: 0.955 - ETA: 2s - loss: 0.1503 - acc: 0.958 - ETA: 2s - loss: 0.1532 - acc: 0.959 - ETA: 2s - loss: 0.1630 - acc: 0.959 - ETA: 2s - loss: 0.1585 - acc: 0.959 - ETA: 2s - loss: 0.1541 - acc: 0.961 - ETA: 2s - loss: 0.1502 - acc: 0.962 - ETA: 2s - loss: 0.1494 - acc: 0.962 - ETA: 2s - loss: 0.1452 - acc: 0.963 - ETA: 2s - loss: 0.1430 - acc: 0.963 - ETA: 2s - loss: 0.1430 - acc: 0.962 - ETA: 1s - loss: 0.1426 - acc: 0.962 - ETA: 1s - loss: 0.1442 - acc: 0.961 - ETA: 1s - loss: 0.1439 - acc: 0.961 - ETA: 1s - loss: 0.1436 - acc: 0.961 - ETA: 1s - loss: 0.1418 - acc: 0.962 - ETA: 1s - loss: 0.1431 - acc: 0.960 - ETA: 1s - loss: 0.1413 - acc: 0.961 - ETA: 1s - loss: 0.1412 - acc: 0.961 - ETA: 1s - loss: 0.1397 - acc: 0.961 - ETA: 1s - loss: 0.1407 - acc: 0.960 - ETA: 1s - loss: 0.1380 - acc: 0.961 - ETA: 1s - loss: 0.1371 - acc: 0.961 - ETA: 1s - loss: 0.1410 - acc: 0.960 - ETA: 1s - loss: 0.1414 - acc: 0.959 - ETA: 1s - loss: 0.1425 - acc: 0.958 - ETA: 1s - loss: 0.1410 - acc: 0.958 - ETA: 1s - loss: 0.1414 - acc: 0.957 - ETA: 0s - loss: 0.1409 - acc: 0.957 - ETA: 0s - loss: 0.1407 - acc: 0.957 - ETA: 0s - loss: 0.1402 - acc: 0.958 - ETA: 0s - loss: 0.1440 - acc: 0.956 - ETA: 0s - loss: 0.1449 - acc: 0.955 - ETA: 0s - loss: 0.1452 - acc: 0.955 - ETA: 0s - loss: 0.1449 - acc: 0.955 - ETA: 0s - loss: 0.1455 - acc: 0.955 - ETA: 0s - loss: 0.1445 - acc: 0.955 - ETA: 0s - loss: 0.1442 - acc: 0.955 - ETA: 0s - loss: 0.1464 - acc: 0.955 - ETA: 0s - loss: 0.1458 - acc: 0.955 - ETA: 0s - loss: 0.1467 - acc: 0.955 - ETA: 0s - loss: 0.1473 - acc: 0.955 - ETA: 0s - loss: 0.1467 - acc: 0.955 - ETA: 0s - loss: 0.1469 - acc: 0.954 - ETA: 0s - loss: 0.1458 - acc: 0.955 - ETA: 0s - loss: 0.1446 - acc: 0.955 - ETA: 0s - loss: 0.1444 - acc: 0.955 - 3s 473us/step - loss: 0.1446 - acc: 0.9549 - val_loss: 1.2394 - val_acc: 0.7365\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.13319\n",
      "Epoch 10/20\n",
      "6680/6680 [==============================] - ETA: 3s - loss: 0.0546 - acc: 0.950 - ETA: 2s - loss: 0.0648 - acc: 0.971 - ETA: 2s - loss: 0.0909 - acc: 0.957 - ETA: 2s - loss: 0.0758 - acc: 0.965 - ETA: 2s - loss: 0.1122 - acc: 0.963 - ETA: 2s - loss: 0.0988 - acc: 0.969 - ETA: 2s - loss: 0.1117 - acc: 0.967 - ETA: 2s - loss: 0.1021 - acc: 0.969 - ETA: 2s - loss: 0.0979 - acc: 0.970 - ETA: 2s - loss: 0.0949 - acc: 0.971 - ETA: 2s - loss: 0.0969 - acc: 0.972 - ETA: 2s - loss: 0.1003 - acc: 0.969 - ETA: 2s - loss: 0.0981 - acc: 0.969 - ETA: 2s - loss: 0.0945 - acc: 0.971 - ETA: 2s - loss: 0.0907 - acc: 0.972 - ETA: 2s - loss: 0.0882 - acc: 0.972 - ETA: 2s - loss: 0.0882 - acc: 0.973 - ETA: 2s - loss: 0.0920 - acc: 0.973 - ETA: 1s - loss: 0.0920 - acc: 0.974 - ETA: 1s - loss: 0.0977 - acc: 0.973 - ETA: 1s - loss: 0.0985 - acc: 0.974 - ETA: 1s - loss: 0.1026 - acc: 0.972 - ETA: 1s - loss: 0.1012 - acc: 0.973 - ETA: 1s - loss: 0.1043 - acc: 0.972 - ETA: 1s - loss: 0.1080 - acc: 0.970 - ETA: 1s - loss: 0.1093 - acc: 0.970 - ETA: 1s - loss: 0.1087 - acc: 0.969 - ETA: 1s - loss: 0.1106 - acc: 0.968 - ETA: 1s - loss: 0.1083 - acc: 0.968 - ETA: 1s - loss: 0.1105 - acc: 0.968 - ETA: 1s - loss: 0.1118 - acc: 0.967 - ETA: 1s - loss: 0.1113 - acc: 0.968 - ETA: 1s - loss: 0.1128 - acc: 0.967 - ETA: 1s - loss: 0.1132 - acc: 0.967 - ETA: 1s - loss: 0.1128 - acc: 0.967 - ETA: 1s - loss: 0.1136 - acc: 0.967 - ETA: 0s - loss: 0.1142 - acc: 0.967 - ETA: 0s - loss: 0.1129 - acc: 0.968 - ETA: 0s - loss: 0.1149 - acc: 0.967 - ETA: 0s - loss: 0.1145 - acc: 0.967 - ETA: 0s - loss: 0.1130 - acc: 0.967 - ETA: 0s - loss: 0.1159 - acc: 0.967 - ETA: 0s - loss: 0.1169 - acc: 0.966 - ETA: 0s - loss: 0.1166 - acc: 0.966 - ETA: 0s - loss: 0.1182 - acc: 0.966 - ETA: 0s - loss: 0.1197 - acc: 0.966 - ETA: 0s - loss: 0.1225 - acc: 0.965 - ETA: 0s - loss: 0.1226 - acc: 0.965 - ETA: 0s - loss: 0.1252 - acc: 0.965 - ETA: 0s - loss: 0.1259 - acc: 0.964 - ETA: 0s - loss: 0.1264 - acc: 0.965 - ETA: 0s - loss: 0.1264 - acc: 0.965 - ETA: 0s - loss: 0.1266 - acc: 0.965 - ETA: 0s - loss: 0.1266 - acc: 0.965 - 3s 453us/step - loss: 0.1282 - acc: 0.9647 - val_loss: 1.3974 - val_acc: 0.7150\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.13319\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 2s - loss: 0.0401 - acc: 1.000 - ETA: 2s - loss: 0.1218 - acc: 0.957 - ETA: 2s - loss: 0.0973 - acc: 0.960 - ETA: 2s - loss: 0.1063 - acc: 0.959 - ETA: 2s - loss: 0.1179 - acc: 0.955 - ETA: 2s - loss: 0.1233 - acc: 0.953 - ETA: 2s - loss: 0.1264 - acc: 0.953 - ETA: 2s - loss: 0.1424 - acc: 0.950 - ETA: 2s - loss: 0.1424 - acc: 0.952 - ETA: 2s - loss: 0.1364 - acc: 0.953 - ETA: 2s - loss: 0.1299 - acc: 0.954 - ETA: 2s - loss: 0.1275 - acc: 0.956 - ETA: 2s - loss: 0.1279 - acc: 0.957 - ETA: 2s - loss: 0.1226 - acc: 0.959 - ETA: 2s - loss: 0.1195 - acc: 0.960 - ETA: 2s - loss: 0.1164 - acc: 0.961 - ETA: 1s - loss: 0.1136 - acc: 0.962 - ETA: 1s - loss: 0.1090 - acc: 0.964 - ETA: 1s - loss: 0.1121 - acc: 0.964 - ETA: 1s - loss: 0.1087 - acc: 0.965 - ETA: 1s - loss: 0.1064 - acc: 0.966 - ETA: 1s - loss: 0.1057 - acc: 0.966 - ETA: 1s - loss: 0.1080 - acc: 0.966 - ETA: 1s - loss: 0.1051 - acc: 0.967 - ETA: 1s - loss: 0.1017 - acc: 0.968 - ETA: 1s - loss: 0.0986 - acc: 0.969 - ETA: 1s - loss: 0.0994 - acc: 0.969 - ETA: 1s - loss: 0.0990 - acc: 0.969 - ETA: 1s - loss: 0.0999 - acc: 0.968 - ETA: 1s - loss: 0.0993 - acc: 0.968 - ETA: 1s - loss: 0.0984 - acc: 0.968 - ETA: 1s - loss: 0.1007 - acc: 0.968 - ETA: 1s - loss: 0.1022 - acc: 0.968 - ETA: 1s - loss: 0.1005 - acc: 0.969 - ETA: 1s - loss: 0.0996 - acc: 0.968 - ETA: 1s - loss: 0.0991 - acc: 0.969 - ETA: 1s - loss: 0.1020 - acc: 0.968 - ETA: 1s - loss: 0.1017 - acc: 0.969 - ETA: 1s - loss: 0.1011 - acc: 0.969 - ETA: 0s - loss: 0.1022 - acc: 0.969 - ETA: 0s - loss: 0.1015 - acc: 0.969 - ETA: 0s - loss: 0.1039 - acc: 0.968 - ETA: 0s - loss: 0.1056 - acc: 0.968 - ETA: 0s - loss: 0.1068 - acc: 0.967 - ETA: 0s - loss: 0.1062 - acc: 0.967 - ETA: 0s - loss: 0.1068 - acc: 0.967 - ETA: 0s - loss: 0.1092 - acc: 0.966 - ETA: 0s - loss: 0.1080 - acc: 0.967 - ETA: 0s - loss: 0.1081 - acc: 0.966 - ETA: 0s - loss: 0.1068 - acc: 0.967 - ETA: 0s - loss: 0.1077 - acc: 0.967 - ETA: 0s - loss: 0.1091 - acc: 0.966 - ETA: 0s - loss: 0.1084 - acc: 0.966 - ETA: 0s - loss: 0.1072 - acc: 0.967 - ETA: 0s - loss: 0.1081 - acc: 0.967 - ETA: 0s - loss: 0.1076 - acc: 0.967 - ETA: 0s - loss: 0.1076 - acc: 0.967 - ETA: 0s - loss: 0.1110 - acc: 0.966 - ETA: 0s - loss: 0.1117 - acc: 0.966 - ETA: 0s - loss: 0.1108 - acc: 0.967 - 3s 508us/step - loss: 0.1121 - acc: 0.9666 - val_loss: 1.5351 - val_acc: 0.7281\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.13319\n",
      "Epoch 12/20\n",
      "6680/6680 [==============================] - ETA: 4s - loss: 0.1194 - acc: 0.950 - ETA: 3s - loss: 0.0906 - acc: 0.971 - ETA: 2s - loss: 0.1040 - acc: 0.961 - ETA: 3s - loss: 0.1176 - acc: 0.958 - ETA: 3s - loss: 0.1193 - acc: 0.962 - ETA: 2s - loss: 0.1197 - acc: 0.961 - ETA: 2s - loss: 0.1237 - acc: 0.964 - ETA: 2s - loss: 0.1222 - acc: 0.965 - ETA: 2s - loss: 0.1130 - acc: 0.967 - ETA: 2s - loss: 0.1053 - acc: 0.970 - ETA: 2s - loss: 0.1122 - acc: 0.971 - ETA: 2s - loss: 0.1105 - acc: 0.970 - ETA: 2s - loss: 0.1058 - acc: 0.971 - ETA: 2s - loss: 0.1050 - acc: 0.971 - ETA: 2s - loss: 0.1002 - acc: 0.973 - ETA: 2s - loss: 0.0989 - acc: 0.973 - ETA: 2s - loss: 0.0978 - acc: 0.973 - ETA: 2s - loss: 0.0970 - acc: 0.973 - ETA: 2s - loss: 0.0958 - acc: 0.973 - ETA: 2s - loss: 0.0950 - acc: 0.974 - ETA: 2s - loss: 0.0929 - acc: 0.974 - ETA: 1s - loss: 0.0957 - acc: 0.973 - ETA: 1s - loss: 0.1010 - acc: 0.971 - ETA: 1s - loss: 0.0999 - acc: 0.971 - ETA: 1s - loss: 0.1019 - acc: 0.969 - ETA: 1s - loss: 0.1043 - acc: 0.968 - ETA: 1s - loss: 0.1065 - acc: 0.968 - ETA: 1s - loss: 0.1068 - acc: 0.968 - ETA: 1s - loss: 0.1073 - acc: 0.967 - ETA: 1s - loss: 0.1079 - acc: 0.967 - ETA: 1s - loss: 0.1096 - acc: 0.967 - ETA: 1s - loss: 0.1077 - acc: 0.968 - ETA: 1s - loss: 0.1062 - acc: 0.968 - ETA: 1s - loss: 0.1048 - acc: 0.969 - ETA: 1s - loss: 0.1074 - acc: 0.968 - ETA: 1s - loss: 0.1060 - acc: 0.968 - ETA: 1s - loss: 0.1064 - acc: 0.968 - ETA: 1s - loss: 0.1065 - acc: 0.968 - ETA: 1s - loss: 0.1106 - acc: 0.966 - ETA: 1s - loss: 0.1099 - acc: 0.967 - ETA: 1s - loss: 0.1103 - acc: 0.966 - ETA: 1s - loss: 0.1134 - acc: 0.965 - ETA: 0s - loss: 0.1122 - acc: 0.966 - ETA: 0s - loss: 0.1148 - acc: 0.964 - ETA: 0s - loss: 0.1150 - acc: 0.964 - ETA: 0s - loss: 0.1153 - acc: 0.964 - ETA: 0s - loss: 0.1150 - acc: 0.963 - ETA: 0s - loss: 0.1141 - acc: 0.964 - ETA: 0s - loss: 0.1138 - acc: 0.964 - ETA: 0s - loss: 0.1141 - acc: 0.963 - ETA: 0s - loss: 0.1123 - acc: 0.964 - ETA: 0s - loss: 0.1118 - acc: 0.964 - ETA: 0s - loss: 0.1133 - acc: 0.964 - ETA: 0s - loss: 0.1134 - acc: 0.963 - ETA: 0s - loss: 0.1163 - acc: 0.962 - ETA: 0s - loss: 0.1181 - acc: 0.961 - ETA: 0s - loss: 0.1186 - acc: 0.961 - ETA: 0s - loss: 0.1216 - acc: 0.961 - 3s 499us/step - loss: 0.1232 - acc: 0.9606 - val_loss: 1.4562 - val_acc: 0.7102\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.13319\n",
      "Epoch 13/20\n",
      "6680/6680 [==============================] - ETA: 3s - loss: 0.0168 - acc: 1.000 - ETA: 3s - loss: 0.1047 - acc: 0.971 - ETA: 2s - loss: 0.1474 - acc: 0.969 - ETA: 2s - loss: 0.1263 - acc: 0.967 - ETA: 2s - loss: 0.1632 - acc: 0.959 - ETA: 2s - loss: 0.1586 - acc: 0.956 - ETA: 2s - loss: 0.1475 - acc: 0.957 - ETA: 2s - loss: 0.1466 - acc: 0.958 - ETA: 2s - loss: 0.1345 - acc: 0.961 - ETA: 2s - loss: 0.1438 - acc: 0.960 - ETA: 2s - loss: 0.1421 - acc: 0.959 - ETA: 2s - loss: 0.1429 - acc: 0.958 - ETA: 2s - loss: 0.1387 - acc: 0.958 - ETA: 2s - loss: 0.1353 - acc: 0.959 - ETA: 2s - loss: 0.1304 - acc: 0.960 - ETA: 2s - loss: 0.1319 - acc: 0.958 - ETA: 2s - loss: 0.1302 - acc: 0.959 - ETA: 2s - loss: 0.1258 - acc: 0.960 - ETA: 2s - loss: 0.1221 - acc: 0.962 - ETA: 2s - loss: 0.1222 - acc: 0.962 - ETA: 1s - loss: 0.1213 - acc: 0.963 - ETA: 1s - loss: 0.1177 - acc: 0.964 - ETA: 1s - loss: 0.1144 - acc: 0.965 - ETA: 1s - loss: 0.1128 - acc: 0.965 - ETA: 1s - loss: 0.1162 - acc: 0.963 - ETA: 1s - loss: 0.1167 - acc: 0.963 - ETA: 1s - loss: 0.1144 - acc: 0.964 - ETA: 1s - loss: 0.1140 - acc: 0.964 - ETA: 1s - loss: 0.1123 - acc: 0.964 - ETA: 1s - loss: 0.1109 - acc: 0.965 - ETA: 1s - loss: 0.1095 - acc: 0.965 - ETA: 1s - loss: 0.1095 - acc: 0.966 - ETA: 1s - loss: 0.1073 - acc: 0.966 - ETA: 1s - loss: 0.1108 - acc: 0.965 - ETA: 1s - loss: 0.1096 - acc: 0.965 - ETA: 1s - loss: 0.1082 - acc: 0.965 - ETA: 1s - loss: 0.1078 - acc: 0.965 - ETA: 1s - loss: 0.1088 - acc: 0.965 - ETA: 0s - loss: 0.1085 - acc: 0.965 - ETA: 0s - loss: 0.1113 - acc: 0.965 - ETA: 0s - loss: 0.1134 - acc: 0.965 - ETA: 0s - loss: 0.1119 - acc: 0.965 - ETA: 0s - loss: 0.1119 - acc: 0.965 - ETA: 0s - loss: 0.1107 - acc: 0.966 - ETA: 0s - loss: 0.1108 - acc: 0.966 - ETA: 0s - loss: 0.1120 - acc: 0.965 - ETA: 0s - loss: 0.1117 - acc: 0.965 - ETA: 0s - loss: 0.1125 - acc: 0.964 - ETA: 0s - loss: 0.1112 - acc: 0.965 - ETA: 0s - loss: 0.1119 - acc: 0.964 - ETA: 0s - loss: 0.1137 - acc: 0.964 - ETA: 0s - loss: 0.1146 - acc: 0.964 - ETA: 0s - loss: 0.1152 - acc: 0.964 - ETA: 0s - loss: 0.1196 - acc: 0.964 - ETA: 0s - loss: 0.1195 - acc: 0.964 - ETA: 0s - loss: 0.1196 - acc: 0.964 - 3s 474us/step - loss: 0.1188 - acc: 0.9641 - val_loss: 1.4860 - val_acc: 0.7545\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.13319\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 6s - loss: 0.1655 - acc: 0.950 - ETA: 5s - loss: 0.1613 - acc: 0.960 - ETA: 5s - loss: 0.1411 - acc: 0.966 - ETA: 4s - loss: 0.0946 - acc: 0.976 - ETA: 3s - loss: 0.1394 - acc: 0.959 - ETA: 3s - loss: 0.1384 - acc: 0.953 - ETA: 3s - loss: 0.1380 - acc: 0.953 - ETA: 3s - loss: 0.1273 - acc: 0.956 - ETA: 3s - loss: 0.1173 - acc: 0.960 - ETA: 3s - loss: 0.1081 - acc: 0.963 - ETA: 2s - loss: 0.0986 - acc: 0.966 - ETA: 2s - loss: 0.1019 - acc: 0.966 - ETA: 2s - loss: 0.1006 - acc: 0.968 - ETA: 2s - loss: 0.0991 - acc: 0.967 - ETA: 2s - loss: 0.0948 - acc: 0.969 - ETA: 2s - loss: 0.0980 - acc: 0.969 - ETA: 2s - loss: 0.0987 - acc: 0.968 - ETA: 2s - loss: 0.1014 - acc: 0.968 - ETA: 2s - loss: 0.0993 - acc: 0.968 - ETA: 2s - loss: 0.1021 - acc: 0.967 - ETA: 2s - loss: 0.1055 - acc: 0.965 - ETA: 2s - loss: 0.1087 - acc: 0.965 - ETA: 2s - loss: 0.1075 - acc: 0.964 - ETA: 2s - loss: 0.1041 - acc: 0.965 - ETA: 2s - loss: 0.1040 - acc: 0.965 - ETA: 1s - loss: 0.1077 - acc: 0.964 - ETA: 1s - loss: 0.1071 - acc: 0.964 - ETA: 1s - loss: 0.1082 - acc: 0.964 - ETA: 1s - loss: 0.1052 - acc: 0.966 - ETA: 1s - loss: 0.1056 - acc: 0.965 - ETA: 1s - loss: 0.1048 - acc: 0.965 - ETA: 1s - loss: 0.1086 - acc: 0.965 - ETA: 1s - loss: 0.1080 - acc: 0.965 - ETA: 1s - loss: 0.1109 - acc: 0.963 - ETA: 1s - loss: 0.1105 - acc: 0.963 - ETA: 1s - loss: 0.1149 - acc: 0.963 - ETA: 1s - loss: 0.1181 - acc: 0.962 - ETA: 1s - loss: 0.1195 - acc: 0.962 - ETA: 1s - loss: 0.1189 - acc: 0.962 - ETA: 1s - loss: 0.1228 - acc: 0.960 - ETA: 0s - loss: 0.1285 - acc: 0.959 - ETA: 0s - loss: 0.1273 - acc: 0.959 - ETA: 0s - loss: 0.1295 - acc: 0.959 - ETA: 0s - loss: 0.1283 - acc: 0.960 - ETA: 0s - loss: 0.1325 - acc: 0.959 - ETA: 0s - loss: 0.1309 - acc: 0.959 - ETA: 0s - loss: 0.1352 - acc: 0.958 - ETA: 0s - loss: 0.1336 - acc: 0.959 - ETA: 0s - loss: 0.1330 - acc: 0.959 - ETA: 0s - loss: 0.1337 - acc: 0.958 - ETA: 0s - loss: 0.1361 - acc: 0.958 - ETA: 0s - loss: 0.1364 - acc: 0.957 - ETA: 0s - loss: 0.1357 - acc: 0.957 - ETA: 0s - loss: 0.1349 - acc: 0.958 - ETA: 0s - loss: 0.1348 - acc: 0.958 - ETA: 0s - loss: 0.1351 - acc: 0.958 - 3s 470us/step - loss: 0.1356 - acc: 0.9578 - val_loss: 1.6425 - val_acc: 0.7198\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.13319\n",
      "Epoch 15/20\n",
      "6680/6680 [==============================] - ETA: 3s - loss: 0.1238 - acc: 0.950 - ETA: 2s - loss: 0.0675 - acc: 0.975 - ETA: 2s - loss: 0.0852 - acc: 0.963 - ETA: 2s - loss: 0.0767 - acc: 0.971 - ETA: 2s - loss: 0.0979 - acc: 0.964 - ETA: 2s - loss: 0.0884 - acc: 0.968 - ETA: 2s - loss: 0.0892 - acc: 0.966 - ETA: 2s - loss: 0.0952 - acc: 0.966 - ETA: 2s - loss: 0.0911 - acc: 0.967 - ETA: 2s - loss: 0.0949 - acc: 0.969 - ETA: 2s - loss: 0.0985 - acc: 0.970 - ETA: 2s - loss: 0.0926 - acc: 0.971 - ETA: 2s - loss: 0.0920 - acc: 0.972 - ETA: 2s - loss: 0.0932 - acc: 0.970 - ETA: 2s - loss: 0.0871 - acc: 0.972 - ETA: 2s - loss: 0.0863 - acc: 0.972 - ETA: 1s - loss: 0.0839 - acc: 0.973 - ETA: 1s - loss: 0.0823 - acc: 0.972 - ETA: 1s - loss: 0.0801 - acc: 0.973 - ETA: 1s - loss: 0.0835 - acc: 0.972 - ETA: 1s - loss: 0.0815 - acc: 0.973 - ETA: 1s - loss: 0.0810 - acc: 0.972 - ETA: 1s - loss: 0.0810 - acc: 0.973 - ETA: 1s - loss: 0.0800 - acc: 0.973 - ETA: 1s - loss: 0.0843 - acc: 0.973 - ETA: 1s - loss: 0.0870 - acc: 0.972 - ETA: 1s - loss: 0.0873 - acc: 0.972 - ETA: 1s - loss: 0.0878 - acc: 0.973 - ETA: 1s - loss: 0.0876 - acc: 0.973 - ETA: 1s - loss: 0.0878 - acc: 0.973 - ETA: 1s - loss: 0.0877 - acc: 0.972 - ETA: 1s - loss: 0.0865 - acc: 0.972 - ETA: 1s - loss: 0.0854 - acc: 0.973 - ETA: 1s - loss: 0.0903 - acc: 0.972 - ETA: 1s - loss: 0.0901 - acc: 0.972 - ETA: 0s - loss: 0.0913 - acc: 0.971 - ETA: 0s - loss: 0.0930 - acc: 0.971 - ETA: 0s - loss: 0.0922 - acc: 0.971 - ETA: 0s - loss: 0.0919 - acc: 0.971 - ETA: 0s - loss: 0.0915 - acc: 0.971 - ETA: 0s - loss: 0.0921 - acc: 0.971 - ETA: 0s - loss: 0.0928 - acc: 0.970 - ETA: 0s - loss: 0.0939 - acc: 0.970 - ETA: 0s - loss: 0.0934 - acc: 0.970 - ETA: 0s - loss: 0.0937 - acc: 0.970 - ETA: 0s - loss: 0.0946 - acc: 0.970 - ETA: 0s - loss: 0.0940 - acc: 0.970 - ETA: 0s - loss: 0.0946 - acc: 0.970 - ETA: 0s - loss: 0.0950 - acc: 0.969 - ETA: 0s - loss: 0.0974 - acc: 0.969 - ETA: 0s - loss: 0.1016 - acc: 0.968 - ETA: 0s - loss: 0.1041 - acc: 0.968 - ETA: 0s - loss: 0.1061 - acc: 0.967 - ETA: 0s - loss: 0.1062 - acc: 0.967 - ETA: 0s - loss: 0.1063 - acc: 0.967 - ETA: 0s - loss: 0.1065 - acc: 0.967 - 3s 464us/step - loss: 0.1060 - acc: 0.9675 - val_loss: 1.6287 - val_acc: 0.7401\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.13319\n",
      "Epoch 16/20\n",
      "6680/6680 [==============================] - ETA: 4s - loss: 0.2083 - acc: 0.950 - ETA: 2s - loss: 0.0638 - acc: 0.981 - ETA: 2s - loss: 0.0539 - acc: 0.980 - ETA: 2s - loss: 0.0466 - acc: 0.983 - ETA: 2s - loss: 0.0449 - acc: 0.985 - ETA: 2s - loss: 0.0451 - acc: 0.985 - ETA: 2s - loss: 0.0425 - acc: 0.985 - ETA: 2s - loss: 0.0387 - acc: 0.987 - ETA: 2s - loss: 0.0488 - acc: 0.986 - ETA: 2s - loss: 0.0506 - acc: 0.986 - ETA: 2s - loss: 0.0555 - acc: 0.983 - ETA: 2s - loss: 0.0547 - acc: 0.983 - ETA: 2s - loss: 0.0546 - acc: 0.982 - ETA: 2s - loss: 0.0521 - acc: 0.983 - ETA: 2s - loss: 0.0623 - acc: 0.982 - ETA: 2s - loss: 0.0638 - acc: 0.982 - ETA: 2s - loss: 0.0668 - acc: 0.982 - ETA: 1s - loss: 0.0676 - acc: 0.981 - ETA: 1s - loss: 0.0707 - acc: 0.980 - ETA: 1s - loss: 0.0698 - acc: 0.979 - ETA: 1s - loss: 0.0826 - acc: 0.978 - ETA: 1s - loss: 0.0826 - acc: 0.978 - ETA: 1s - loss: 0.0841 - acc: 0.978 - ETA: 1s - loss: 0.0870 - acc: 0.977 - ETA: 1s - loss: 0.0850 - acc: 0.978 - ETA: 1s - loss: 0.0883 - acc: 0.977 - ETA: 1s - loss: 0.0897 - acc: 0.976 - ETA: 1s - loss: 0.0930 - acc: 0.975 - ETA: 1s - loss: 0.0910 - acc: 0.976 - ETA: 1s - loss: 0.0922 - acc: 0.975 - ETA: 1s - loss: 0.0917 - acc: 0.974 - ETA: 1s - loss: 0.0921 - acc: 0.974 - ETA: 1s - loss: 0.0940 - acc: 0.973 - ETA: 1s - loss: 0.0950 - acc: 0.973 - ETA: 1s - loss: 0.0986 - acc: 0.972 - ETA: 0s - loss: 0.0983 - acc: 0.973 - ETA: 0s - loss: 0.1002 - acc: 0.973 - ETA: 0s - loss: 0.1018 - acc: 0.972 - ETA: 0s - loss: 0.1024 - acc: 0.971 - ETA: 0s - loss: 0.1029 - acc: 0.971 - ETA: 0s - loss: 0.1028 - acc: 0.971 - ETA: 0s - loss: 0.1018 - acc: 0.971 - ETA: 0s - loss: 0.1016 - acc: 0.971 - ETA: 0s - loss: 0.1012 - acc: 0.971 - ETA: 0s - loss: 0.1012 - acc: 0.971 - ETA: 0s - loss: 0.1002 - acc: 0.971 - ETA: 0s - loss: 0.1018 - acc: 0.971 - ETA: 0s - loss: 0.1035 - acc: 0.970 - ETA: 0s - loss: 0.1038 - acc: 0.970 - ETA: 0s - loss: 0.1022 - acc: 0.971 - ETA: 0s - loss: 0.1018 - acc: 0.971 - ETA: 0s - loss: 0.1010 - acc: 0.971 - ETA: 0s - loss: 0.0995 - acc: 0.971 - ETA: 0s - loss: 0.0984 - acc: 0.971 - ETA: 0s - loss: 0.0977 - acc: 0.971 - 3s 462us/step - loss: 0.0980 - acc: 0.9716 - val_loss: 1.4719 - val_acc: 0.7521\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.13319\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 2s - loss: 0.0593 - acc: 0.950 - ETA: 2s - loss: 0.0521 - acc: 0.971 - ETA: 2s - loss: 0.0487 - acc: 0.967 - ETA: 2s - loss: 0.0604 - acc: 0.967 - ETA: 2s - loss: 0.0658 - acc: 0.967 - ETA: 2s - loss: 0.0638 - acc: 0.968 - ETA: 2s - loss: 0.0555 - acc: 0.973 - ETA: 2s - loss: 0.0657 - acc: 0.973 - ETA: 2s - loss: 0.0586 - acc: 0.977 - ETA: 2s - loss: 0.0811 - acc: 0.974 - ETA: 2s - loss: 0.0747 - acc: 0.975 - ETA: 2s - loss: 0.0725 - acc: 0.976 - ETA: 2s - loss: 0.0738 - acc: 0.976 - ETA: 2s - loss: 0.0755 - acc: 0.975 - ETA: 2s - loss: 0.0770 - acc: 0.974 - ETA: 2s - loss: 0.0756 - acc: 0.975 - ETA: 2s - loss: 0.0802 - acc: 0.975 - ETA: 2s - loss: 0.0859 - acc: 0.974 - ETA: 2s - loss: 0.0836 - acc: 0.974 - ETA: 2s - loss: 0.0884 - acc: 0.974 - ETA: 2s - loss: 0.0892 - acc: 0.973 - ETA: 2s - loss: 0.0918 - acc: 0.973 - ETA: 1s - loss: 0.0909 - acc: 0.973 - ETA: 1s - loss: 0.0934 - acc: 0.973 - ETA: 1s - loss: 0.0941 - acc: 0.972 - ETA: 1s - loss: 0.0936 - acc: 0.973 - ETA: 1s - loss: 0.0927 - acc: 0.973 - ETA: 1s - loss: 0.0930 - acc: 0.973 - ETA: 1s - loss: 0.0924 - acc: 0.973 - ETA: 1s - loss: 0.0963 - acc: 0.972 - ETA: 1s - loss: 0.0952 - acc: 0.973 - ETA: 1s - loss: 0.0952 - acc: 0.972 - ETA: 1s - loss: 0.0947 - acc: 0.972 - ETA: 1s - loss: 0.0947 - acc: 0.972 - ETA: 1s - loss: 0.0949 - acc: 0.972 - ETA: 1s - loss: 0.0956 - acc: 0.972 - ETA: 1s - loss: 0.0957 - acc: 0.972 - ETA: 1s - loss: 0.0963 - acc: 0.972 - ETA: 1s - loss: 0.0953 - acc: 0.972 - ETA: 0s - loss: 0.0957 - acc: 0.972 - ETA: 0s - loss: 0.0937 - acc: 0.972 - ETA: 0s - loss: 0.0961 - acc: 0.972 - ETA: 0s - loss: 0.0975 - acc: 0.972 - ETA: 0s - loss: 0.0962 - acc: 0.972 - ETA: 0s - loss: 0.0959 - acc: 0.972 - ETA: 0s - loss: 0.0969 - acc: 0.972 - ETA: 0s - loss: 0.0979 - acc: 0.972 - ETA: 0s - loss: 0.0985 - acc: 0.971 - ETA: 0s - loss: 0.0976 - acc: 0.972 - ETA: 0s - loss: 0.0970 - acc: 0.972 - ETA: 0s - loss: 0.0988 - acc: 0.972 - ETA: 0s - loss: 0.0990 - acc: 0.971 - ETA: 0s - loss: 0.1004 - acc: 0.971 - ETA: 0s - loss: 0.0998 - acc: 0.971 - ETA: 0s - loss: 0.0980 - acc: 0.971 - ETA: 0s - loss: 0.0990 - acc: 0.971 - ETA: 0s - loss: 0.0999 - acc: 0.971 - 3s 477us/step - loss: 0.0999 - acc: 0.9711 - val_loss: 1.6855 - val_acc: 0.7485\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.13319\n",
      "Epoch 18/20\n",
      "6680/6680 [==============================] - ETA: 6s - loss: 0.3039 - acc: 0.900 - ETA: 3s - loss: 0.1353 - acc: 0.966 - ETA: 5s - loss: 0.1187 - acc: 0.971 - ETA: 5s - loss: 0.2326 - acc: 0.955 - ETA: 5s - loss: 0.1933 - acc: 0.960 - ETA: 5s - loss: 0.1825 - acc: 0.963 - ETA: 5s - loss: 0.1767 - acc: 0.961 - ETA: 4s - loss: 0.1551 - acc: 0.966 - ETA: 4s - loss: 0.1425 - acc: 0.969 - ETA: 4s - loss: 0.1342 - acc: 0.971 - ETA: 4s - loss: 0.1199 - acc: 0.974 - ETA: 4s - loss: 0.1236 - acc: 0.975 - ETA: 4s - loss: 0.1271 - acc: 0.974 - ETA: 4s - loss: 0.1205 - acc: 0.975 - ETA: 5s - loss: 0.1174 - acc: 0.975 - ETA: 4s - loss: 0.1124 - acc: 0.975 - ETA: 4s - loss: 0.1043 - acc: 0.977 - ETA: 4s - loss: 0.0982 - acc: 0.978 - ETA: 4s - loss: 0.0905 - acc: 0.979 - ETA: 4s - loss: 0.0893 - acc: 0.979 - ETA: 3s - loss: 0.0835 - acc: 0.980 - ETA: 3s - loss: 0.0777 - acc: 0.981 - ETA: 3s - loss: 0.0790 - acc: 0.980 - ETA: 3s - loss: 0.0776 - acc: 0.980 - ETA: 3s - loss: 0.0736 - acc: 0.981 - ETA: 3s - loss: 0.0779 - acc: 0.980 - ETA: 3s - loss: 0.0799 - acc: 0.979 - ETA: 3s - loss: 0.0818 - acc: 0.978 - ETA: 2s - loss: 0.0813 - acc: 0.978 - ETA: 2s - loss: 0.0931 - acc: 0.976 - ETA: 2s - loss: 0.0997 - acc: 0.975 - ETA: 2s - loss: 0.1046 - acc: 0.974 - ETA: 2s - loss: 0.1113 - acc: 0.973 - ETA: 2s - loss: 0.1105 - acc: 0.974 - ETA: 2s - loss: 0.1143 - acc: 0.973 - ETA: 2s - loss: 0.1128 - acc: 0.972 - ETA: 2s - loss: 0.1116 - acc: 0.973 - ETA: 2s - loss: 0.1085 - acc: 0.973 - ETA: 2s - loss: 0.1112 - acc: 0.973 - ETA: 1s - loss: 0.1137 - acc: 0.973 - ETA: 1s - loss: 0.1143 - acc: 0.973 - ETA: 1s - loss: 0.1112 - acc: 0.974 - ETA: 1s - loss: 0.1096 - acc: 0.974 - ETA: 1s - loss: 0.1134 - acc: 0.973 - ETA: 1s - loss: 0.1114 - acc: 0.973 - ETA: 1s - loss: 0.1117 - acc: 0.973 - ETA: 1s - loss: 0.1112 - acc: 0.972 - ETA: 1s - loss: 0.1094 - acc: 0.972 - ETA: 1s - loss: 0.1106 - acc: 0.972 - ETA: 1s - loss: 0.1123 - acc: 0.971 - ETA: 1s - loss: 0.1132 - acc: 0.970 - ETA: 1s - loss: 0.1131 - acc: 0.970 - ETA: 1s - loss: 0.1121 - acc: 0.970 - ETA: 0s - loss: 0.1121 - acc: 0.970 - ETA: 0s - loss: 0.1110 - acc: 0.970 - ETA: 0s - loss: 0.1090 - acc: 0.971 - ETA: 0s - loss: 0.1072 - acc: 0.971 - ETA: 0s - loss: 0.1068 - acc: 0.971 - ETA: 0s - loss: 0.1059 - acc: 0.971 - ETA: 0s - loss: 0.1047 - acc: 0.971 - ETA: 0s - loss: 0.1041 - acc: 0.971 - ETA: 0s - loss: 0.1048 - acc: 0.971 - ETA: 0s - loss: 0.1057 - acc: 0.971 - ETA: 0s - loss: 0.1049 - acc: 0.971 - ETA: 0s - loss: 0.1052 - acc: 0.971 - ETA: 0s - loss: 0.1049 - acc: 0.971 - ETA: 0s - loss: 0.1039 - acc: 0.971 - ETA: 0s - loss: 0.1033 - acc: 0.971 - ETA: 0s - loss: 0.1026 - acc: 0.971 - 4s 596us/step - loss: 0.1020 - acc: 0.9720 - val_loss: 1.7256 - val_acc: 0.7413\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.13319\n",
      "Epoch 19/20\n",
      "6680/6680 [==============================] - ETA: 9s - loss: 0.0779 - acc: 0.950 - ETA: 3s - loss: 0.0786 - acc: 0.962 - ETA: 3s - loss: 0.0512 - acc: 0.976 - ETA: 3s - loss: 0.0634 - acc: 0.973 - ETA: 3s - loss: 0.0673 - acc: 0.976 - ETA: 3s - loss: 0.0603 - acc: 0.978 - ETA: 2s - loss: 0.0603 - acc: 0.977 - ETA: 2s - loss: 0.0686 - acc: 0.976 - ETA: 2s - loss: 0.0671 - acc: 0.977 - ETA: 3s - loss: 0.0640 - acc: 0.979 - ETA: 2s - loss: 0.0615 - acc: 0.978 - ETA: 2s - loss: 0.0559 - acc: 0.981 - ETA: 2s - loss: 0.0534 - acc: 0.981 - ETA: 2s - loss: 0.0537 - acc: 0.981 - ETA: 2s - loss: 0.0512 - acc: 0.982 - ETA: 2s - loss: 0.0522 - acc: 0.982 - ETA: 2s - loss: 0.0511 - acc: 0.982 - ETA: 2s - loss: 0.0604 - acc: 0.981 - ETA: 2s - loss: 0.0574 - acc: 0.983 - ETA: 2s - loss: 0.0567 - acc: 0.982 - ETA: 2s - loss: 0.0545 - acc: 0.983 - ETA: 2s - loss: 0.0545 - acc: 0.982 - ETA: 2s - loss: 0.0566 - acc: 0.982 - ETA: 2s - loss: 0.0556 - acc: 0.982 - ETA: 2s - loss: 0.0561 - acc: 0.982 - ETA: 2s - loss: 0.0551 - acc: 0.982 - ETA: 2s - loss: 0.0534 - acc: 0.982 - ETA: 1s - loss: 0.0522 - acc: 0.983 - ETA: 1s - loss: 0.0514 - acc: 0.983 - ETA: 1s - loss: 0.0507 - acc: 0.983 - ETA: 1s - loss: 0.0501 - acc: 0.983 - ETA: 1s - loss: 0.0517 - acc: 0.983 - ETA: 1s - loss: 0.0521 - acc: 0.983 - ETA: 1s - loss: 0.0511 - acc: 0.984 - ETA: 1s - loss: 0.0508 - acc: 0.984 - ETA: 1s - loss: 0.0507 - acc: 0.984 - ETA: 1s - loss: 0.0533 - acc: 0.983 - ETA: 1s - loss: 0.0533 - acc: 0.982 - ETA: 1s - loss: 0.0525 - acc: 0.983 - ETA: 1s - loss: 0.0537 - acc: 0.983 - ETA: 1s - loss: 0.0545 - acc: 0.982 - ETA: 1s - loss: 0.0566 - acc: 0.982 - ETA: 1s - loss: 0.0577 - acc: 0.981 - ETA: 1s - loss: 0.0596 - acc: 0.981 - ETA: 1s - loss: 0.0604 - acc: 0.981 - ETA: 1s - loss: 0.0608 - acc: 0.980 - ETA: 1s - loss: 0.0599 - acc: 0.981 - ETA: 1s - loss: 0.0602 - acc: 0.981 - ETA: 0s - loss: 0.0590 - acc: 0.981 - ETA: 0s - loss: 0.0586 - acc: 0.981 - ETA: 0s - loss: 0.0595 - acc: 0.980 - ETA: 0s - loss: 0.0607 - acc: 0.980 - ETA: 0s - loss: 0.0612 - acc: 0.980 - ETA: 0s - loss: 0.0651 - acc: 0.979 - ETA: 0s - loss: 0.0655 - acc: 0.978 - ETA: 0s - loss: 0.0674 - acc: 0.978 - ETA: 0s - loss: 0.0672 - acc: 0.978 - ETA: 0s - loss: 0.0711 - acc: 0.977 - ETA: 0s - loss: 0.0708 - acc: 0.978 - ETA: 0s - loss: 0.0704 - acc: 0.977 - ETA: 0s - loss: 0.0701 - acc: 0.977 - ETA: 0s - loss: 0.0704 - acc: 0.977 - ETA: 0s - loss: 0.0726 - acc: 0.977 - ETA: 0s - loss: 0.0743 - acc: 0.977 - ETA: 0s - loss: 0.0761 - acc: 0.977 - ETA: 0s - loss: 0.0761 - acc: 0.977 - ETA: 0s - loss: 0.0781 - acc: 0.976 - 4s 566us/step - loss: 0.0777 - acc: 0.9769 - val_loss: 1.7549 - val_acc: 0.7305\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.13319\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 6s - loss: 0.0025 - acc: 1.000 - ETA: 4s - loss: 0.0764 - acc: 0.975 - ETA: 4s - loss: 0.0686 - acc: 0.977 - ETA: 4s - loss: 0.0815 - acc: 0.976 - ETA: 4s - loss: 0.0904 - acc: 0.979 - ETA: 4s - loss: 0.0813 - acc: 0.980 - ETA: 4s - loss: 0.1031 - acc: 0.972 - ETA: 4s - loss: 0.1146 - acc: 0.973 - ETA: 4s - loss: 0.1137 - acc: 0.972 - ETA: 3s - loss: 0.1046 - acc: 0.974 - ETA: 3s - loss: 0.1005 - acc: 0.972 - ETA: 3s - loss: 0.1042 - acc: 0.969 - ETA: 3s - loss: 0.0964 - acc: 0.971 - ETA: 3s - loss: 0.0941 - acc: 0.972 - ETA: 3s - loss: 0.0986 - acc: 0.972 - ETA: 2s - loss: 0.0916 - acc: 0.973 - ETA: 2s - loss: 0.0912 - acc: 0.972 - ETA: 2s - loss: 0.0871 - acc: 0.973 - ETA: 2s - loss: 0.0857 - acc: 0.973 - ETA: 2s - loss: 0.0859 - acc: 0.973 - ETA: 2s - loss: 0.0837 - acc: 0.973 - ETA: 2s - loss: 0.0819 - acc: 0.973 - ETA: 2s - loss: 0.0820 - acc: 0.973 - ETA: 2s - loss: 0.0888 - acc: 0.972 - ETA: 2s - loss: 0.0913 - acc: 0.971 - ETA: 1s - loss: 0.0938 - acc: 0.971 - ETA: 1s - loss: 0.0945 - acc: 0.970 - ETA: 1s - loss: 0.0914 - acc: 0.971 - ETA: 1s - loss: 0.0880 - acc: 0.972 - ETA: 1s - loss: 0.0882 - acc: 0.972 - ETA: 1s - loss: 0.0906 - acc: 0.971 - ETA: 1s - loss: 0.0876 - acc: 0.972 - ETA: 1s - loss: 0.0914 - acc: 0.971 - ETA: 1s - loss: 0.0926 - acc: 0.971 - ETA: 1s - loss: 0.0927 - acc: 0.972 - ETA: 1s - loss: 0.0912 - acc: 0.973 - ETA: 1s - loss: 0.0930 - acc: 0.972 - ETA: 1s - loss: 0.0967 - acc: 0.972 - ETA: 1s - loss: 0.0958 - acc: 0.972 - ETA: 1s - loss: 0.0968 - acc: 0.972 - ETA: 0s - loss: 0.0964 - acc: 0.972 - ETA: 0s - loss: 0.0968 - acc: 0.972 - ETA: 0s - loss: 0.0974 - acc: 0.971 - ETA: 0s - loss: 0.0969 - acc: 0.971 - ETA: 0s - loss: 0.0979 - acc: 0.971 - ETA: 0s - loss: 0.0967 - acc: 0.971 - ETA: 0s - loss: 0.0986 - acc: 0.971 - ETA: 0s - loss: 0.0979 - acc: 0.971 - ETA: 0s - loss: 0.0981 - acc: 0.971 - ETA: 0s - loss: 0.0994 - acc: 0.971 - ETA: 0s - loss: 0.0998 - acc: 0.970 - ETA: 0s - loss: 0.1043 - acc: 0.970 - ETA: 0s - loss: 0.1045 - acc: 0.970 - ETA: 0s - loss: 0.1032 - acc: 0.971 - ETA: 0s - loss: 0.1029 - acc: 0.971 - ETA: 0s - loss: 0.1019 - acc: 0.971 - ETA: 0s - loss: 0.1020 - acc: 0.971 - ETA: 0s - loss: 0.1010 - acc: 0.971 - 3s 487us/step - loss: 0.1003 - acc: 0.9720 - val_loss: 1.7640 - val_acc: 0.7293\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.13319\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dea589f080>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO: Train the model.\n",
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "#The below fits the CNN and saves the weights for the epoch which yields the ebst results\n",
    "checkpointer = ModelCheckpoint(filepath='model/notebook/weights.best.VGG19.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "VGG19_model.fit(train_VGG19, train_targets, \n",
    "          validation_data=(valid_VGG19, valid_targets),\n",
    "          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Load the model weights with the best validation loss.\n",
    "VGG19_model.load_weights('model/notebook/weights.best.VGG19.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 73.4450%\n"
     ]
    }
   ],
   "source": [
    "### TODO: Calculate classification accuracy on the test dataset.\n",
    "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Predict Dog Breed with the Model\n",
    "\n",
    "Write a function that takes an image path as input and returns the dog breed (`Affenpinscher`, `Afghan_hound`, etc) that is predicted by your model.  \n",
    "\n",
    "Similar to the analogous function in Step 5, your function should have three steps:\n",
    "1. Extract the bottleneck features corresponding to the chosen CNN model.\n",
    "2. Supply the bottleneck features as input to the model to return the predicted vector.  Note that the argmax of this prediction vector gives the index of the predicted dog breed.\n",
    "3. Use the `dog_names` array defined in Step 0 of this notebook to return the corresponding breed.\n",
    "\n",
    "The functions to extract the bottleneck features can be found in `extract_bottleneck_features.py`, and they have been imported in an earlier code cell.  To obtain the bottleneck features corresponding to your chosen CNN architecture, you need to use the function\n",
    "\n",
    "    extract_{network}\n",
    "    \n",
    "where `{network}`, in the above filename, should be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Write a function that takes a path to an image as input\n",
    "### and returns the dog breed that is predicted by the model.\n",
    "\n",
    "def VGG19_predict_breed(img_path):\n",
    "    \"\"\"Takes a file path of an image, extracts features then classifies it then\n",
    "    states what breed the image mostly resembles according to the VGG19 model.\n",
    "\n",
    "    Parameters:\n",
    "    img_path (String): Path to a file containing the image.\n",
    "\n",
    "    Returns:\n",
    "    (Boolean): The breed which the image submitted most resembles\n",
    "    \"\"\"\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = extract_VGG19(path_to_tensor(img_path))\n",
    "    # obtain predicted vector\n",
    "    predicted_vector = VGG19_model.predict(bottleneck_feature)\n",
    "    # return dog breed that is predicted by the model\n",
    "    return dog_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step6'></a>\n",
    "## Step 6: Write your Algorithm\n",
    "\n",
    "Write an algorithm that accepts a file path to an image and first determines whether the image contains a human, dog, or neither.  Then,\n",
    "- if a __dog__ is detected in the image, return the predicted breed.\n",
    "- if a __human__ is detected in the image, return the resembling dog breed.\n",
    "- if __neither__ is detected in the image, provide output that indicates an error.\n",
    "\n",
    "You are welcome to write your own functions for detecting humans and dogs in images, but feel free to use the `face_detector` and `dog_detector` functions developed above.  You are __required__ to use your CNN from Step 5 to predict dog breed.  \n",
    "\n",
    "A sample image and output for our algorithm is provided below, but feel free to design your own user experience!\n",
    "\n",
    "![Sample Human Output](images/sample_human_2.png)\n",
    "\n",
    "This photo looks like an Afghan Hound.\n",
    "### (IMPLEMENTATION) Write your Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Write your algorithm.\n",
    "### Feel free to use as many code cells as needed.\n",
    "def dog_breed_predictor(img_path):\n",
    "    \"\"\"Takes a file path of an image, classifies if it is potentially a dog or human then\n",
    "    states what breed the image mostly resembles.\n",
    "\n",
    "    Parameters:\n",
    "    img_path (String): Path to a file containing the image.\n",
    "\n",
    "    Returns:\n",
    "    (Boolean): The breed which the image submitted most resembles if a human/dog is found.\n",
    "    \"\"\"\n",
    "    #Detects if there's human or dog faces\n",
    "    if face_detector(img_path) > 0 or dog_detector(img_path) > 0:\n",
    "        #Predicts breed\n",
    "        output = VGG19_predict_breed(img_path).split('.')[-1]\n",
    "    else:\n",
    "        output = \"No human face or dog found.\"\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step7'></a>\n",
    "## Step 7: Test Your Algorithm\n",
    "\n",
    "In this section, you will take your new algorithm for a spin!  What kind of dog does the algorithm think that __you__ look like?  If you have a dog, does it predict your dog's breed accurately?  If you have a cat, does it mistakenly think that your cat is a dog?\n",
    "\n",
    "### (IMPLEMENTATION) Test Your Algorithm on Sample Images!\n",
    "\n",
    "Test your algorithm at least six images on your computer.  Feel free to use any images you like.  Use at least two human and two dog images.  \n",
    "\n",
    "__Question 6:__ Is the output better than you expected :) ?  Or worse :( ?  Provide at least three possible points of improvement for your algorithm.\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wayne's world:  Pekingese\n",
      "Johnny Depp:  Parson_russell_terrier\n",
      "Ariana:  French_bulldog\n",
      "Bulldog:  Boxer\n",
      "Malamute:  Chinese_crested\n",
      "Labrador:  Golden_retriever\n"
     ]
    }
   ],
   "source": [
    "print(\"Wayne's world: \", dog_breed_predictor('app/static/sample_human_2.png'))\n",
    "print(\"Johnny Depp: \", dog_breed_predictor('app/static/Johnny-Depp.jpg'))\n",
    "print(\"Ariana: \", dog_breed_predictor('app/static/Ariana.jpg'))\n",
    "print(\"Bulldog: \", dog_breed_predictor('app/static/Bulldog.jpg'))\n",
    "print(\"Malamute: \", dog_breed_predictor('app/static/Malamute.jpg'))\n",
    "print(\"Labrador: \", dog_breed_predictor('app/static/Lab.jpg')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performs well in some scenarios, but in others it hasn't done so well (i.e. Malamute).\n",
    "\n",
    "In short, the above model will predict a cat as a dog. This is because it is trying map the images to a dog breed and there will be a breed which looks most similar. This is NOT to say, it thinks a cat is a dog with high probability, but it looks more like one breed than another\n",
    "\n",
    "## Improvements\n",
    "Firstly,The model is currently biased as there are more images for Malamute than there are for Manchester Terriers, this means the model will favour features which detect Malamutes over those for Manchester Terriers.\n",
    "\n",
    "Secondly, The model is quite basic, taking taking only the features for one model. We could potentially add more features, from multiple models which may give us more information for prediction.\n",
    "\n",
    "Thirdly, the model is currently geared up to only predict over the total image. IF there are multiple dogs in an image, this is not the prepared well for prediction. Extra preprocessing / tagging could be used to isolate different animals within the dataset, isolate these images out, then predict on what they are. This could give better results / be more applicable to a wider range of images.\n",
    "\n",
    "Finally, images tend to be of the correct orientation (by inspection) in the dataset. To improve on this, or to refine the model further, it could be possible to change the orientation of these images and refit for these newly generated images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
